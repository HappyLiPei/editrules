%\VignetteIndexEntry{Error localization for numerical and categorical edits as a mixed integer problem}
\documentclass[11pt, fleqn, a4paper]{article}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{threeparttable}
\usepackage{natbib}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{threeparttable}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\Lor}{\lor}
\DeclareMathOperator*{\Land}{\land}
\DeclareMathOperator{\res}{\mathfrak{R}}
\newcommand{\rhomap}{\xrightarrow{\rho}}

\newtheorem{theorem}{Theorem}

\usepackage{float} 
 
\floatstyle{boxed}
\newfloat{Rcode}{t}{rco}
\floatname{Rcode}{Figure}



% stimulate latex to put multiple floats on a page.
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{3}
\setcounter{dbltopnumber}{2}
\renewcommand{\topfraction}{.9}
\renewcommand{\textfraction}{.1}
\renewcommand{\bottomfraction}{.75}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\dblfloatpagefraction}{.9}
\renewcommand{\dbltopfraction}{.9}
\hyphenation{time-stamp}

<<echo=false>>=
library(editrules)
@

\title{Error localization as a mixed integer problem with the {\tt editrules} package\\
{\small package version \Sexpr{packageVersion("editrules")}}}
\author{Edwin de Jonge and Mark van der Loo}
\begin{document}
\maketitle
\begin{abstract}
{\em This vignette is far from finished. Version 2.0 of the package will have
the full vignette. At the moment, functionality for solving error localization problems
with lp solvers is experimental.}
\end{abstract}

\newpage
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Analyses of real data are often hindered by occurrences of incomplete or
inconsistent raw data records.  The process of locating and correcting such
errors is referred to as {\em data editing}, and it has been estimated that
National Statistics Institutes spend up to 40\% of their resources on this
process \citep{waal:2011}. For this reason, considerable attention is paid to
the development of data editing methods that can be automated. Since data are
often required to obey many interrelated consistency rules, data
editing can be too complex to perform manually. \citet{winkler:1999} mentions
practical cases where records have to obey 250, 300 or even 750 internal
consistency rules.  Although the {\sf R} statistical environment has numerous
facilities for analyzing categorical data [See {\em e.g.} \citet{husson:2010}],
the options for error localization and record correction are currently limited.

The {\sf R} package {\sf editrules} was
developed to help closing the gap between raw data retrieval and data analysis
with {\sf R}.  The main purpose of the {\sf editrules} package is to provide a
user-friendly environment for handling data restriction rules, to apply those
rules to data, and to localize erroneous fields in data based on the
generalized principle of \cite{fellegi:1976}.  The package does not offer
functionality for data correction. However, it does facilitate the
identification of the set of solutions for an error correction problem. 
For a detailed description we refer to \cite{jonge:2011} and \cite{loo:2011b}.

{\sf editrules} offers a fairly complete toolbox to work with numerical and 
categorical edits. It contains a flexible object for localizing errors, which 
can be adapted by the user of {\sf editrules} based on a branch and bound
algorithm \citep{waal:2011}. However, for the time to solve the error 
localization problem with a branch and bound algorithm grows exponentially 
with the number of (incorrect) variables. Many surveys have hundreds of variables, 
which results in (very) long processing times for records with many errors. 

This paper describes the error localization problem for numerical and categorical 
edits as a mixed integer problem and its implementation in {\sf editrules}. 
Mixed integer programming is a special case of linear programming. Linear 
programming maximizes a linear objective function, which is subject to linear
equality and linear inequality constraints. More formally: 
\begin{eqnarray}
\label{eq:mip}
   \textrm{Maximize } {\bf c}^T {\bf x} \\
   A {\bf x} \leq {\bf b} \\
   \textrm{with } {\bf x} \geq 0
\end{eqnarray}
where ${\bf x}$ is the vector of (numerical) variables to be optimized, ${\bf c}$ is a vector of weights, ${\bf b}$ is a vector of upper bounds and $A$ is a coefficient matrix for the constraints. In mixed integer programming (mip) ${\bf x}$ is a mixture of continious and integer variables.

Error localization implemented as a mixed integer programming results in a fast procedure, which is typically much faster than the branch and bound method also available in {\sf editrules}.

In section \ref{sec:numdataerror} we describe a mip formulation for numerical records. Section ref describes the mip formulation for categorical records. We end with a discussion on the implementation in {\sf editrules}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical data errors}
\label{sec:numdataerror}
A numerical record $\bf x$  with reported values ${\rm(c_1,\dots,c_m)}$ has the following linear constraints:
\begin{equation}
\label{eq:edit}
A {\bf x \odot b} \textrm{ with }\odot\in\{<,\leq,=\}^n,
\end{equation}

It can easily be checked with {\sf editrules} if $\bf x$ violates any of these constraints. If this is the
case, the task is to find the minimal (weighted) number of adjustments to the reported values such that it
complies to the edits. These edits are (usually) valid for all records of a data set. 
For each reported record we define a seperate mip, which contains the set of edits extended with edits that depend on the reported values.

In practice reported values are always bounded: No person is older than 200 years old or taller than 30 feet. So each variable $x_i$  has a lower boundary $\rm l_i$ and an upper boundary $\rm u_i$\footnote{Note that equation \ref{eq:boundary} can also be written in the form of \ref{eq:edit}. We will use univariate or boundary constraints explicitly and exclude them from \ref{eq:edit}.}.
\begin{equation}
{\rm l}_i \leq x_i \leq {\rm u}_i \label{eq:boundary}
\end{equation}

For $x_i$ the following value is reported
\begin{equation}
x_i = {\rm c}_i \label{eq:values}
\end{equation}

For each $x_i$ we introduce a binary variable $d_i \in \{0,1\}$ and add 
the following two edits to the edit set: 
\begin{eqnarray} \label{eq:binconstraint}
x_i &\leq& {\rm c}_i + ({\rm u}_i - {\rm c}_i)d_i \\
({\rm l}_i - {\rm c}_i)d_i + {\rm c}_i &\leq& x_i
\end{eqnarray}
Variable $d_i$ signifies if reported value $\rm c_i$ for variable $x_i$ is wrong: If $d_i = 1$ the edits 
reduce to equation \ref{eq:boundary}, meaning that the value of $x_i$ can take any feasible value within 
its boundaries $\rm l_i$ and $\rm u_i$.
If $d_i = 0$ these edits reduce to equation \ref{eq:values}, meaning
that the reported ${\rm c}_i$ is assumed correct.

Using principle of Felligi Holt \citep{fellegi:1976} which minimizes the weigthed sum of adaptations, the error localization problem can be written as:

{\textrm Minimize} $\sum^m_{j=1} {\rm w_j} d_j$, with:
\begin{equation}
\label{eq:nummip}
\begin{array}{rcrcl}
\sum^m_{j=1} a_{1j} x_j &&                  &\odot_1& b_1    \\
\ldots                 &&                  &\ldots& \ldots  \\
\sum^m_{j=1}a_{nj} x_j &&                  &\odot_n& b_n    \\
    x_1 &-& ({\rm u_1 - c_1}) d_1   &\leq   & {\rm c}_1 \\
   -x_1 &+& ({\rm l_1} - {\rm c_1}) d_1  &\leq   & -{\rm c}_1 \\
\ldots  & & \ldots          &       & \ldots \\
    x_m &-& ({\rm u_m} - {\rm c_m}) d_m   &\leq   & {\rm c}_m \\
   -x_m &+& ({\rm l_m} - {\rm c_m}) d_m  &\leq   & -{\rm c}_m

\end{array}
\end{equation}
with $\rm w_j$ a weight for variable $x_j$. This problem is equal to the following mixed integer problem
$$
\begin{array}{rcl}
   \textrm{Maximize } {\bf c}^T {\bf y}& \\
   A^* {\bf y} & \leq & {\bf b} \\
   \textrm{with } {\bf y} & \geq & 0 \\
   {\bf y} & = &(x_1 - {\rm l_1}, \ldots, x_m - {\rm l_m}, d_1, \ldots, d_m) \\
   {\bf c} & = & (0, \ldots, 0, -{\rm w_1}, \ldots, -{\rm w_m}) \\
   {\bf b} & = & (b_1, \ldots, b_m, {\rm c_1}, -{\rm c_1}, \ldots, {\rm c_m}, -{\rm c_m})
\end{array}
$$

\subsection{Boundaries}
The extended edit set derived in (\ref{eq:nummip}) depends on the boundaries $\rm l_i$ and 
$\rm u_i$. Mathematically the size of these coefficients does not matter. Computationally however their
size is important. If any $|{\rm u_i - c_i}| \gg |a_j|$ or $|{\rm l_i - c_i}| \gg |a_j|$ the resulting mixed integer program may become numerical
unstable. From the lpsolve manual \citep{lpsolveman:2012}:

\begin{quote}
The chance for numerical instability and rounding errors is considerably larger when the input data contains both large and small numbers. So to improve stability, one must try to work with numbers that are somewhat in the same range. Ideally in the neighbourhood of 1.\\

You should realize, that you the user are probably in a better position to scale the problem than any computer algorithm. 
\end{quote}

For the error localization problem this means that the upper and lower boundaries of variables $x_i$ 
shouldn't be too large. A reasonable approach is to take the minimum and maximum values for all variables 
in a dataset and multiply them with a factor $f$ (e.g. $1000$).

\subsection{Implementation in {\sf editrules}}
The function {\sf localizeErrors} is described in detail in \citep{jonge:2011}. It accepts an {\sf editmatrix} and a {\sf
data.frame}, and returns an object of class {\sf errorLocation}. An {\sf
errorLocation} object contains the locations of errors for each record in the
{\sf data.frame} as well as logging information, solution weights and
degeneracy. We extended this function with a parameter {\tt method}, that can be used to select the {\tt 
localizer} or {\tt mip} method. If the method "mip" is given, {\sf editrules} internally creates for each 
record an extended {\sf editmatrix} {\tt E} that contains the conditional boundary conditions as defined 
in (\ref{eq:nummip}).

\cite{lpSolveAPI:2011}
%

Apart from the mandatory arguments (an {\sf editarray} and a {\sf data.frame}),
The arguments are given in table \ref{tabArglocalize}.
%

\begin{table}
\begin{threeparttable}
\caption{Arguments of {\sf localizeErrors}. Optional arguments are given in square brackets.
The optional arguments are also arguments of the underlying {\sf errorLocalizer} function.
}
\label{tabArglocalize}
\begin{tabular}{lp{0.7\textwidth}}
\hline
Argument & description\\
\hline
{\sf E}        & An {\sf editmatrix} or an {\sf editarray}.\\
{\sf dat}      & The data, in the form of a {\sf data.frame}.\\
$[\textrm{\sf weight}]$   & Nonnegative weights for each variable in {\sf dat}.\\
$[\textrm{\sf maxadapt}]$ & Maximum number of variables to adapt.\\
$[\textrm{\sf maxweight}]$& Maximum weight of solution, if weights are not given, this
                    is equal to the maximum number of variables to adapt.\\
$[\textrm{\sf maxduration}]$  &  Maximum time (in seconds), spent searching for a solution for a single 
record.\\
$[\textrm{\sf method}]$   & Method to be used for solving error localization problem. Can be "mip" or 
"localizer".\\
\hline
\end{tabular}
\end{threeparttable}
\end{table}

Figure \ref{RlocalizeErrors} shows an example of localizing errors using mip. 

%
\begin{Rcode}
<<keep.source=true>>=
E <- editmatrix(c(
    "x + y == z",
    "x > 0",
    "y > 0",
    "z > 0"))

dat <- data.frame(
    x = c(1,-1,1),
    y = c(-1,1,1),
    z = c(2,0,2))

# localize all errors in the data using mip
localizeErrors(E,dat, method="mip")
@
\caption{Localizing errors for every record in a data.frame with {\sf localizeErrors}}
\label{RlocalizeErrors}
\end{Rcode}

The default boundary conditions used in {\tt localizeErrors} use ${\rm u_i} = 1000 {\rm c_i}$ and ${\rm l_i} = -1000 {\rm c_i}$.

\section{Categorical data errors}
\citep{loo:2011b} describes an boolean representation for categorical edits that is conform data editing 
literature. In this paper we choose a different representation that can be used to formulate the error 
localization problem for categorical data.


Each edit should hold (conjunction) and each edit states that at least one statement has to be true.
An categorical editmatrix can be seen as a conjunction of disjunctions: it is in conjunctive normal form (reference). 

* Write a categorical variable as a disjunction of boolean variables for each category, with the restriction that their sum is atmost 1. (equals 1 for a closed domain).

* each edit can be written as a disjunction of statements

* each disjunction can be written as a numerical edit where at least one of the variables has to be one

\begin{equation}
   \lnot \bigwedge_{i=1}^m (v_i \in A_i) = \bigvee_{i=1}^m\lnot (v_i \in {A_i}) = \bigvee_{i=1}^m (v_i \in \lnot {A_i})
\end{equation}

$$
\lnot \bigwedge_{i=1}^m v_{i} \in A_i= \bigvee_{i=1}^m \lnot v_{i} \in A_i
$$

\subsection{The cateditmatrix object}

* error localizing edits: for each variable ${\rm c}_i$ $c^{{\rm c}_i}$, introduce an edit
with $d_i = 1 - c^{{\rm c}_i}$. When $d_i = 0$,  ${\rm c}_i = {\rm c}_{i}$, meaning that the
reported value of ${\rm c}_i$ is assumed correct,  When $d_i = 1$, $c^{\rm c}_i = 0$,
meaning that ${\rm c}_i$ should be adapted.

\section{Discussion}

Is a usefull addition to {\tt editrules}, finds quickly solutions to error localization  problems with 
hundreds to thousands of variables. 

Solutions given by current lp solvers can be numerical unstable, which may result in a false positive 
or a false negative solution. Luckily {\tt editrules} contains {\tt substValue} and {\tt isFeasible} 
that can be used together to check the validity of a solution. Furthermore several heuristics can be 
used to increase the numerical stability by using smaller boundaries for the variables.

\subsection{Comparison to {\tt backtracker}}
errorLocalizer is more complete, offers a more complete tool box for finding an optimal solution.
It can also find more equivalent solutions, which is not possible or difficult with MIP solvers.

However when speed of finding a solution matters, it is
\subsection{Previous work}

\bibliographystyle{chicago}
\bibliography{editrules}

\end{document}
