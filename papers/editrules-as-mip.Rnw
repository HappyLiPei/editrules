%\VignetteIndexEntry{Error localization for numerical and categorical edits as a mixed integer problem}
\documentclass[10pt, fleqn]{cbsreport}
%\usepackage{inconsolata}
\usepackage[english]{babel}
\usepackage{amsmath, amsthm}
\usepackage{bbm} % mathbb numbers
\usepackage{array}
\usepackage{natbib}
\usepackage{threeparttable}
\usepackage{makeidx}
\makeindex

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\Lor}{\lor}
\DeclareMathOperator*{\Land}{\land}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\R}{\code{R}} %% call as \R{} to avoid extra spaces.

%\renewcommand{\mathbf}[1]{\ensuremath{\mathbf{#1}}}

\usepackage{float} 
\floatstyle{boxed}
\newfloat{Rcode}{t!}{rco}
\floatname{Rcode}{{\rm Figure}}


% stimulate latex to put multiple floats on a page.
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{3}
\setcounter{dbltopnumber}{2}
\renewcommand{\topfraction}{.9}
\renewcommand{\textfraction}{.1}
\renewcommand{\bottomfraction}{.75}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\dblfloatpagefraction}{.9}
\renewcommand{\dbltopfraction}{.9}


\hyphenation{
}

<<init, echo=FALSE, results='hide', message=FALSE>>=
library(editrules)
#library(xtable)
@

\title{Error localization as a mixed integer problem with the {\tt editrules} package}
\author{Edwin de Jonge and Mark van der Loo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle
\begin{abstract}
Error localization is the problem of finding out which fields in raw data
records contain erroneous values. The \code{editrules} extension package for
the \R{} environment for statistical computing was recently extended with a module
that allows for error localization based on a mixed integer programming
formulation (mip). In this paper we describe the mip formulation of the error
localization problem for the case of numerical, categorical, or mixed numerical
and categorical datasets. The new module is benchmarked against a previously
available module, which is based on a branch-and-bound approach. The benchmark
shows that the mip-based approach is significantly faster and has a smaller memory
footprint. Trade-offs between the branch-and-bound and mip approaches are discussed
as well.
\end{abstract}

\newpage
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Analyses of data are often hindered by occurrences of incomplete or
inconsistent raw data records.  The process of locating and correcting such
errors is referred to as {\em data editing}, and it has been estimated that
National Statistics Institutes may spend up to 40\% of their resources on this
process \citep{waal:2011}. Moreover, data are often required to obey many
cross-variable consistency rules which significantly complicate the data
editing process. Indeed, \citet{winkler:1999} mentions practical cases
(household surveys) where records have to obey 250, 300 or even 750
user-defined interrelated consistency rules.  For these reasons, considerable
attention is paid to the development of data editing methods that can be
automated. 

\subsection{Error localization}
Automated (and even manual) data editing strategies typically consist of three steps:
\begin{enumerate}
\item Find out which rules (expectations) a record violates;
\item find out which fields in a record cause those violations;
\item replace the values in those fields with better estimates, such that no rules
are violated anymore.
\end{enumerate}
The second step is usually referred to as the \emph{error localization}
problem, which is the focus of the current paper.  Although it is widely
recognized that data editing is a necessary step in the statistical process,
the amount of changes made to the data should obviously be minimized to avoid
introducing bias in estimations based on the edited data. This then, led to the
development of the following minimisation problem.
%
\begin{quote}
Given a record of $n$ variables, subject to a number of possibly multivariate
consistency rules. Find the smallest (weighted) subset of fields, such that their
original values can be replaced in such a way that no rules are violated anymore. 
\end{quote}
%
This minimization problem is named after \cite{fellegi:1976}, who first
formulated and solved the problem for the case of categorical data. Error
localization has been extensively discussed in literature\footnote{See
\citet{waal:2011} and references therein}, so we will suffice with a few
remarks. First, note that the minimization problem is possibly over a search
space of size $2^n$, rendering a brute-force approach that runs through all
possible solution candidates computationally infeasible. Second, the problem is
complicated by the occurrence of \emph{implied rules}. That is, the solution
set must not only allow the record to obey the original, user-defined set of
rules, but also rules that are logically or arithmetically implied by the
original set. To cope with these complications, several algorithmic approaches
have been developed, of which two are worth mentioning in this context:
the first is the branch-and-bound approach, developed by \cite{waal:2003a}.
The second is an approach based on mixed-integer programming (mip), described
in \cite{waal:2011}.

\subsection{The \code{editrules} package}
Over the past five to ten years, the \R{} statistical environment received a
surge in popularity and as a consequence it has been extended with many
packages that allow for statistical analyses of data. However, the number of
packages specifically aimed at data editing seems to be somewhat limited,
except possibly in the area of imputation.  The {\sf R} package {\sf editrules}
\citep{jonge:2011a} was developed to help to bridge the gap between raw data
retrieval and data analysis with {\sf R}.  The main purpose of the package is
to provide an easy and consistent interface to define data consistency rules
(often referred to as \emph{edit rules}) in \R{} and to confront them with
data. Furthermore, the package allows for basic rule manipulation (deriving new
rules, finding inconsistencies, \emph{etc.}) and for error localization
functionality.  As such, the package does not offer functionality
to correct data.  Rather, it is aimed at identifying the set of solutions to an
error correction problem: the second step mentioned in the data editing
strategy above. Previous developments of the package have been described
in \cite{jonge:2011,loo:2011b} and \cite{loo:2011a}.


The \code{editrules} package offers a fairly complete toolbox, allowing users
to work with numerical, categorical or mixed-type data editing rules. Up until
now, error localization was performed by an implementation of the
branch-and-bound algorithm described by \cite{waal:2003}. The main disadvantage
of this approach is that the branch-and-bound algorithm has $\mathcal{O}(2^n)$
worst-case time and memory complexity, where $n$ is the number of variables
occurring in a connected set of rules. Moreover, the branch-and-bound solver is
written in pure \R{}, making it intrisically slower than a compiled language
implementation. The main advantages of this approach are the ease of
implementation and the opportunity for users to exert fine-grained control over
the algorithm.

As stated before, the error localisation problem can, under mild conditions, be
translated to a mixed-integer programming problem, which offers the oppertunity
to reuse well-established results from the field of linear and mixed-integer
programming. Indeed, many advanced algorithms for solving such problems have
been developed and in many cases implementations in a compiled language are
available under a permissive license. In \code{editrules}, the solver of the
\code{lpsolve} library \citep{berkelaar:2004} is used through \R{}'s
\code{lpSolveAPI} package \citep{lpSolveAPI:2011}. The \code{lpsolve} library
is written in \code{ANSI} \code{C} and has been tried and tested extensively.

The strategy to solve error localisation problems through this library from 
\R{} therefore consists of translating the problem to a suitable mixed-integer
programming problem, feeding this problem to \code{lpSolveAPI} and retranslating
the results to an error location. It is necessary
to distinguish between:
\begin{itemize}
  \item linear restrictions on purely numerical data,
  \item restrictions on purely categorical data, and
  \item restrictions on mixed-type data,
\end{itemize}
since for each data type a different translation strategy is necessary.

The main part of this paper will focus on how to translate these types of error
localisation problems to a mixed-integer formulation, paying attention to both
theoretical and practical details.  We will give examples of how users can
apply this functionality to their own problemd  in \R{} and benchmark the new
mip-functionality against the existing branch-and-bound solution.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Error localization and mixed integer programming}
\label{sec:mipproblem}
%
A mixed integer programming problem is an optimisation problem that can be
written in the form
\begin{equation}
\begin{array}{r}
\textrm{Minimize } \mathbf{c}^T\mathbf{z}; \\
\textrm{s.t. }\mathbf{Rz} \leq \mathbf{d},
\label{eq:mipmin}
\end{array}
\end{equation}
%
where $\mathbf{c}$ is a constant vector and $\mathbf{z}$ is a vector consisting
of real and integer coefficients. The inner product $\mathbf{c}^T\mathbf{z}$ is
referred to as the \emph{objective function}.  Furthermore, $\mathbf{R}$ is a
coefficent matrix and $\mathbf{d}$ a vector of upper bounds. Formally, the
elements of $\mathbf{c}$, $\mathbf{R}$ and $\mathbf{d}$ are limited to the rational
numbers \citep{schrijver:1998}, which is never a problem in practice since we
are always working with a computer representation of numbers. 

The name \emph{mixed-integer programming} stems from the fact that $\mathbf{z}$
contains continuous as well as integer variables. When $\mathbf{z}$ consists
solely of continuous or integer variables, Problem~\eqref{eq:mipmin} reduces
respectively to a \emph{linear} or an \emph{integer programming} problem.  An
important special case occurs when the integer coefficients of $\mathbf{z}$ may
only take values from \{0,1\}. Such variables are often called binary variables
or decision variables. It occurs as a special case since defining $\mathbf{z}$ to
be integer and applying the appropriate restrictions yields the same problem.

Mixed integer programming is well understood and several software packages
packages are available that implement efficient solvers.  Most mip software
allows for a broader (but equivalent) formulation of the mip problem, allowing
the set of restrictions to include inequalities as well as equalities. As a
side note, we mention that under equality constraints, solutions for the
integer part of $\mathbf{z}$ only exist when the equality constraints
pertaining to the integer part of $\mathbf{z}$ are \emph{totally
unimodular}\footnote{This means that every square submatrix of the coefficient
matrix pertaining to the integer part of $\mathbf{z}$ has determinant 0 or
$\pm1$.}. However, as we will see below, constraints that pertain to the real
and/or integer part of $\mathbf{z}$ are always inequalities in our case, so this
is of no particular concern to us.

In this paper we reformulate Felligi Holt error localization
\citep{fellegi:1976} for numerical, categorical and mixed constraints in terms
of mip problems.  The precise reformulations of the error localization problem
for the three types of rules are different, but in each case the objective
function is of the form 
%
\begin{equation}
\mathbf{w}^T\mathbf{\Delta},
\end{equation}
where $\mathbf{w}$ is a vector of positive weights and $\mathbf{\Delta}$ a vector
of binary variables, one for each variable in the original record, that
indicates whether its value should be adapted. More precisely,
for a record $\mathbf{r}=(r_1,r_2,\ldots,r_n)$ of $n$ variables, we have
\begin{equation}
\Delta_i =\left\{\begin{array}{l}
1 \textrm{ if the value of }r_i\textrm{ must be adapted}\\
0 \textrm{ otherwise}.
\end{array}\right.
\label{eq:defineDelta}
\end{equation}
%
This objective function obviously  meets the requirement that the minimal
(weighted) number of variables should be adapted. 

For an error localisation problem, the restrictions of Problem
\eqref{eq:mipmin} consist of two parts, which we denote
\begin{equation}
\left[\begin{array}{c}
  \mathbf{R}^H\\
  \mathbf{R}^0
\end{array}\right] \mathbf{z} \leq 
\left[\begin{array}{c}
  \mathbf{d}^H\\
  \mathbf{d}^0
\end{array}\right]
\label{eq:errlocasmip}
\end{equation}
Here, the restrictions indicated with $H$ represent a matrix representation of
the user-defined (hard) restrictions that the original record $\mathbf{r}$ must
obey. The vector $\mathbf{z}$ is a numerical vector, containing at least a
numerical representation of the values in a record and the binary variables
$\mathbf{\Delta}$. An algorithmic mip-solver will iteratively alter the values of
$\mathbf{z}$ until a solution satisfying \eqref{eq:errlocasmip} is reached. To make
sure that the objective function reflects the (weighted) number of variables
altered in the process, the restrictions in $\mathbf{R}^0$ serve to make sure that
the values in $\mathbf{z}$ that represent values in $\mathbf{r}$ cannot be altered
without setting the corresponding value in $\mathbf{\Delta}$ to 1.

Summarizing, in order to translate the error localisation problem for the cases
of linear, categorical or conditional mixed restrictions to a mixed integer
problem, we need to properly define $\mathbf{z}$, the restriction set
$\mathbf{R}^H\mathbf{z}\leq \mathbf{d}^H$ and the restriction set
$\mathbf{R}^0\mathbf{z}\leq \mathbf{d}^0$.

\subsection{Linear edit rules}
\label{sec:linedits:mip}
For a numerical record $\mathbf{x}$ taking values in $\mathbb{R}^n$, a set of linear
restrictions can be written as
\begin{equation}
\label{eq:linedits}
\mathbf{Ax}\leq \mathbf{b},
\end{equation}
where in \code{editrules}, we allow the set of restrictions to contain equalities,
inequalities ($\leq$) and strict inequalities ($<$). 
The formulation of these edit rules is very close to the formulation of the original
mip problem of Eq.\ \eqref{eq:mipmin}.
The vector to miminize over is defined as follows:
\begin{equation}
\mathbf{z} = (x_1,x_2,\ldots,x_n,\Delta_1,\Delta_2,\ldots,\Delta_n).
\end{equation}
with the $\Delta_i$ as in Eq.\ \eqref{eq:defineDelta}. The set of restrictions
$\mathbf{R}^H\mathbf{z}\leq \mathbf{d}^H$ is equal to the set of restrictions
of Eqn.\ \eqref{eq:linedits}, except in the case of strict inequalities. The
reason is that while \code{editrules} allows the user to define strict
inequalities ($<$), the \code{lpsolve} library used by \code{editrules} only allows
for inclusive inequalities ($\leq$). For this reason, strict inequalities of the
form $\mathbf{a}^T\mathbf{x}<b$ are rewritten as $\mathbf{a}^T\mathbf{x}\leq
b-\epsilon$, with $\epsilon$ a suitably small constant.

In the case of linear edits, the set of constraints $\mathbf{R}^0\mathbf{z}\leq\mathbf{d}^0$
consists of pairs of the form
\begin{eqnarray*}
  x_i - M\Delta_i   &\leq& x_i^0\\
  -x_i - M\Delta_i  &\leq& -x^0_i
\end{eqnarray*}
for $i=1,2,\ldots,n$. Here The $x^0_i$ are the actual observed values in the
record and $M$ is a suitably large constant allowing $x_i$ to vary between
$x_i^0-M$ and $x_i^0+M$. It is not difficult to see that if $x_i$ is different
from $x_i^0$ then $\Delta_i$ must equal 1. For, if we choose
$\Delta_i=0$ we obtain the set of restrictions
\begin{equation}
  x_i^0 \leq  x_i  \leq x_i^0
\end{equation}
which states that $x_i$ equals $x_i^0$

\subsection{Categorical edit rules}
\label{sec:catdataerror}
Categoricar records $\mathbf{v}\in D$ take values in a cartesian product domain 
\begin{equation}
D = D_1\times D_2\times\cdots\times D_m,
\end{equation}
where each $D_i$ is a finite set of categories for the $i^\textrm{th}$
categorical variable. The category names themselves are unimportant, so for
each $D_i$ we may write
\begin{equation}
\label{eq:defineD}
D_i = \{1,2,\ldots,|D_i|\}.
\end{equation}
The size of the domain, $|D|$, for categorical records is equal to the product
of the $|D_i|$.

An edit splits $D$ into a region where records are considered valid and
a region where records are considered invalid. Specifically, each categorical edit
$e$ can be written as 
\begin{equation}
e = F_{1}\times F_{2}\times\cdots \times F_{m},
\end{equation}
where each $F_{i}$ is a subset of $D_i$. It is understood that if a record
$\mathbf{v}\in e$ then the record violates $e$. Hence, categorical edits are
negatively formulated (they specify the region of $D$ where $\mathbf{v}$ may
not be) in contrast to linear edits which are positively formulated (they
specify the region of $\mathbb{R}^n$ where $\mathbf{x}$ must be). To be able to
translate categorical edits to a MIP problem, we need to reformulate them as
positive edits $\overline{e}$, such that if $\mathbf{v}\in\overline{e}$ then
$\mathbf{v}$ satisfies $e$.  Here, $\overline{e}$ is the complement of
$\mathbf{e}$ in $D$, which can be written as
%
\begin{eqnarray}
\label{eq:invedit}
\lefteqn{
\overline{e} = \overline{F}_{1}\times D_{2}\times\cdots \times D_{m}
}\nonumber\\
&\cup& D_{1}\times \overline{F}_{2}\times\cdots \times D_{m}\cup\cdots 
\cup D_{1}\times D_{2}\times\cdots \times \overline{F}_{m},
\label{eq:complement}
\end{eqnarray}
%
where for each variable $v_i$, $\overline{F}_{i}$ is the complement of
$F_{i}$ in $D_i$.  Observe that Eq.\ \eqref{eq:complement} states that if at
least one $v_i\in\overline{F}_{i}$, then $\mathbf{v}$ satisfies $e$. Below, we
will use this property and construct a linear relation that counts the number
of $v_i\in\overline{F}_i$ over all variables.

To be able to formulate the Felligi Holt-problem in terms of a MIP problem, we
first associate with each categorical variable $v_i$ a binary vector
$\mathbf{d}$ of which the coeffiecients are defined as follows (also see Eq.\
\eqref{eq:defineD}).
\begin{equation}
\label{eq:definedvec}
d_j(v_i) = \left\{\begin{array}{l}
1\textrm{ if } v_i = j\\
0\textrm{ otherwise}.
\end{array}\right.
\end{equation}
Thus, each element of $\mathbf{d}(v_i)$ corresponds to one category in $D_i$.
It is zero everywhere except at the value of $v_i\in D_i$. We will write
$\mathbf{d}(\mathbf{v})$ to indicate the concatenatenated vector
$(\mathbf{d}(v_1),\ldots,\mathbf{d}(v_m))$ which represents a complete record.
Similarly, each edit $e$ is defined a binary vector $\mathbf{e}$ given by
\begin{equation}
\mathbf{e}(e) = \Lor_{\mathbf{v}\in \overline{e}}\mathbf{d}(\mathbf{v}),
\end{equation}
where we interpret 1 and 0 as \code{true} and \code{false} respectively and
the logical 'or' ($\lor$) is applied elementwise to the coefficients of
$\mathbf{d}$.  The above relation can be interpreted as stating that
$\mathbf{e}(e)$ is the the union of all records satisfying $e$.



To set up the hard restriction matrix $\mathbf{R}^H$ of  Eq.\ \eqref{eq:errlocasmip},
we first impose the obvious restriction that each variable can take but a single value:
\begin{equation}
\label{eq:hardmaxcat}
\sum_{j=1}^{|D_i|}d_j(v_i) = 1,
\end{equation}
for $i=1,2,\ldots,m$. It is now not difficult to see that the demand (Eq.\ \eqref{eq:complement}) that at 
least one of the $v_i\in\overline{F}_i$ may be written as
\begin{equation}
\label{eq:hardcat}
\mathbf{e}(e)^T\mathbf{d}(\mathbf{v}) \geq 1.
\end{equation}
Equations \eqref{eq:hardcat} and \eqref{eq:hardmaxcat} constitute the hard restrictions, stored in
$\mathbf{R}^H$.

Using the binary vector notation for $\mathbf{v}$, and adding the $\Delta$-variables
that indicate variable change, the vector to minimize over (Eq.\
\eqref{eq:mipmin}) is written as
\begin{equation}
\mathbf{z} = (\mathbf{d}(\mathbf{v}),\Delta_1,\Delta_2,\ldots,\Delta_m).
\end{equation}
To ensure that a change in $v_i$ results in a change in $\Delta_i$, the 
matrix $\mathbf{R}^0$ contains the restrictions
\begin{equation}
d_{j^0}(v_i) =  1-\Delta_i,
\end{equation}
for $i=1,2,\ldots,m$.  Here, $j^0\in D_i$ is the observed
value for variable $v_i$. It is not difficult to check, using Eq.
\eqref{eq:definedvec}, that the above equation can only hold for all $j$ when
either $v_i=j^0$ and $\Delta_i=0$ (the original value is retained) or
$v_i\not=j^0$ and $\Delta_i=1$ (the value changes).


%\begin{equation}
%  e_j = \bigvee_{i=1}^m v_i \in \overline{F}_{ij}
%\end{equation}


%%%%


% with reported values $({v^0_1,\dots,v^0_m)}$ has 
% the following $n$ constraints:
% \begin{equation}e_j = 
%   \begin{array}{rl}
%     \label{eq:catedit}
%     \mbox{{\bf if }} & v_i \in F_{i,j} \mbox{ for }i = 1, \ldots, m \\
%     \mbox{ {\bf then}} & \mbox{FALSE}
%   \end{array}
% \end{equation}
% with $F_{i,j} \subseteq D_i$ where $i$ is the number of variables, $D_i$ is the
% possible set of categories $\{c_{i,1}, \ldots, c_{i,l_i}\}$ for $v_i$ and $j = 1 \ldots n$ the number of edits.
% 
% Remarkable about categorical edits is their negative formulation in 
% literature (e.g. \ref{}): if a condition is true than the edit is violated. 
% This is in contrast with numerical edits, which state rules that the record
% must obey. To translate categorical edits into mip we need a possitive 
% reformulation, since mip requires constraints 
% that must hold.
% 
% Equation (\ref{eq:catedit}) can be written in the following form:
% \begin{eqnarray}
%      e_j  & = & \lnot \bigwedge_{i=1}^m v_i \in F_{i,j} = \bigvee_{i=1}^m\lnot (v_i \in {F_{i,j}}) \\
%           & = &  \bigvee_{i=1}^m v_i \not\in {F_{i,j}} = \bigvee_{i=1}^m v_i 
%           \in T_{i,j} \label{eq:catpos}
% \end{eqnarray}
% with $T_{i,j} = D_i \setminus F_{i,j}$. 
% This means that edit $e_j$ is satisfied if at least one $v_i \in T_{i,j}$. 


%% susbsubwordt verwijderd maar voor nu even handig.
%\subsubsection{Mip formulation}
%\begin{itemize}
%  \item[a)]
%  Recall from section \ref{sec:mipproblem} that in a mip problem ${\bf x}$ is a 
%  numerical vector. We therefore encode each categorical variable $v_i$ in $l_i$
%  binary dummy variables
%  $$
%  v_i = (d_{i,1}, \ldots, d_{i,l_i})
%  $$
%  with
%  \begin{equation}
%    d_{i,k} = \left\{ 
%    \begin{array}{ll}
%           1 & \mbox{if $v_i = c_{i,k}$}\\
%           0 & \mbox{if $v_i \neq c_{i,k}$}
%    \end{array} \right. 
%  \end{equation}
%  
%  The numerical representation of $\bf{v}$, a record of $m$ categorical variables 
%  can then be written as:
%  \begin{equation}
%    {\bf x} = (d_{1,1}\ldots d_{1,l_1}, \ldots, d_{m, 1}, \ldots, d_{m, l_m}, 
%    \Delta_1, \ldots, \Delta_m)
%  \end{equation}
%  
%  \item[b)]
%  Constraint matrix ${\bf A}^H$ for categorical edits is composed out of two parts.
%  First the fact that each $v_i$ can only have one categorical value:
%  \begin{equation}
%     \sum^{n_i}_{k=1} d_{i,k} = 1
%  \end{equation}
%  for $i$ is $(1, \ldots, m)$
%  
%  Second the categorical constraints of equation \ref{eq:catpos}. 
%  They can now be written as:
%  
%  \begin{equation} \label{eq:mipcatedit}
%    \sum^m_{i=1} \left(\sum^{l_i}_{k=1} t_{i,j,k} \cdot d_{i,k}\right) 
%    \geq 1
%  \end{equation}
%  
%  \begin{equation} \label{eq:aijk}
%    \mbox{with }
%    t_{i,j,k} = \left\{ 
%    \begin{array}{ll}
%           0 & \mbox{if } c_{i,k} \not\in T_{i,j}\\
%           1 & \mbox{if } c_{i,k} \in T_{i,j}
%    \end{array} \right. 
%  \end{equation}
%  Meaning that if one of the $c_{i,k} \in T^j_i$ then the edit is valid.\\
%  
%  
%  \item[c)]
%  The constraints ${\bf A}^0$ can be written as:
%  
%  \begin{equation}
%     d_{i,k^0_i}  =  1 - \Delta_i 
%  \end{equation}
%  where $k^0_i$ denotes the reported value $v^0_i$.
%  It can be easily checked that if $\Delta_i = 0$ $d_{i,k^0} = 1$, 
%  meaning that the reported value 
%  $v^0_i$ is assumed correct. 
%  If $\Delta_i = 1$, $d_{i,k^0_i} = 0$, meaning that $v_i$ must have a different 
%  value. Since mip minimizes the objective function it will try to keep the reported 
%  values at $v^0_i$.
%\end{itemize}
%This makes the mip formulation for categorical edits complete.

\subsection{Conditional rules on mixed type data}
\label{sec:mixdataerror}

Often a data set or survey contains both numerical as categorical variables.
In that case there are often hard constraints including both types
of variables. Such a constraint is called a mixed edit or mixed constraint.
{\sf editrules} allows for mixed constraints which are extensively described in 
\ref{}.

The branch and bound implementation of {\sf editrules} works, but is inefficient
for larger problems. It very quickly generates large memory objects and 
takes a long time to finish. For these problems a {\sf mip} implementation comes
to the rescue.

A set of mixed constraints contains pure numerical, pure categorical
and mixed constraints. 

A mixed record $\bf r$  with reported values $({r^0_1, \dots,r^0_l,
v^0_1,\dots,v^0_m)}$ has the 
following $n$ constraints:
\begin{equation}e_j = 
  \begin{array}{rl}
    \label{eq:mixedit}
    \mbox{{\bf if }} & v_i \in F_{i,j} \mbox{ for }i = 1, \ldots, m \\
    \mbox{ {\bf then}} & \sum^l_{k=1} a_{jk} \cdot r_k \leq b_j \\ 
  \end{array}
\end{equation}
with $F_{i,j} \subseteq D_i$ where $i$ is the number of variables, $D_i$ is the 
possible set of categories $\{c_{i,1}, \ldots, c_{i,l_i}\}$ for $v_i$ and 
$j = 1 \ldots n$ the number of edits.

Equation \ref{eq:mixedit} can written in a more general form:

\begin{equation}
e_j = \bigvee_{i=1}^m (v_i \in T_{i,j}) \vee \sum^l_{k=1} a_{jk} \cdot r_k 
\odot b_j
\end{equation}

\subsubsection{Mip formulation}
\begin{itemize}
  \item[a]
  
  The numerical representation of $\bf{x}$, a record of $k$ numerical variables
  and $m$ categorical variables can then be written as:
  \begin{equation}
    {\bf x} = (r_1, \ldots, r_k, 
              d_{1,1}\ldots d_{1,l_1}, \ldots, d_{m, 1}, \ldots, d_{m, l_m},
              \Delta_1, \ldots, \Delta_{k+m}, n_1, \ldots, n_p)
  \end{equation}
  with
  \begin{equation}
    n_j = \left\{ 
    \begin{array}{ll}
           0 & \mbox{if $\sum^l_{k=1} a_{jk} \cdot r_k \leq b_j$}\\
           1 & \mbox{if $\sum^l_{k=1} a_{jk} \cdot r_k \not \leq b_j$}
    \end{array} \right. 
  \end{equation}

  It is the union of the numerical and categorical description plus extra binary 
  variables $n_i$. Each $n_i$ is a dummy variable introduced to represent a 
  negated numerical constraint which is part of a mixed edit.
  
  \item[b]
  The constraints ${\bf A}^H$ are a combination of the categorical and numerical 
  descriptions. For purely numerical and categorical constraints we use the 
  formulations as stated in previous sections.
  
  A mixed constraint is formulated in (at least) two parts: a categorical part and
  numerical parts.
  First equation \ref{eq:mipcatedit} is extended with 
  numerical dummy 
  variable $n_j$. For each mixed edit we add the following constraint to ${\bf A}^H$
  \begin{equation}
    \label{eq:mixcatedit}
    \sum^m_{i=1} \left(\sum^{l_i}_{k=1} t_{i,j,k} \cdot d_{i,k}\right) + (1-n_j)
    \geq 1
  \end{equation}
  meaning that one of the categorical or numerical conditions must hold.
  
  Further more for each $n_j$ we add the following constraint
  \begin{equation}
    \label{eq:mixnumedit}
    \sum a_{jk} \cdot r_k \leq b_j + Mn_j
  \end{equation}
  With $M$ a sufficiently large number.
  
  
  This means that if $n_j = 0$ the numerical constraint must hold and the
  constraint of eq. \ref{eq:mixcatedit} always holds. If $n_j=1$ then the 
  numerical constraint is not enforced, since $M$ makes the constraint of
  eq. \ref{eq:mixnumedit} always true.
  
  \item[c]
  Constraints ${\bf A}^0$ for a mixed record $r$ are simply the 
  union of the numerical ${\bf A}^0$ for ${\bf r}^0$ and the categorical ${\bf A}^0$ 
  for ${\bf v}^0$.
  
\end{itemize}

This makes the mip formulation for mixed constraints complete.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation and numerical stability issues}
\label{sec:linedits:num}
Reformulating the error localisation problem in terms of a mip problem
introduces two constants: $M$ and $\epsilon$. These constants end up in the
coefficient matrix of the mip solver.  Mathematically, there are no bounds on
the values of $M$ and $\epsilon$, as long as they are chosen so that the
problem remains feasible. In practice however, limitations on the accuracy with
wich numbers are represented on a computer put restrictions on the value range
of numbers that enter the mip solver. In the \code{lpsolve} library (as well
as in \R{}), numbers on the real line are represented as \code{double precision}
numbers, meaning that there are about 15 decimal positions of precision. 

To avoid significant roundoff errors, it is a good idea to restrict the range
of input data. In fact, the authors of the \code{lpsolve} reference manual
\citep{lpsolveman:2012} state the following.
%
\begin{quote}
The chance for numerical instability and rounding errors is considerably 
larger when the input data contains both large and small numbers. 
So to improve stability, one must try to work with numbers that are somewhat 
in the same range. Ideally in the neighborhood of 1.
[...]You should realize, that you the user are probably in a better position to 
scale the problem than any computer algorithm. 
\end{quote}


\begin{equation}
\left[\begin{array}{c|c}
  \mathbf{A}^H &\mathbf{b}^H\\
  \mathbf{A}^0 &\mathbf{b}^0
\end{array}\right] 
= 
\left[\begin{array}{rr|r}
  \multicolumn{1}{c}{ \mathbf{A}^H} & \multicolumn{1}{c|}{\mathbf{0}} & \mathbf{b}^H\\
  \mathbbm{1} & M\mathbbm{1}  &\mathbf{x}^0\\
  -\mathbbm{1} & -M\mathbbm{1} &-\mathbf{x^0}
\end{array}\right] 
\end{equation}
In {\sf editrules} by default $M = 10^7$ and $\epsilon = 10^{-3}$ is used. This 
will 
be sufficient for most purposes, since numerical values and coefficients of the
constraints 
in most cases will be in this range. If not, you should rescale some of your
variables and constraints to make it so.

The linear programming manual from \code{lpsolve}  \citep{lpsolveman:2012} says the 
following on this matter:



\section{Usage}
\subsection{Error localisation}
The mip implementation in \code{editrules} is transparent.
The \code{localizeErrors} function is extended to include an extra parameter
\code{method}. The default value of \code{method} is \code{'bb'}: branch and bound.
Setting \code{method} to \code{'mip'} gives identical but faster result.

<<num example>>=
E <- editmatrix("x <= y")
dat <- data.frame(x=c(10, 2), y=c(1,2))
weight <- data.frame(x=c(2,1), y=c(1,1))

bb <- localizeErrors(E, dat, weight=weight)
mip <- localizeErrors(E, dat, weight=weight, method="mip")

bb$adapt
mip$adapt
@

Internally {\sf localizeErrors} uses for each record {\sf errorLocalizer.mip}.
Unlike {\em branch and bound} this is not a \code{backtracker} object.
\code{errorlocalizer.mip} writes the constraints and values into a mip problems,
feeds it into the \code{lpSolveApi} and formats the resulting output.
<<num mip>>=
E <- editmatrix("x <= y")
r <- c(x=10, y=1)
weight <- c(x=2, y=1)
el <- editrules:::errorLocalizer.mip(E, r, weight=weight)
ls.str(el)
@
Both \code{localizerErrors} as \code{errorLocalizer.mip} can be executed with an 
\code{editmatrix}, \code{editarray} or \code{editset} object.

Note in the example that the {\em mip} solver returns two extra properties that are
not available in {\em branch and bound}: \code{lp} and \code{x\_feasible}.
Since a mip-solvers goal is to find a feasible solution, a set of feasible x 
values is available. It can be helpful to have these values, but note 
that the set is not unique and that the proposed values for the variables to be 
adjusted lie on the boundary of a convex solution region. 
So  in many cases it is better to use a proper imputation method to generate
sensible values for the variable that need to be adjusted.
Secondly \code{errorlocalizer.mip} returns the \code{lp} object. This object is 
a \code{lpSolveApi} object and can be further manipulated or written to disk.
For more information consult the manual of \code{lpSolveApi}.

An advantage of the branch and bound algorithm is that it can generate all optimal
solutions for a given record.  The mip implementation only generates one of the 
best. This is implemented by adding a small uniform pertubation to the weights, 
so mip chooses one of the best at random. This is identical 
to the default behavior of \code{localizeErrors} for \code{method='bb'}.


\subsection{Setting up your own lp problem}
\code{editrules} uses \code{editarray} for dealing with categorical edits. These
are described in \cite{loo:2011b}. An example \code{editarray} is given below.
<<tidy=FALSE>>=
(E <- editarray(expression(
    gender %in% c('male','female'),
    pregnant %in% c(TRUE, FALSE),
    if (pregnant) gender == 'female'
    )
))
@
\code{E} is an editarray. That \code{E} parses the constraints can be seen in 
the pregnant constraint: it has been rewritten in an equivalent form.
To create a mip formulation for the categorical error localization problem,
\code{editrules} rewrites categorical edits into a numerical matrix using the
function \code{as.mip}.
<<>>=
as.mip(E)
@
Above example shows the rewriting of ${\bf A}^H$. All variables are binary 
variables $\in \{0,1\}$. Constraint \code{num1} states
that \code{gender} either must be \code{'male'} or \code{'female'}. Constraint 
\code{num2} states that \code{pregnant} and \code{gender:male} cannot both be
1 (true).

Note that \code{as.mip} 
recognizes that \code{pregnant} is a \code{logical}. The mip formulation contains
just one binary variable for \code{pregnant}.
(explain...)

If we add the $A^0$
constraints we get:
<<tidy=FALSE>>=
as.mip( E
      , x=list(gender="male", pregnant=TRUE)
      , weight=c(1,2)
      )
@
(explain)

\subsection{Implementation}

<<tidy=FALSE>>=
E <- editset(expression(
  married %in% c(TRUE, FALSE),
  if (married) age >= 17
  ))
@

Writing this to ${\bf A}^H$ form we get:
<<>>=
as.mip(E)
@
(explain...)

Adding some ${\bf A}^0$ we get:
<<>>=
as.mip(E, x=list(age=9, married=TRUE))
@
(explain...)

\section{Benchmarks}
\subsection{Tests}


\section{Conclusion}
\code{editrules} includes a mip implementation of the localize errors formalism 
of Felligi and Holt. This mip implementation is typically must faster than the
branch and bound algorithm. 

For most problems mip will generate a good solution. However for problems where
coefficients or values exceed the range of $[10^{-3} - 10^7]$ the user has to rescale
the problem: otherwise the mip solver will become unstable.

\bibliographystyle{chicago}
\bibliography{editrules}

\end{document}

