%\VignetteIndexEntry{Error localization for numerical and categorical edits as a mixed integer problem}
\documentclass[10pt, fleqn, english, rapport]{cbsdiscussionpaper}
%\usepackage{inconsolata}
\usepackage[english]{babel}
\usepackage{amsmath, amsthm}
\usepackage{bbm} % mathbb numbers
\usepackage{array}
\usepackage{natbib}
\usepackage{threeparttable}
\usepackage{makeidx}
\usepackage{todonotes}
\usepackage{listings}


% fgcolor komt uit knitr. Pas het knitr theme aan om te veranderen.
% originele color is \color[rgb]{0.345, 0.345, 0.345}
\lstset{basicstyle=\small\color{fgcolor},frame=lines,rulecolor=\basecolor}
\makeindex

% Footnote symbols to avoid confusion with superscript references.
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator{\fl}{\textrm{fl}}
\DeclareMathOperator*{\Lor}{\lor}
\DeclareMathOperator*{\Land}{\land}
\DeclareMathOperator*{\Minimize}{\textrm{\sf Minimize}}


\renewcommand{\code}[1]{\textcolor{fgcolor}{\texttt{#1}}}
\newcommand{\R}{\code{R}} %% call as \R{} to avoid extra spaces.

\newcommand{\E}{E}
\newcommand{\F}{F}
\newcommand{\Fc}{\mathbf{\overline{F}}}
\newcommand{\D}{D}
\newcommand{\Ls}{\mathbf{L}}
\newcommand{\FD}{\mathbb{F}}
\newcommand{\FDc}{\overline{\mathbb{F}}}


\usepackage{float} 

\newtheorem{example}{Example}

% stimulate latex to put multiple floats on a page.
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{3}
\setcounter{dbltopnumber}{2}
\renewcommand{\topfraction}{.9}
\renewcommand{\textfraction}{.1}
\renewcommand{\bottomfraction}{.75}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\dblfloatpagefraction}{.9}
\renewcommand{\dbltopfraction}{.9}


\hyphenation{
}

<<setup, echo=FALSE, results='hide', message=FALSE>>=
library(editrules)
library(knitr)
set.seed(41)
opts_chunk$set(
   size="small"
  , highlight=FALSE
  , background=rep(1,3)
  , prompt=TRUE
  , comment=NA
)
@

\renewenvironment{kframe}{}{}
%\renewenvironment{knitrout}
%  {\vspace{1ex}\basecolor\hrule\vspace{-3ex}}
%  {\basecolor\hrule\vspace{-2ex}}

\newenvironment{rcode}
  {\vspace{1ex}\basecolor\hrule\vspace{-3ex}}
  {\basecolor\hrule\vspace{-2ex}}

\title{Error localization as a mixed integer problem with the editrules package}
\author{Edwin de Jonge\\  Mark van der Loo}
\Number{2014}{01}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle
\begin{abstract}
Error localization is the problem of finding out which fields in raw data
records contain erroneous values. The \code{editrules} extension package for
the \R{} environment for statistical computing was recently extended with a module
that allows for error localization based on a mixed integer programming
formulation (mip). In this paper we describe the mip formulation of the error
localization problem for the case of numerical, categorical, or mixed numerical
and categorical datasets. We introduce a mip formulation that is a generalization
of both linear as well as categorical restrictions. We derive the numerical
boundaries in which a mip solver generates a stable solution and give directions
on changing them to your own needs.
The new module is benchmarked against a previously
available module, which is based on a branch-and-bound approach. The benchmark
shows that the mip-based approach is significantly faster and has a smaller memory
footprint. Trade-offs between the branch-and-bound and mip approaches are discussed
as well.
\end{abstract}

\newpage
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Analyses of data are often hindered by occurrences of incomplete or
inconsistent raw data records.  The process of locating and correcting such
errors is referred to as {\em data editing}, and it has been estimated that
National Statistics Institutes may spend up to 40\% of their resources on this
process \citep{waal:2011}. Moreover, data are often required to obey many
cross-variable consistency rules which significantly complicate the data
editing process. Indeed, \citet{winkler:1999} mentions practical cases
(household surveys) where records have to obey 250, 300 or even 750
user-defined interrelated consistency rules.  For these reasons, considerable
attention is paid to the development of data editing methods that can be
automated. 

\subsection{Error localization}
Automated (and even manual) data editing strategies typically consist of three steps:
\begin{enumerate}
\item Find out which rules (expectations) a record violates;
\item Find out which fields in a record cause those violations;
\item Replace the values in those fields with better estimates, such that no rules
are violated anymore.
\end{enumerate}
The second step is usually referred to as the \emph{error localization}
problem, which is the focus of the current paper.  Although it is widely
recognized that data editing is a necessary step in the statistical process,
the amount of changes made to the data should obviously be minimized to avoid
introducing bias in estimations based on the edited data. This then, led to the
development of the following minimization problem.
%
\begin{quote}
Given a record of $n$ variables, subject to a number of possibly multivariate
consistency rules. Find the smallest (weighted) subset of fields, such that their
original values can be replaced in such a way that no rules are violated anymore. 
\end{quote}
%
This minimization problem is named after \cite{fellegi:1976}, who first
formulated and solved the problem for the case of categorical data. Error
localization has been extensively discussed in literature\footnote{See
\citet{waal:2011} and references therein.}, so we will suffice with a few
remarks. First, the search space related to the minimization problem grows
exponentially with the number of fields, rendering a brute-force approach that
runs through all possible solution candidates computationally unfeasible.
Second, the problem is complicated by the occurrence of \emph{implied rules}.
That is, the solution set must not only allow the record to obey the original,
user-defined set of rules, but also rules that are logically or arithmetically
implied by the original set. To cope with these complications, several
algorithmic approaches have been developed, of which two are worth mentioning
in this context: the first is the branch-and-bound approach, developed by
\cite{waal:2003a}.  The second is an approach based on mixed-integer
programming (mip), described in \cite{waal:2011}.

\subsection{The editrules package}
Over the past five to ten years, the \R{} statistical environment received a
surge in popularity and as a consequence it has been extended with many
packages that allow for statistical analyses of data. However, the number of
packages specifically aimed at data editing seems to be somewhat limited,
except possibly in the area of imputation.  The {\sf R} package {\sf editrules}
\citep{jonge:2011a} was developed to help to bridge the gap between raw data
retrieval and data analysis with {\sf R}.  The main purpose of the package is
to provide an easy and consistent interface to define data consistency rules
(often referred to as \emph{edit rules}) in \R{} and to confront them with
data. Furthermore, the package allows for basic rule manipulation (deriving new
rules, finding inconsistencies, \emph{etc.}) and for error localization
functionality.  As such, the package does not offer functionality
to correct data.  Rather, it is aimed at identifying the set of solutions to an
error correction problem: the second step mentioned in the data editing
strategy above. Previous developments of the package have been described
in \cite{jonge:2011,loo:2011b} and \cite{loo:2011a}.


The \code{editrules} package offers a fairly complete toolbox, allowing users
to work with numerical, categorical or mixed-type data editing rules. Up until
now, error localization was performed by an implementation of the
branch-and-bound algorithm described by \cite{waal:2003}. The main disadvantage
of this approach is that the branch-and-bound algorithm has $\mathcal{O}(2^n)$
worst-case time and memory complexity, where $n$ is the number of variables
occurring in a connected set of rules. Moreover, the branch-and-bound solver is
written in pure \R{}, making it intrinsically slower than a compiled language
implementation. The main advantages of this approach are the ease of
implementation and the opportunity for users to exert fine-grained control over
the algorithm.

As stated before, the error localization problem can, under mild conditions, be
translated to a mixed-integer programming problem, which offers the opportunity
to reuse well-established results from the field of linear and mixed-integer
programming. Indeed, many advanced algorithms for solving such problems have
been developed and in many cases implementations in a compiled language are
available under a permissive license. In \code{editrules}, the solver of the
\code{lpsolve} library \citep{berkelaar:2010} is used through \R{}'s
\code{lpSolveAPI} package \citep{lpSolveAPI:2011}. The \code{lpsolve} library
is written in \code{ANSI} \code{C} and has been tried and tested extensively.

The strategy to solve error localization problems through this library from 
\R{} therefore consists of translating the problem to a suitable mixed-integer
programming problem, feeding this problem to \code{lpSolveAPI} and translating
the results back to an error location. It is necessary
to distinguish between:
\begin{itemize}
  \item linear restrictions on purely numerical data,
  \item restrictions on purely categorical data, and
  \item restrictions on mixed-type data,
\end{itemize}
since for each data type a different translation strategy is necessary.

The main part of this paper will focus on how to translate these types of error
localization problems to a mixed-integer formulation, paying attention to both
theoretical and practical details.  We will give examples of how users can
apply this functionality to their own problem  in \R{} and benchmark the new
mip-functionality against the existing branch-and-bound solution.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Error localization and mixed integer programming}
\label{sec:mipproblem}
%
A mixed integer programming problem is an optimization problem that can be
written in the form
\begin{equation}
\begin{array}{r}
\textrm{Minimize } f(\mathbf{z}) = \mathbf{c}^T\mathbf{z}; \\
\textrm{s.t. }\mathbf{Rz} \leq \mathbf{d},
\label{eq:mipmin}
\end{array}
\end{equation}
%
where $\mathbf{c}$ is a constant vector and $\mathbf{z}$ is a vector consisting
of real and integer coefficients. One usually refers to $\mathbf{z}$ as the
\emph{decision vector} and the inner product $\mathbf{c}^T\mathbf{z}$ as the
\emph{objective function}.  Furthermore, $\mathbf{R}$ is a coefficient matrix
and $\mathbf{d}$ a vector of upper bounds. Formally, the elements of
$\mathbf{c}$, $\mathbf{R}$ and $\mathbf{d}$ are limited to the rational numbers
\citep{schrijver:1998}, which is never a problem in practice since we are
always working with a computer representation of numbers. 

The name \emph{mixed-integer programming} stems from the fact that $\mathbf{z}$
contains continuous as well as integer variables. When $\mathbf{z}$ consists
solely of continuous or integer variables, Problem~\eqref{eq:mipmin} reduces
respectively to a \emph{linear} or an \emph{integer programming} problem.  An
important special case occurs when the integer coefficients of $\mathbf{z}$ may
only take values from \{0,1\}. Such variables are often called binary variables. 
It occurs as a special case since defining $\mathbf{z}$ to
be integer and applying the appropriate restrictions yields the same problem.

Mixed integer programming is well understood and several software packages
packages are available that implement efficient solvers.  Most mip software
allows for a broader (but equivalent) formulation of the mip problem, allowing
the set of restrictions to include inequalities as well as equalities. As a
side note, we mention that under equality restrictions, solutions for the
integer part of $\mathbf{z}$ only exist when the equality restrictions
pertaining to the integer part of $\mathbf{z}$ are \emph{totally
unimodular}\footnote{This means that every square submatrix of the coefficient
matrix pertaining to the integer part of $\mathbf{z}$ has determinant 0 or
$\pm1$.}. However, as we will see below, restrictions that pertain to the real
and/or integer part of $\mathbf{z}$ are always inequalities in our case, so this
is of no particular concern to us.

In this paper we reformulate Felligi Holt error localization
\citep{fellegi:1976} for numerical, categorical and mixed-type restrictions in terms
of mip problems.  The precise reformulations of the error localization problem
for the three types of rules are different, but in each case the objective
function is of the form 
%
\begin{equation}
\label{eq:objfun}
\mathbf{w}^T\mathbf{\Delta},
\end{equation}
where $\mathbf{w}$ is a vector of positive weights and $\mathbf{\Delta}$ a vector
of binary variables, one for each variable in the original record, that
indicates whether its value should be adapted. More precisely,
for a record $\mathbf{r}=(r_1,r_2,\ldots,r_n)$ of $n$ variables, we have
\begin{equation}
\Delta_i =\left\{\begin{array}{l}
1 \textrm{ if the value of }r_i\textrm{ must be adapted}\\
0 \textrm{ otherwise}.
\end{array}\right.
\label{eq:defineDelta}
\end{equation}
%
This objective function obviously  meets the requirement that the minimal
(weighted) number of variables should be adapted. 

For an error localization problem, the restrictions of Problem
\eqref{eq:mipmin} consist of two parts, which we denote
\begin{equation}
\left[\begin{array}{c}
  \mathbf{R}^H\\
  \mathbf{R}^0
\end{array}\right] \mathbf{z} \leq 
\left[\begin{array}{c}
  \mathbf{d}^H\\
  \mathbf{d}^0
\end{array}\right]
\label{eq:errlocasmip}
\end{equation}
Here, the restrictions indicated with $H$ represent a matrix representation of
the user-defined (hard) restrictions that the original record $\mathbf{r}$ must
obey. The vector $\mathbf{z}$ is a numerical vector, containing at least a
numerical representation of the values in a record and the binary variables
$\mathbf{\Delta}$. An algorithmic mip-solver will iteratively alter the values of
$\mathbf{z}$ until a solution satisfying \eqref{eq:errlocasmip} is reached. To make
sure that the objective function reflects the (weighted) number of variables
altered in the process, the restrictions in $\mathbf{R}^0$ serve to make sure that
the values in $\mathbf{z}$ that represent values in $\mathbf{r}$ cannot be altered
without setting the corresponding value in $\mathbf{\Delta}$ to 1.

Summarizing, in order to translate the error localization problem for the cases
of linear, categorical or conditional mixed restrictions to a mixed integer
problem, we need to properly define $\mathbf{z}$, the restriction set
$\mathbf{R}^H\mathbf{z}\leq \mathbf{d}^H$ and the restriction set
$\mathbf{R}^0\mathbf{z}\leq \mathbf{d}^0$.

\subsection{Linear edit rules}
\label{sec:linedits:mip}
For a numerical record $\mathbf{x}$ taking values in $\mathbb{R}^n$, a set of linear
restrictions can be written as
\begin{equation}
\label{eq:linedits}
\mathbf{Ax}\leq \mathbf{b},
\end{equation}
where in \code{editrules}, we allow the set of restrictions to contain equalities,
inequalities ($\leq$) and strict inequalities ($<$). 
The formulation of these edit rules is very close to the formulation of the original
mip problem of Eq.\ \eqref{eq:mipmin}.
The vector to minimize over is defined as follows:
\begin{equation}
\label{eq:zlin}
\mathbf{z} = (x_1,x_2,\ldots,x_n,\Delta_1,\Delta_2,\ldots,\Delta_n).
\end{equation}
with the $\Delta_i$ as in Eq.\ \eqref{eq:defineDelta}. The set of restrictions
$\mathbf{R}^H\mathbf{z}\leq \mathbf{d}^H$ is equal to the set of restrictions
of Eqn.\ \eqref{eq:linedits}, except in the case of strict inequalities. The
reason is that while \code{editrules} allows the user to define strict
inequalities ($<$), the \code{lpsolve} library used by \code{editrules} only allows
for inclusive inequalities ($\leq$). For this reason, strict inequalities of the
form $\mathbf{a}^T\mathbf{x}<b$ are rewritten as $\mathbf{a}^T\mathbf{x}\leq
b-\epsilon$, with $\epsilon$ a suitably small positive constant.

In the case of linear edits, the set of constraints $\mathbf{R}^0\mathbf{z}\leq\mathbf{d}^0$
consists of pairs of the form
\begin{eqnarray}
  x_i - M\Delta_i   &\leq& x_i^0\nonumber\\
  -x_i - M\Delta_i  &\leq& -x^0_i
\label{eq:deltanum}
\end{eqnarray}
for $i=1,2,\ldots,n$. Here The $x^0_i$ are the actual observed values in the
record and $M$ is a suitably large positive constant allowing $x_i$ to vary between
$x_i^0-M$ and $x_i^0+M$. It is not difficult to see that if $x_i$ is different
from $x_i^0$ then $\Delta_i$ must equal 1. For, if we choose
$\Delta_i=0$ we obtain the set of restrictions
\begin{equation}
  x_i^0 \leq  x_i  \leq x_i^0
\end{equation}
which states that $x_i$ equals $x_i^0$

\begin{example}
Consider a record with business survey data, consisting of the variables
\emph{Number of staff} $p$ and \emph{Personnel cost}  $c$.  We have the rules
$p\geq0$, $c\geq 0$ and $c\geq p$. The latter rule expresses the notion that
for each staff member, more than one monetary unit is spent. Given two observed
values $p^0$ and $c^0$, disobeying one or more of the rules, the mip problem
for error localization has the following form.
\begin{displaymath}
 \Minimize_{(\mathbf{x},\mathbf{\Delta})\in\mathbb{R}^2\times\{0,1\}^2} \Delta_p + \Delta_c
\end{displaymath}
\begin{equation*}
  \textrm{s.t. }\left[\begin{array}{rrrr}
    1 & -1 & 0 & 0\\
   -1 &  0 & 0 & 0\\
    0 & -1 & 0 & 0\\
    1 &  0 & -M & 0\\
   -1 &  0 & -M & 0\\
    0 &  1 & 0 & -M\\
   -1 &  1 & 0 & -M\\
  \end{array}\right]
  \left[\begin{array}{c}
   p \\ c \\ \Delta_p \\ \Delta_c
  \end{array}\right] \leq
  \left[\begin{array}{c}
   0 \\ 0 \\ 0 \\ p^0\\-p^0 \\ c^0\\ -c^0
  \end{array}\right].
\end{equation*}
Here, the first three rows in the set of restrictions represent the consistency rules
while the other rows connect the indicator variables $\mathbf{\Delta}=(\Delta_p,\Delta_c)$
with $p$ and $c$. 
\hfill\qed
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Categorical edit rules}
\label{sec:catdataerror}
Categorical records $\mathbf{v}\in D$ take values in a Cartesian product domain 
\begin{equation}
\label{eq:defineD}
\D = D_1\times D_2\times\cdots\times D_m,
\end{equation}
where each $D_i$ is a finite set of categories for the $i^\textrm{th}$
categorical variable. The category names are unimportant in this formulation, 
so for each $D_i$ we may write
\begin{equation}
\label{eq:defineD}
D_i = \{1,2,\ldots,|D_i|\}.
\end{equation}
The size of the domain, $|\D|$, for categorical records is equal to the
product of the $|D_i|$.

A categorical edit is a subset $\F$ of $\D$ where records are
considered invalid, and we may write
\begin{equation}
\label{eq:defcatedit}
\F = F_{1}\times F_{2}\times\cdots \times F_{m},
\end{equation}
where each $F_{i}$ is a subset of $D_i$. It is understood that if a record
$\mathbf{v}\in \F$ then the record violates the edit.
Hence, categorical edits are
negatively formulated (they specify the region of $D$ where $\mathbf{v}$ may
not be) in contrast to linear edits which are positively formulated (they
specify the region of $\mathbb{R}^n$ where $\mathbf{x}$ must be). To be able to
translate categorical edits to a mip problem, we need to specify 
$\overline{\F}$, such that if $\mathbf{v}\in\overline{\F}$ then
$\mathbf{v}$ satisfies $e$.  Here, $\overline{\F}$ is the complement of
$\F$ in $D$, which can be written as
%
\begin{eqnarray}
\label{eq:invedit}
\lefteqn{
\overline{\F} = \overline{F}_{1}\times D_{2}\times\cdots \times D_{m}
}\nonumber\\
&\cup& D_{1}\times \overline{F}_{2}\times\cdots \times D_{m}\cup\cdots 
\cup D_{1}\times D_{2}\times\cdots \times \overline{F}_{m},
\label{eq:complement}
\end{eqnarray}
%
where for each variable $v_i$, $\overline{F}_{i}$ is the complement of
$F_{i}$ in $D_i$.  Observe that Eq.\ \eqref{eq:complement} states that if at
least one $v_i\in\overline{F}_{i}$, then $\mathbf{v}$ satisfies $e$. 
%%% OPM: dit is extra notatie, die de zaak niet per se duidelijker maakt.
%Membership to $\overline{\F}$ can also be 
%written with the following logical statement:
%\begin{equation}
%\label{eq:cat_logical}
%\mathbf{v} \in \overline{\F}= \bigvee^m_{i=1} v_i \in \overline{F}_i
%\end{equation}
Below, we
will use this property and construct a linear relation that counts the number
of $v_i\in\overline{F}_i$ over all variables.

To be able to formulate the Felligi Holt-problem in terms of a mip problem, we
first associate with each categorical variable $v_i$ a binary vector
$\mathbf{d}$ of which the coefficients are defined as follows (see also Eq.\
\eqref{eq:defineD}).
\begin{equation}
\label{eq:definedvec}
d_\lambda(v_i) = \left\{\begin{array}{l}
1\textrm{ if } v_i = \lambda\\
0\textrm{ otherwise},
\end{array}\right.
\end{equation}
where $\lambda\in D_i$.
Thus, each element of $\mathbf{d}(v_i)$ corresponds to one category in $D_i$.
It is zero everywhere except at the value of $v_i\in D_i$. We will write
$\mathbf{d}(\mathbf{v})$ to indicate the concatenated vector
$(\mathbf{d}(v_1),\ldots,\mathbf{d}(v_m))$ which represents a complete record.
Similarly, each edit can be represented by a binary vector $\mathbf{e}$ given by
\begin{equation}
\mathbf{e} = \left(\bigvee _{\lambda\in \overline{F}_1}\mathbf{d}(\lambda), \ldots ,\bigvee _{\lambda\in \overline{F}_m}\mathbf{d}(\lambda)
\right),
\label{eq:ecat}
\end{equation}
where we interpret 1 and 0 as \code{true} and \code{false} respectively and
the logical 'or' ($\lor$) is applied element-wise to the coefficients of
$\mathbf{d}$.  The above relation can be interpreted as stating that
$\mathbf{e}$ represents the valid value combinations of variables contained in
the edit. 

To set up the hard restriction matrix $\mathbf{R}^H$ of  Eq.\ \eqref{eq:errlocasmip},
we first impose the obvious restriction that each variable can take but a single value:
\begin{equation}
\label{eq:hardmaxcat}
\sum_{\lambda\in D_i}d_\lambda(v_i) = 1,
\end{equation}
for $i=1,2,\ldots,m$. It is now not difficult to see that the demand (Eq.\ \eqref{eq:complement}) that at 
least one of the $v_i\in\overline{F}_i$ may be written as
\begin{equation}
\label{eq:hardcat}
\mathbf{e}^T\mathbf{d}(\mathbf{v}) \geq 1.
\end{equation}
Equations \eqref{eq:hardmaxcat} and \eqref{eq:hardcat} constitute the hard restrictions, stored in
$\mathbf{R}^H$.

Using the binary vector notation for $\mathbf{v}$, and adding the $\Delta$-variables
that indicate variable change, the vector to minimize over (Eq.\
\eqref{eq:mipmin}) is written as
\begin{equation}
\mathbf{z} = (\mathbf{d}(\mathbf{v}),\Delta_1,\Delta_2,\ldots,\Delta_m).
\end{equation}
To ensure that a change in $v_i$ results in a change in $\Delta_i$, the 
matrix $\mathbf{R}^0$ contains the restrictions
\begin{equation}
\label{eq:deltacat}
d_{\lambda^0}(v_i) =  1-\Delta_i,
\end{equation}
for $i=1,2,\ldots,m$.  Here, $\lambda^0\in D_i$ is the observed
value for variable $v_i$. It is not difficult to check, using Eq.
\eqref{eq:definedvec}, that the above equation can only hold for all $\lambda$ when
either $v_i=\lambda^0$ and $\Delta_i=0$ (the original value is retained) or
$v_i\not=\lambda^0$ and $\Delta_i=1$ (the value changes).

\begin{example}
Consider a two-variable record from the census with the variables \emph{Marital status} 
$m$ and \emph{Age class} $a$. We have $\mathbf{v}=(m,a)\in D$ where
\begin{equation*}
D = D_m\times D_a = \{\textsf{married, unmarried}\}\times\{\textsf{child, adult}\}.
\end{equation*}
Using the binary representation we see that a married adult is represented by the vector
$\mathbf{v}^0=(\mathbf{d}(\textsf{married}),\mathbf{d}(\textsf{adult})) = (1,0,0,1)$. The rule that
states ``\emph{A child cannot be married}'' translates to
\begin{equation*}
F = F_m\times F_a =  \{\textsf{married}\}\times\{\textsf{child}\}
\end{equation*}
which gives $\overline{F}_m=\{\textrm{unmarried}\}$ and $\overline{F}_a=\{\textrm{adult}\}$.
Using Eq.\ \eqref{eq:ecat} we get $\mathbf{e}=(0,1,0,1)$ and one may verify
that $\mathbf{e}^T\mathbf{d}(\textsf{married, child})=0$. For $\mathbf{v}^0$, the mip
problem for error localisation now looks like this.
\begin{equation}
  \Minimize_{(\mathbf{v},\mathbf{\Delta})\in D\times \{0,1\}^2} \Delta_m + \Delta_a
\end{equation}
\begin{equation*}
  \textrm{s.t. }\left[\begin{array}{rrrrrr}
    1 &  0 & 1 & 0 & 0 & 0\\
    1 &  1 & 0 & 0 & 0 & 0\\
    0 &  0 & 1 & 1 & 0 & 0\\
    1 &  0 & 0 & 0 &-1 & 0\\
    0 &  0 & 1 & 0 & 0 &-1\\
  \end{array}\right]
  \left[\begin{array}{c}
   d_{\textrm{married}}(m) \\ d_{\textrm{unmarried}}(m) \\ 
   d_{\textrm{child}}(a)\\ d_{\textrm{adult}}(a)\\ \Delta_m \\ \Delta_a
  \end{array}\right]
\begin{array}{c}
\geq\\=\\=\\=\\=
\end{array}
  \left[\begin{array}{c}
   1 \\ 1 \\ 1 \\ 1 \\ 1
  \end{array}\right].
\end{equation*}
Here, the first row represents the edit rule, the second and third force that each
variable can take but one value (Eq.\ \ref{eq:hardcat}), the last two rows
connect the indicator variables $\Delta_m$ and $\Delta_a$ with the value
of $m$ and $a$ (Eq.\ \ref{eq:deltacat}).
\hfill\qed
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Mixed-type edit rules}
\label{sec:mixdataerror}
Records $\mathbf{r}$ containing both numerical and categorical data can be denoted
as a concatenation of numerical and categorical variables 
taking values in $D\times\mathbb{R}^n$: 
\begin{equation}
\mathbf{r} = (v_1,\ldots,v_m,x_1,\ldots,x_n) = (\mathbf{v},\mathbf{x}),
\end{equation}
where $D$ is defined in Eq.\ \eqref{eq:defineD}. 
As stated above, categorical edits are usually defined negatively as a region of
$D$ that is disallowed while linear edits define regions in $\mathbb{R}^n$ that
are allowed. We may choose a negative formulation of edits containing
both variable types by defining $\E$: 
\begin{equation}
\E = \{\mathbf{r}\in \D \times\mathbb{R}^n: 
\mathbf{v}\in \F \land \mathbf{x} \in P \},
\end{equation}
where $\F \subseteq \D$ and $P$ is a convex subset of $\mathbb{R}^n$ defined by a
(possibly empty) set of $k$ linear inequalities of the form $\mathbf{a}^T\mathbf{x}
> b$.  It is understood that if $\mathbf{r} \in \E$, then $\mathbf{r}$ violates
the edit. 

To obtain a positive reformulation, we first negate the set membership
condition and apply basic rules of proposition logic:
\begin{eqnarray}
&&\lnot\big(\mathbf{v}\in \F \land \mathbf{x} \in P \big)\nonumber\\
&\Leftrightarrow& \lnot\big(\mathbf{v}\in \F \land \mathbf{a}^T_{1}\mathbf{x} > b_{1} 
\land \ldots \land \mathbf{a}^T_{k}\mathbf{x} > b_{k}\big)\nonumber\\
&\Leftrightarrow& \mathbf{v}\in \overline{\F} \lor \mathbf{a}^T_{1}\mathbf{x}\leq b_{1} 
\lor \ldots \lor \mathbf{a}^T_{k}\mathbf{x}\leq b_{k}.
\label{eq:implication}
\end{eqnarray}
This then yields a positive formulation of $E$. That is, a record $\mathbf{r}$
satisfies $E$ if and only if
\begin{equation}
  \label{eq:mixedtype}
  \mathbf{r} \in \overline{\E} \Leftrightarrow 
\bigvee_{i=1}^m v_i \in \overline{F}_i \:\lor\: \bigvee^{k}_{j=1} \mathbf{a}_j^T\mathbf{x} \leq b_j.
\end{equation}
This formulation is both a generalization of linear inequality (Eq.\ \ref{eq:linedit})
and
categorical edits (Eq.\ \ref{eq:defcatedit}). Choosing $k=0$, we get
$P=\mathbb{R}^n$ and only the categorical part remains. Similarly, choosing
$\F=\D$, only the linear inequalities remain. The definition in 
Eq.\ \eqref{eq:implication} can be rewritten as a ``conditional edit'' by using
the implication replacement rule from propositional logic  which states that
$\lnot p\lor q$ may be replaced by $p\Rightarrow q$. If we limit
Eq.\ \eqref{eq:implication} to a single inequality, we obtain the normal form of
\cite{waal:2003}.
\begin{equation}
\mathbf{v} \in \F\Rightarrow \mathbf{a}^T\mathbf{x}\leq b.
\end{equation}
If we choose $\overline{F}=D$ and leave two inequalities we obtain a conditional edit
on numerical data:
\begin{equation}
\mathbf{a}_1^T\mathbf{x} > b_1 \Rightarrow \mathbf{a}_2^T\mathbf{x} \leq b_2.
\end{equation}
These conditional type edits are more natural for users as they can directly be
translated to an \code{if} statement in a scripting language. Finally, note
that equalities can be introduced by defining pairs of
edits like so:
\begin{equation}
\left\{\begin{array}{l}
\mathbf{v} \in \F\Rightarrow \mathbf{a}^T\mathbf{x}\leq b\\
\mathbf{v} \in \F\Rightarrow -\mathbf{a}^T\mathbf{x}\leq -b.
\end{array}\right.
\end{equation}


To reformulate Eq. \eqref{eq:implication} as a restriction in a mip problem, we
first define binary variables $\ell_j$ that indicate whether $\mathbf{x}$ obeys
$\mathbf{a}^T_k\mathbf{x} > b$:
\begin{equation}
\ell_j = \left\{\begin{array}{l}
0 \textrm{ when } \mathbf{a}^T_j\mathbf{x}\leq b_j\\
1 \textrm{ when } \mathbf{a}^T_j\mathbf{x} > b_j %( = \mathbf{x} \in P_j)
\end{array}\right.
\end{equation}
Using the or-form of the set condition (the second line of Eq.\ \eqref{eq:implication})
and the definition of $\mathbf{e}$, we can write the mixed-data edit as
\begin{equation}
\label{eq:mixedit}
\mathbf{e}^T\mathbf{d}(\mathbf{v}) + \sum^{k}_{j=1} (1-\ell_j) \geq 1.
\end{equation}
Rules of this form constitute the user-defined part of the $\mathbf{R}^H$ part
of the restriction matrix. To connect $\ell_j$ with the linear restrictions we
also add
\begin{equation}
\label{eq:linconsequent}
\mathbf{a}^T_j\mathbf{x} \leq b_j + M\ell_j,
\end{equation}
to $\mathbf{R}^H$ with $M$ a suitably large positive constant. Indeed, if
$\ell_j=0$, the inequality $\mathbf{a}^T_j\mathbf{x}\leq b_j$ is enforced. When
$\ell_j=1$ the whole restriction can hold regardless of whether the inequality
holds.  Finally, similar to the purely categorical case we need to add
restrictions on the binary representation of $\mathbf{v}$ as in Eq.\
\eqref{eq:hardmaxcat}, so Eq.\ \eqref{eq:hardmaxcat}, Eq.\
 \eqref{eq:mixedit} and Eq.\ \eqref{eq:linconsequent} constitute $\mathbf{R}^H$.

In general there may be $k$ multiple mixed-type edits yielding an equal number
of indicator variables for inequalities in the consequent. So the vector to
minimize over becomes
\begin{equation}
\mathbf{z} = (\mathbf{d}(\mathbf{v}),\mathbf{x},\Delta_1,\ldots,\Delta_m,\ldots,\Delta_{m+n},
\ell_1,\ldots,\ell_K),
\end{equation}
where $K$ is the total number of linear edits occurring in all the mixed-type edits.
Finally, the $\mathbf{R}^0$ matrix, connecting the $\Delta$ variables with the actual
recorded values consists of the union of the restrictions for categorical variables
(Eq.\ \eqref{eq:deltacat}) and those for numerical variables (Eq.\ \eqref{eq:deltanum}).

\begin{example}
We consider a record $\mathbf{r}$ with the variables \emph{type of business}
$t$, which takes values in $D_t=\{\textrm{sp},\textrm{other}\}$, where ``sp''
stands for ``sole proprietorship'', \emph{personnel cost} $c\in \mathbb{R}$ and
\emph{number of staff} $p\in\mathbb{R}$. Hence, we have $\mathbf{r}=(t,p,c)\in
D_t\times\mathbb{R}^2$. We impose the following rules on $\mathbf{r}$:
$p\geq0$, $c\geq0$, $c\geq p$ and if the \emph{business type} is a sole
proprietorship, then the number of staff must equal zero. This may be expressed
as $(t \in \{\textrm{sp}\})\Rightarrow (p=0)$ or equivalently
$(t\in\{\textrm{other}\})\lor (p=0)$. For a record
$\mathbf{r}^{0}=(\textrm{sp},p^0,c^0)$, the error localisation problem takes
the following form.
\begin{equation*}
\Minimize_{(\mathbf{r},\mathbf{\Delta},\ell)\in D_t\times\mathbb{R}^2\times\{0,1\}^4} \Delta_t + \Delta_p + \Delta_c
\end{equation*}
\begin{equation*}
\textrm{s.t. }\left[\begin{array}{rrrrrrrr}
0 & 1& 0& 0& 0& 0& 0&-1\\
0 & 0& 1& 0& 0& 0& 0& 0\\
0 & 0& 0& 1& 0& 0& 0& 0\\
0 & 0& 1&-1& 0& 0& 0& 0\\
1 & 1& 0& 0& 0& 0& 0& 0\\
0 & 0& 1& 0& 0& 0& 0&-M\\
1 & 0& 0& 0& 1& 0& 0& 0\\
0 & 0& 1& 0& 0&-M& 0& 0\\
0 & 0&-1& 0& 0&-M& 0& 0\\
0 & 0& 0& 1& 0& 0&-M& 0\\
0 & 0& 0&-1& 0& 0&-M& 0\\
\end{array}\right]
\left[\begin{array}{cccccccc}
d_{\textrm{sp}}(t)\\ d_{\textrm{other}}(t)\\
p\\c\\ \Delta_t \\ \Delta_p \\ \Delta_c \\ \ell
\end{array}\right]
\begin{array}{c}
\geq \\ \geq \\ \geq \\ \geq \\ = \\ > \\ = \\ \leq \\ \leq \\ \leq \\ \leq
\end{array}
\left[\begin{array}{c}
0 \\ 0 \\ 0 \\  0 \\  1 \\ 0 \\ 1 \\ p^0 \\ -p^0 \\ c^0 \\ -c^0
\end{array}\right]
\end{equation*}
The first row in the restriction represents the mixed-type rule, translated as shown
in Eq.\ \eqref{eq:mixedit}. Row six connects the indicator variable $\ell$
with the numerical edit in the consequent of
$t\in\{\textrm{sp}\}\Rightarrow(p=0)$.  Rows two, three and four represent the
numerical edits limiting values of $p$ and $c$. Row five forces $t$ to have only one 
value and row seven connects the value of $t$ with that of $\Delta_t$. Finally, 
rows eight to eleven connect the numerical variables with the corresponding
change indicators.
\hfill\qed
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical stability issues}
\label{sec:linedits:num}
An error localisation problem, in its original formulation, is an optimization
problem over $n$ decision variables that indicate which variables in a record
should be adapted. Depending on the type of rules, its reformulation as a mip
problem adds at least $n$ variables and $2n$ restrictions. Moreover, the
reformulation as a mip problem introduces a constant $M$, the value of which
has no mathematical significance but for which a value must be chosen in
practice. Because of limitations in machine accuracy, typically on the order of
$10^{-16}$, the range of problems that can be solved is limited as well. In
particular, mip problems that involve both very large and very small numbers in
the objective function and/or the restriction matrix  may yield erroneous
solutions or become numerically unfeasible. Indeed, the manual of
\code{lp\_solve} \citep{berkelaar:2010} points out that `\emph{[...] to improve
stability, one must try to work with numbers that are somewhat in the same
range. Ideally in the neighborhood of 1.}' The following subsections point out
a number of sources of numerical instabilities and provides ways to handle
them.


\subsection{A short overview of mip-solving}
Consider a set of linear restrictions on numerical data of the form
$\mathbf{Ax}\leq\mathbf{b}$, where we assume $\mathbf{b}\geq\mathbf{0}$ and the
restrictions consist solely of inequalities ($\leq$). In practice, these
restrictions will not limit the type of linear rules covered by this
discussion, since it can be shown that all linear rules can be brought to this
form, possibly by introducing dummy variables [see \emph{e.g.}
\citet{schrijver:1998,bradley:1977}]. Furthermore, suppose we have a record
$\mathbf{x}^0\geq\mathbf{0}$ which doesn't obey the restrictions. The mip
formulation of the error localisation problem can be written as follows.
\begin{equation}
\begin{array}{l}
  \textrm{Minimize }f = \mathbf{w}^T\mathbf{\Delta}\\
  \textrm{s.t.}
  \left[\begin{array}{rr}
    \mathbf{A} & \mathbf{0}\\
    \mathbb{1} &-\mathbf{M}\\
   -\mathbb{1} &-\mathbf{M}\\
    \mathbf{0} &\mathbb{1}
  \end{array}\right]
  \left[\begin{array}{c}
    \mathbf{x}\\ \mathbf{\Delta}
  \end{array}\right]
  \leq
  \left[\begin{array}{c}
    \mathbf{b}\\ \mathbf{x}^0 \\ -\mathbf{x}^0\\ \mathbf{1}
  \end{array}\right],
\end{array}
\label{eq:numxx}
\end{equation}
and $\mathbf{x},\mathbf{\Delta}\geq\mathbf{0}$. Also, $\mathbb{1}$ denotes the
unit matrix, $\mathbf{1}$ a vector with all coefficients equal to $1$ and
$\mathbf{M}=\mathbb{1}M$. The last row is added to force $\mathbf{\Delta}\leq
\mathbf{1}$. This is necessary because we will initially treat the binary
variables $\Delta_j$ as if they are real numbers in the range $[0,1]$.

The \code{lp\_solve} library uses an approach based on the revised Phase I -
Phase II simplex algorithm to solve mip problems. In this approach every
inequality of Eq.\ \eqref{eq:numxx} is transformed to an equality by adding
dummy variables: each row $\mathbf{a}^T\mathbf{x}\leq b$ is replaced by
$\mathbf{a}^T\mathbf{x}+s = b$, with $s\ge0$. Depending on the sign of the
inequality, the extra variable $s$ is called a \emph{slack} or \emph{surplus}
variable. It is customary to combine this new set of restrictions and the
objective function in a single \emph{tableau} notation as follows.
\begin{equation}
\left[\begin{array}{r|rrrrrr|l}
1   & \mathbf{0}    &-\mathbf{w}^T  & \mathbf{0}   & \mathbf{0}   & \mathbf{0} &\mathbf{0}      & 0\\
\hline
0   & \mathbf{A}    &  \mathbf{0}    & \mathbb{1} & \mathbf{0} & \mathbf{0}   &\mathbf{0} &\mathbf{b}\\
0   & \mathbb{1}    & -\mathbf{M}    & \mathbf{0} & \mathbb{1} & \mathbf{0}   &\mathbf{0} &\mathbf{x}^0\\
0   & \mathbb{1}    &  \mathbf{M}    & \mathbf{0} & \mathbf{0} & -\mathbb{1}  &\mathbf{0} &\mathbf{x}^0\\ 
0   & \mathbf{0}    &  \mathbb{1}    & \mathbf{0} & \mathbf{0} & \mathbf{0}   &\mathbb{1} & \mathbf{1}\\ 
\end{array}\right].
\label{eq:tab1}
\end{equation}
%
The tableau represents a set of linear equalities and may therefore be
manipulated as such.  In fact, the simplex method is based on performing  a
number of cleverly chosen Gauss-Jordan elimination steps on the tableau. For a
complete discussion the reader is referred to one of the many textbooks
discussing it (e.g. \citet{bradley:1977}), but in short the Phase I - Phase II
simplex algorithm consists of the following steps.
\begin{description}
\item[Phase I:] Repeatedly apply Gauss-Jordan elimination steps (called
\emph{pivots}) to derive a decision vector that obeys all restrictions. 
A vector obeying all restrictions is called a \emph{basic solution}.
\item[Phase II:] Repeatedly apply pivots to move from the initial non-optimal
solution to the solution that minimizes the objective function $f$. 
\end{description}
%
In Phase I, a decision vector $(\mathbf{x},\mathbf{\Delta},\mathbf{s})$ (with
$\mathbf{s}$ the vector of slack and surplus variables) is derived that obeys
all restrictions. This is done by adding again extra variables where necessary
and then manipulating the system of equalities represented by the tableau so
that those extra variables are driven to zero. The binary variables
$\mathbf{\Delta}$ are first treated as if they were real variables. In the
appendix we show how an initial solution for Eq.~\eqref{eq:tab1} can be found,
here we just state the result of a Phase-I operation:
%
\begin{equation}
\left[\begin{array}{r|cccccc|l}
1 & \mathbf{w}^T\mathbf{M}^{-1} & \mathbf{0} & \mathbf{0}   & \mathbf{0}   & -\mathbf{w}^T\mathbf{M}^{-1} &\mathbf{0} & \mathbf{w}^T\mathbf{M}^{-1}\mathbf{x}^0\\
\hline
0 & \mathbf{A}     & \mathbf{0} & \mathbb{1}   & \mathbf{0}   & \mathbf{0}       &\mathbf{0} & \mathbf{b}\\
0 &2\mathbb{1}     & \mathbf{0} & \mathbf{0}   & \mathbb{1}   & -\mathbb{1}      &\mathbf{0} & 2\mathbf{x}^0\\
0 &\mathbf{M}^{-1} &\mathbb{1}  & \mathbf{0}   & \mathbf{0}   & -\mathbf{M}^{-1} &\mathbf{0} & \mathbf{M}^{-1}\mathbf{x}^0\\
0 &-\mathbf{M}^{-1}&\mathbf{0}  & \mathbf{0}   & \mathbf{0}   & \mathbf{M}^{-1}  &\mathbb{1} & \mathbf{1}-\mathbf{M}^{-1}\mathbf{x}^0 
\end{array}\right].
\label{eq:phaseI}
\end{equation}
%
This tableau immediately suggests a valid solution: it is easily confirmed by
matrix multiplication that the vector $(\mathbf{x},\mathbf{\Delta},\mathbf{s})
=
(\mathbf{0},\mathbf{x}^0\mathbf{M}^{-1},[\mathbf{b},2\mathbf{x^0},\mathbf{0},\mathbf{1}-\mathbf{M}^{-1}\mathbf{x}^0])$
obeys all restrictions. The above form of a tableau, where the restriction
matrix contains a (column permutation of) the unit matrix, the right-hand-side
has only non-negative coefficients, and the cost vector equals zero for the
columns above the unit matrix is called the \emph{canonical form}.

Now, a \emph{pivot} operation consists of the following steps:
\begin{enumerate}
\item Select a positive element $R_{ij}$ from the restriction matrix.
\item Multiply the $i$th row by $R_{ij}^{-1}$.
\item Subtract the $i$th row, possibly after rescaling, from all other rows of the tableau such that
their $j$th column equals zero.
\end{enumerate}
%
The result of a pivot operation is again a tableau in canonical form but with
possibly a different value for the cost function. The simplex algorithm proceeds
by selecting pivots that decrease the cost function until the minimum is reached
or the problem is shown to be unfeasible.

Up until this point, we have treated the binary variables $\mathbf{\Delta}$ as
if they were real variables, so the tableaux discussed above do not represent
solutions to our original problem which demands that all $\Delta_j$ are either
$0$ or $1$. In the \code{lp\_solve} library this is solved as follows. 
\begin{enumerate}
\item For each optimized value $\Delta_j^*$ test whether it is $0$ or $1$. If
all $\Delta_j^*$ are integer, we have a valid solution of objective value
$\mathbf{w}^T\mathbf{\Delta}$ and we are done.
\item For the first variable $\Delta_j^*$ that is not integer, create two
sub-models: one where the minimum value of $\Delta_j$ equals $1$ and one where
the maximum value of $\Delta_j$ equals $0$.
\item Optimize the two sub-models. If solutions exist, the result will contain
an integer $\Delta_j$.
\item For the sub-models that have a solution and whose current objective value
does not exceed that of an earlier found solution, return to step 1.
\end{enumerate}
%
The above \emph{branch-and-bound} approach completes this overview. The above
discussion of pivot and branch-and-bound operations is purely mathematical: no
choices have been made regarding issues like how to decide when the
floating-point representation of a value is regarded zero or how to handle
badly scaled problems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scaling numerical records}
In the mip formulation of error localisation over numerical records under
linear restrictions, Eq.~\eqref{eq:deltanum} restricts the search space around
the original value $x^0$ to $|x-x^0|\leq M$. This restriction may prohibit a
mip solver from finding the actual minimal set of values to adapt or even
render the mip-problem unsolvable. As an example, consider the following error
localisation problem on a two-variable record.
\begin{displaymath}
\left\{\begin{array}{l}
x_1 \geq x_2\\
\mathbf{x}^0 = (10^6,10^9).
\end{array}\right.
\end{displaymath}
Obviously, the record can be made to obey the restriction by multiplying
$x^0_1$ by $10^3$ or by dividing $x^0_2$ by the same amount. However, in
\code{editrules} the default value for $M=10^7<10^9-10^6$ which renders the
corresponding mip problem unsolvable. Practical examples where such errors
occur is when a value is recorded in the wrong unit of measure (\emph{e.g.} in
\euro{} instead of k\euro{}.).

It is therefore advisable to remove such unit-of-measure errors prior to error
localisation\footnote{Methods for detecting such errors exist, see for example
\cite{waal:2011}, Chapter 2.} and to express numerical records on a scale such
that all $|x^0|<M$.  Note that under linear restrictions (Eq.~\ref{eq:linedits})
one may always apply a scaling factor $k>0$ to a numerical record $\mathbf{x}$
by replacing $\mathbf{A}\mathbf{x}\leq \mathbf{b}$ with
$\mathbf{A}(k\mathbf{x})\leq k\mathbf{b}$. Indeed, in the above example, one
may replace $\mathbf{x}^0$ by $10^{-6}\mathbf{x}^0$ for the purpose of error
localisation. If $\mathbf{b}=\mathbf{0}$ and the coefficients of $\mathbf{x}$
do not vary over many orders of magnitude, such a scaling will suffice to
numerically stabilize the mip problem. 

%
\subsection{Setting numerical threshold values}
On most modern computer systems real numbers are represented in
\citet{ieee:2008} double precision format. In essence, real numbers are 
represented as rounded-off fractions so arithmetic operations on such numbers
always result in loss of precision and round-off errors. For example, even though
mathematically we have $0.7-0.5-0.2=0$ exactly, in the floating point
representation (denoted $\fl(\cdot)$) we have
$\fl(0.7)-\fl(0.5)-\fl(0.2)\not=\fl(0)$. In fact, the difference is about
$0.56\cdot10^{-16}$ in this case. 

This means that in practice one cannot rely on equality tests to determine
whether two floating point numbers are equal.  Rather, one assumes that two
numbers $v$ and $w$ must be equal when $|\fl(v)-\fl(w)|$ is smaller than a
predefined tolerance. For this reason \code{lp\_solve} comes with a number of
predefined tolerances that have default values but may be altered by the user.

The tolerances implemented by \code{lp\_solve} are summarized in
Table~\ref{tab:tols}. The value of \code{epspivot} is used to determine whether
an element of the restriction matrix is positive so it may be used as a
pivoting element. Its default value is $2\cdot10^{-7}$, but note that after
Phase I, our restriction matrix contains elements on the order of
$M^{-1}=10^{-7}$. For this reason, the value of \code{epspivot} is lowered in
\code{editrules} by default, but users may override these settings again. For
the same reason, the value of \code{epsint}, which determines when a value for
one of the $\mathbf{\Delta}_j$ can be considered integer is lowered in
\code{editrules} as well.  The other tolerance settings of \code{lp\_solve}: 
\code{epsb} (to test if the right-hand-side of the restrictions
differ from $0$), \code{epsd} (to test if two values of the objective function
differ), \code{epsel} (all other values) and \code{mip\_gap} (to test whether a
bound condition has been hit in the branch-and-bound algorithm) have not been altered.

The limited precision inherent to floating point calculations imply that
computations get more inaccurate as the operands differ more in magnitude. For
example, on any system that uses double precision arithmetic the difference
$\fl(1)-\fl(10^{-17})$ is indistinguishable from $\fl(1)$. This then, leads to
two contradictory demands on our translation of an error localisation problem
to a mip problem. On one hand, one would like to set $M$ as large as possible
so the ranges $x^0_j\pm M$ contain a valid value of $x_j$. On the other hand,
large values for $M$ may imply that mip problems such as Eq.~\eqref{eq:numxx}
become numerically inaccurate. 

In practice, the tableau used by \code{lp\_solve} will not be exactly the same
as represented in Eq.~\eqref{eq:phaseI}. Over the years, many optimizations and
heuristics have been developed to make solving linear programming problems fast
and reliable, and several of those optimizations have been implemented in
\code{lp\_solve}. However, the tableau of Eq.~\eqref{eq:phaseI} does
fundamentally show how numerical instabilities may occur: the tableau
simultaneously contains numbers on the order of $M^{-1}$ and on the order of
$\mathbf{x}^0$. It is not at all unlikely that the two differ in many orders of
magnitude.

The above discussion suggests the following rules of thumb to avoid numerical
instabilities in error localisation problems.
\begin{enumerate}
\item Make sure that elements of $\mathbf{x}^0$ are expressed in units such
that $\mathbf{A}$, $\mathbf{b}$ and $\mathbf{x}$ are on the order of 1 wherever
possible.
\item Choose a value of $M$ appropriate for $\mathbf{x}^0$.
\item If the above does not help in stabilizing he problem, try lowering the
numerical constants of Table \ref{tab:tols}.
\end{enumerate}

In our experience, the settings denoted in Table \ref{tab:tols} have performed
well in a range of problems where elements of $\mathbf{A}$ and $\mathbf{b}$ are
on the order of $1$ and values of $\mathbf{x}^0$ are in the range $[1,10^8]$.
However, these settings have been made configurable so users may choose their
own settings as needed.





\begin{table}
\begin{threeparttable}
\caption{Numerical parameters for mip-based error localisation.}
\label{tab:tols}
\begin{tabular}{lrrl}
\hline
&\multicolumn{2}{c}{Default value} \\
\cline{2-3}
Parameter       & \code{lp\_solve}  & \code{editrules} & meaning \\
\hline
\code{M}       & \multicolumn{1}{c}{$-$}& $10^7$     & set bounds so $\mathbf{x}\in\mathbf{x}^0\pm M$\\
\code{eps}     & \multicolumn{1}{c}{$-$}& $10^{-3}$  & translate $x<0$ to $x\leq\varepsilon$\\
\code{epspivot}& $2\cdot10^{-7}$      & $10^{-15}$ & test if pivot element $R_{ij}>0$ \\
\code{epsint}  & $10^{-7}$            & $10^{-15}$ & test if $\Delta_j\in\mathbb{N}$ \\
\code{epsb}    & $10^{-10}$           & $10^{-10}$ & test if $b_i>0$ \\
\code{epsd}    & $10^{-9}$            & $10^{-9}$  & test if obj. values $|f-f'|>0$ during simplex \\
\code{epsel}   & $10^{-12}$           & $10^{-12}$ & test if other numbers $\neq0$\\
\code{mip\_gap}& $10^{-11}$           & $10^{-11}$ & test if obj. values $|f-f'|>0$ during B\&B\\
\hline
\end{tabular}
\end{threeparttable}
\end{table}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION IV: USAGE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Usage}
In the \code{editrules} package, edits can be defined with the \code{editset}
function. For example, the command
\begin{rcode}
<<tidy=FALSE>>=
E <- editset(expression(
   x + y == z
  , if ( x > y ) y > 0
))
@
\end{rcode}
defines two edits on the variables $x$, $y$ and $z$ and stores them in an object
called \code{E}. Here, \code{E} is an object of type \code{editset} and it can
be used to store and manipulate linear (in)equality edits, edits on categorical
data as well as edits on mixed-type data. Besides \code{editset} there are
specialized functions called \code{editmatrix} and \code{editarray} which can
be used to define rules on purely numerical or purely categorical data
respectively. Edits are defined in basic \code{R} syntax; one may use
multiplication, addition, \code{if}-\code{else} statements, logical operators
and the \code{\%in\%} operator for set inclusion on categorical variables. 

Besides defining rules on the command line, as in the example above,
one may store the rules in a text file and read the rules into \R{} using
the \code{editfile} function.
\begin{rcode}
<<eval=FALSE>>=
E <- editfile('myedits.txt')
@
\end{rcode}
Here, \code{myedits.txt} is the name of a textfile containing the edits. The
resulting object is by default of class \code{editset}. If the extra argument
\code{type="num"} or \code{type="cat"} is passed, only numerical or
categorical edits are read from the file. For a further discussion of the
functions mentioned here we refer the reader to the technical manual. Also see 
the papers of \cite{loo:2011b, loo:2012a} for a precise description of the 
edit-definition syntax.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Error localization}
The the main interface to error localisation functionality is the
\code{localizeErrors} function.  The function accepts editrules in the form
an \code{editset}, \code{editmatrix} or \code{editarray} object, and a data set
in the form of a \code{data.frame}. By default the function localizes errors
using the branch-and-bound algorithm. One may switch to the MIP-based approach
by setting the parameter \code{method="mip"} as in the following example.
\begin{rcode}
<<num example>>=
E <- editmatrix("x <= y")
dat <- data.frame(x=c(10,3), y=c(1,5))
el <- localizeErrors(E, dat, method="mip")
@
\end{rcode}
The object returned by \code{localizeErrors} contains the error locations as
well as some details on how the algorithm ran. The error locations are stored
in a boolean array called \code{adapt} which can be accessed as follows.
\begin{rcode}
<<>>=
el$adapt
@
\end{rcode}
Here, the array has dimension $2\times2$ since the input data set consisted of
a two records with two variables. Here, the result indicates for the first record
that by altering the value for $x$, the record can be corrected to obey the edit rule stored in
\code{E}. For the second record, no alterations are necessary.

Details on the error localisation procedure are stored in a \code{data.frame}
called \code{status}, which can be accessed as follows.
\begin{rcode}
<<>>=
el$status
@
\end{rcode}
The status \code{data.frame} contains one record of information for each record
in the input data. The column \code{weight} contains the value of the objective
function as defined by Eq.~\eqref{eq:objfun}. The time it took to perform the
calculation is subdivided into user, system and elapsed time, where the latter
corresponds to the actual time that has passed on the clock.  The boolean
variable (\code{maxDurationExceeded})  indicates whether the time limit for
finding a solution was exceeded.  There are two status columns that have no
relevance when the error localisation method is \code{mip}.  First, the
indicator \code{memfail} can only be set to \code{true} when the
branch-and-bound algorithm is used. It indicates that perhaps the optimal
solution could not be found because of memory limitations.  Second, and more
importantly, the \code{degeneracy} parameter is not set when
\code{method="mip"}. This parameter indicates how many equivalent solutions
there are to each error localisation problem.  Contrary to the branch-and-bound
method for error localisation, the \code{mip}-based approach does not return
this information.

The output of \code{localizeErrors} can be controlled with two parameters.
Most importantly, positive weights for each variable and optionally
for each record can be set; variables with lower weights attached to them are
more likely to be part of a chosen solution which is otherwise degenerate.
Furthermore, a maximum search time for each solution can be specified, the
default setting being 10 minutes. Finally, the optional parameter \code{lpcontrol}
may contain a list of parameters to be passed to \code{lpSolveAPI}. The default settings
can be listed as follows
\begin{rcode}
<<>>=
options("er.lpcontrol")
@
\end{rcode}
These options are precisely the values that differ from \code{lpSolve}'s
default settings listed in Table \ref{tab:tols}. Changing or adding options can
be done either by passing the \code{lpcontrol} parameter to
\code{localizeErrors}, in which case it is only used for the current error
localisation problem. To adapt a parameter for the remainder of the running
R-session or until the option is reset one can use the \code{options}
function. For example, to alter the \code{epsb} parameter through localizeErrors,
one may use either of the two calls below.
\begin{rcode}
<<eval=FALSE,tidy=FALSE>>=
localizeErrors( E, dat, lpcontrol = c(epsb=1e-12, options("er.lpcontrol")) )
options( er.lpcontrol = c(options("er.lpcontrol"), epsb=1e=12) )
@
\end{rcode}
The important thing to note is that it is up to the user to merge new options
with existing ones since \code{lpcontrol} completely overwrites the default settings.
A precise description of possible options is also given in the reference manual of
\code{lpSolveApi}.




%When \code{method=mip} is specified \code{localizeErrors} internally 
%uses for each record the function \code{errorLocalizer\_mip}.
%Unlike {\em branch and bound} this is not a \code{backtracker} object
%\citep[see][]{jonge:2011}. \code{errorLocalizer\_mip} writes the constraints 
%and values into a mip problem,
%feeds it into the \code{lpSolveApi} and formats the resulting output.
%<<num mip>>=
%el <- errorLocalizer_mip(E, list(x=10,y=1))
%ls.str(el)
%@
%Both \code{localizeErrors} as \code{errorLocalizer\_mip} can be executed with an 
%\code{editmatrix}, \code{editarray} or \code{editset} object.
%
%Note in the example above that the {\em mip} solver returns two extra properties that are
%not available in {\em branch and bound}: \code{lp} and \code{x\_feasible}.
%When it has found a solution (which is usually the case), a set of feasible x 
%values is available. It can be helpful to have an explicit solution, but note 
%that it is in general not unique and that the feasible values 
%lie on the boundary of a convex solution region. 
%In many cases it is better to use a proper imputation method to generate
%sensible values for the variables that need to be adjusted.
%Secondly \code{errorlocalizer\_mip} returns the \code{lp} object. This object is 
%a \code{lpSolveApi} object and can be further manipulated or written to disk.
%For more information consult the manual of \code{lpSolveApi}
%(\ref{lpSolveAPI:2011}).
%
%An advantage of the branch and bound algorithm is that it can generate all optimal
%solutions for a given record.  The mip implementation only generates one of the 
%best. In \code{editrules} this is implemented by adding a small uniform 
%perturbation to the weights, so mip chooses one of the best at random. 
%This is identical 
%to the default behavior of \code{localizeErrors} for \code{method='bb'}.
%


\subsection{Fine-grained control options}
For users who wish to exert more fine-grained control on the MIP-solver or who
wish to interface \code{editrules} with another MIP-solving engine, two lower-level
functionalities have been developed.

The first is \code{errorLocalizer\_mip}. This function takes
a set of edit rules in the form of an \code{editset}, \code{editmatrix}
or \code{editarray} and a single data record in the form of a named list.
\begin{rcode}
<<num mip>>=
L <- errorLocalizer_mip(E, list(x=10,y=1))
@
\end{rcode}
Here, \code{errorLocalizer\_mip} translates the error localisation problem for
a single record to a MIP problem, feeds it to \code{lpSolveAPI} and returns all
the results in a list. The list contains two extra pieces of information not
available in the output. The first is a parameter called \code{x\_feasible}, containing
a record that actually obeys all the edit rules.
\begin{rcode}
<<>>=
L$x_feasible
@
\end{rcode}
The second parameter is called \code{lp}. This is an object of class
\code{lpExtPtr} which points to an object of \code{lpSolveApi}, stored outside
of \R{}'s memory. It contains precise information on the definition of the MIP
problem as interpreted by \code{lpSolveAPI} program. It can be manipulated or
exported to a text file using \code{write.lp} of the \code{lpSolveAPI} package.

The second functionality entails the functions \code{\code{as.mip}} and
\code{as.lp.mip}.  The function \code{as.mip} allows users to translate the
combination of a set of editrules and a data record to a MIP problem.
\begin{rcode}
<<>>=
mip <- as.mip(E,list(x=10,y=1))
print(mip)
@
\end{rcode}
The object returned by \code{as.mip} can be used to inspect how
\code{editrules} translates an error localisation problem to a MIP problem. The
interested reader may want to compare the above representation with
Eqs.~\eqref{eq:objfun}, \eqref{eq:zlin} and \eqref{eq:deltanum}.

This representation of the MIP problem can be translated to a form that is
suited for solving with \code{lpSolveAPI}.
\begin{rcode}
<<>>=
lp <- as.lp.mip(mip)
@
\end{rcode} 
The \code{lp} object can directly be used as input for \code{lpSolveAPI} or
written to disk with \code{write.lp} as follows.
\begin{rcode}
<<eval=FALSE>>=
write.lp(lp,file="myLPfile.lp")
@
\end{rcode}
This command produces a text file that is written in a syntax understood by the
\code{lp\_solve} commandline program.  \code{lp\_solve} also has facilities to
translate this syntax to other formats, suitable for commercial LP solvers, for
example.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% benchmarks
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Benchmarks}

The \emph{branch and bound} and \emph{mip} method for error localization 
have a different performance both in computing time as well in memory usage.
The performance of error 
localization methods depends highly on the number of variables, the number of errors,
the number of restrictions and the order of the variables.

To compare the performance of {\em branch and bound} and the {\em mip} method, 
we conducted benchmarks for the three types of restrictions using edits with 
synthetic data. Each ot the benchmarks is constructed to reflect 
properties of a typical error localization problem in official statistics practice.
 
\subsection{Linear restrictions}
The synthetic edits used in this benchmark from a balance system.
This type of edits
are found frequently in business statistics and energy statistics, where the main
variables a composites of other variables.
\begin{example}
The turnover of a car sale company is the sum of the turnovers of the regions where
this company is active.
\end{example}

The numerical
variables form a binary tree where the value of a node is the sum of its child node
values. The value of the top node is restricted to be non-negative and always at least as
large as any other value.
\begin{eqnarray*}
x_1 & =    & x_2 + x_3 \\
&\vdots & \\
x_n &=     & x_{2n} + x_{2n+1} \\
x_1 & \geq & 0 \\
x_1 & \geq & x_i
\end{eqnarray*}
A solution to this system is the zero vector $(x_1, \ldots, x_{2n+1}) = (0,\ldots,0)$.
An error is introduced in the data by setting a $x_i$ to $-1$. 

The benchmark was run for balance systems of size $1$ up to $101$. For each system
records with $1$ to $10$ errors were created. Since \code{bb} is highly dependent on the 
order of the (faulty) variables, records with errors at the beginning, middle or end
were added to the test set.

The time for localizing the errors 
using \code{localizeErrors} with \code{method} set to \code{"bb"} and \code{"mip"}
was measured.

<<numbench, echo=FALSE,warning=FALSE, fig.height=3.5, fig.cap="Linear edits, each line is a different number of errors">>=
library(ggplot2)
theme_cbs <- theme_bw() + 
             theme( panel.grid.major.x = element_blank()
                  , axis.title.y = element_text(angle=0)
                  )
data("benchmip_balance")
sdat <- subset(benchmip_balance, errorloc=="middle")
#sdat <- within(sdat, elapsed[weight == 0] <- 200)
ann <- subset(sdat, elapsed <= 150 & method=="bb")
ann <- aggregate(elapsed ~ nerrors, data=ann, FUN=max)
ann <- merge(ann, sdat)
qplot( data=sdat, x=nvar, y=elapsed, color=method, group=paste0(method, nerrors), geom=c("point","line"), 
       ) +
       labs(title="", x="number of variables", y="", color="method") + 
       theme_cbs + theme() + 
       scale_y_continuous(labels=function(x){paste0(x,"s")}) + coord_cartesian(ylim=c(0,150)) + 
       annotate("text", x=ann$nvar+1,y=ann$elapsed+1, label=ann$nerrors, size=2.5)
@
Figure \ref{fig:numbench} shows the performance of error localization for increasing
number of variables and increasing number of errors with errors injected in 
the middle. \code{mip} performance is vastly
superior for problem size of $100$ variables and $10$ errors. With \code{"bb"} no 
(optimal) solution is found within 10 minutes for problems with 
more then 50 variables en 4 errors. In contrast \code{"mip"} 
solves the problem with $100$ variables an $10$ errors within seconds.
\code{bb} for up-to $2$ errors and 50 variables is reasonable. Note that the 
performance of \code{bb} in practice often is better than in this test: when
using confidence \code{weight}s the variables with errors are likely
to be visited first and this increases performance for \code{bb} considerably.

\subsection{Categorical restrictions}
Pure categorical edits form a web of interconnected edits. 
\begin{example}
If you are married, then you are an adult.
\end{example}
To reflect this connectivity the categorical benchmark creates a 
chain of edits of the form:
\begin{eqnarray*}
v_{1}&\texttt{==}&\texttt{TRUE}\\
\texttt{if (}v_{1}\texttt{==TRUE) }v_{2} &\texttt{==}&\texttt{TRUE}\\
&\vdots&\\
\texttt{if (}v_{n-1}\texttt{==TRUE) }v_{n} &\texttt{==}&\texttt{TRUE}
\end{eqnarray*}
<<catbench, echo=FALSE, fig.height=3.5, warning=FALSE, fig.cap="Categorical edits: each line represents a different number of errors.">>=

data(benchmip_categorical)
dat <- benchmip_categorical
sdat <- subset(dat, errorloc=="middle")
ann <- subset(sdat, elapsed <= 150 & method=="bb")
ann <- aggregate(elapsed ~ nerrors, data=ann, FUN=max)
ann <- merge(ann, sdat)
qplot( data=sdat, x=nvar, y=elapsed, color=method, group=paste0(method, nerrors), geom=c("point","line"), 
       ) +
       labs(title="", x="number of variables", y="", color="") + 
       theme_cbs + theme() + 
       scale_y_continuous(labels=function(x){paste0(x,"s")})+ coord_cartesian(ylim=c(0,150)) + 
       annotate("text", x=ann$nvar+0.5,y=ann$elapsed+1, label=ann$nerrors, size=2.5)
@
The only solution to this system is the vector $(v_1,\ldots,v_n)=$\code{(TRUE, ..., TRUE)}.
An error in the data can be introduced by setting on of the $v_i$'s to \code{FALSE}.

The benchmark was run for balance systems of size $1$ up to $50$. For each system
records with $1$ to $10$ errors were created. Since \code{bb} is highly dependent on the 
order of the (faulty) variables, records with errors at the beginning, middle or end
were added to the test set.

Figure \ref{fig:catbench} shows the performance of error localization for increasing
number of variables and increasing number of errors with the errors inject in the middle
\code{mip} performance is vastly
superior for problem size of $50$ variables and $10$ errors. With \code{"bb"} no 
(optimal) solution is found within 10 minutes for problems with 
more then 30 variables en 4 errors. In contrast \code{"mip"} 
solves the problem with $50$ variables an $10$ errors within seconds.
\code{bb} for up-to $2$ errors and 50 variables is reasonable. Note that the 
performance of \code{bb} in practice often is better than in this test: when
using confidence \code{weight}s the variables with errors are likely
to be visited first and this increases performance for \code{bb} considerably.

\subsection{Mixed-type restrictions}
For mixed-type restrictions a set of edits of the following form was generated:
\code{if ($x_i \geq 0$) $x_{i+1} \geq 0$}, with the extra restriction 
$x_1 \geq 0$. Although all variables $x_i$ are numeric, the use
of conditional edits makes these restrictions mixed edits. Since conditions
are modeled internally with a boolean variable, the resulting error localization
problem is technically equivalent with restrictions on a mixed-type record.
This type of restriction is very common in business and social statistics. 
\begin{example}
If in a business record the variable salary is bigger than zero, then the number
of employees must be higher than zero (and vice versa).
\end{example}

The mixed-type restriction benchmark therefore creates a chain of 
conditional restrictions. 
\begin{eqnarray*}
&& x_1 \geq 0\\
\texttt{if (}x_{1} \geq 0 \texttt{)} && x_{2} \geq 0 \\
&\vdots\\
\texttt{if (}x_{N-1} \geq 0 \texttt{)} && x_{N} \geq 0 \\
\end{eqnarray*}
A solution to this system is the vector \code{($0$, ..., $0$}.
We can introduce an error by setting a $x_i$
to $-1$. 
This benchmark was run with the number of variables up to 50 and the number of errors
up to 10.
%\todo[inline]{Mixed data graphs/data is not yet correct! Benchmark returns solutions
%with weight less then nerrors.}
<<mixedbench, echo=FALSE, fig.height=3.5, warning=FALSE, fig.cap="Mixed type edits: each line represents a different number of errors.">>=
data(benchmip_mixed2)
dat <- benchmip_mixed2
sdat <- subset(dat, errorloc=="middle")
sdat <- within(sdat, {elapsed[weight==0] <- 200})
ann <- subset(sdat, elapsed <= 150 & method=="bb")
#ann <- subset(sdat, method=="bb")

ann <- aggregate(elapsed ~ nerrors, data=ann, FUN=max)
ann <- merge(ann, sdat)
qplot( data=sdat, x=nvar, y=elapsed, color=method, group=paste0(method, nerrors), geom=c("point", "line"), 
       ) +
       labs(title="", x="# variables", y="", color="") + 
       theme_cbs + theme() + 
       scale_y_continuous(labels=function(x){paste0(x,"s")})+ coord_cartesian(ylim=c(0,150)) +
       annotate("text", x=ann$nvar+0.5,y=ann$elapsed+1, label=ann$nerrors, size=2.5)
@

Solving the error localization problem for mixed type restrictions using 
{\emph branch and bound} is very 
memory (and time) consuming. For each if-statement \code{bb} splits off two copies
, one in which the condition is true and another one in which it is not true.
The performance of \code{mip} is again vastly superior to the \code{bb} version.
All benchmarks for \code{mip} are finished within seconds. In problems with more
than 35 variables, the solution time for \code{bb} rapidly increases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCLUSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
This paper describes the error localization algorithm for linear, categorical and
mixed-type records as mixed integer problems. We derived a new formulation for 
the Felligi Holt error localization problem that is a generalization of linear,
categorical as well as mixed-type edit rules.

The resulting mip problem can be solved with a mixed integer problem solver. However
if the data and coefficients are not of in the same order of size, 
numerical stability issues arise, due to numerical precision. We show when 
and how this happens and give directions in handling such issues. 

Benchmarks on typical synthetic but realistic problems show that performance of 
the mip algorithm is vastly superior to the branch and bound in \code{editrules}.
% For most problems mip will generate a good solution. However for problems where
% coefficients or values exceed the range of $[10^{-3} - 10^7]$ the user has to rescale
% the problem: otherwise the mip solver will become unstable.

\bibliographystyle{chicago}
\bibliography{editrules}

\appendix
\section{Derivation of Equation \ref{eq:phaseI}}
In the Phase I Phase II simplex method, phase I is aimed to derive a valid
solution which is then iteratively updated to an optimal solution in Phase II.
Here, we derive a Phase I solution, specific for error localisation problems.


Recall the tableau of Eq.~\eqref{eq:tab1}; for clarity, the top row indicates to what variables
the columns of the tableau pertain.
%
\begin{displaymath}
\left[\begin{array}{r|rrrrrr|l}
f   & \mathbf{x}^T  &\mathbf{\Delta} & \mathbf{s}_x &\mathbf{s}_+  & \mathbf{s}_- &\mathbf{s}_\Delta\\
1   & \mathbf{0}    &-\mathbf{w}^T   & \mathbf{0}   & \mathbf{0}   & \mathbf{0} &\mathbf{0}      & 0\\
\hline
0   & \mathbf{A}    &  \mathbf{0}    & \mathbb{1} & \mathbf{0} & \mathbf{0}   &\mathbf{0} &\mathbf{b}\\
0   & \mathbb{1}    & -\mathbf{M}    & \mathbf{0} & \mathbb{1} & \mathbf{0}   &\mathbf{0} &\mathbf{x}^0\\
0   & \mathbb{1}    &  \mathbf{M}    & \mathbf{0} & \mathbf{0} & -\mathbb{1}  &\mathbf{0} &\mathbf{x}^0\\ 
0   & \mathbf{0}    &  \mathbb{1}    & \mathbf{0} & \mathbf{0} & \mathbf{0}   &\mathbb{1} & \mathbf{1}\\ 
\end{array}\right].
\label{eq:tab1again}
\end{displaymath}
%
Here, the $\mathbf{s}_i$ are slack or surplus variables, aimed to write the
original inequality restrictions as equalities. The $\mathbf{s}_x$ are used to
rewrite restrictions on observed variables, the $\mathbf{s}_\pm$ to write the
upper and lower limits on $\mathbf{x}$ as equalities and $\mathbf{s}_\Delta$ to
write the upper limits on $\mathbf{\Delta}$ as equalities.

Observe that the above tableau \emph{almost} suggests a trivial solution. If we choose
$\mathbf{s}_x=\mathbf{b}$, $\mathbf{s}_\pm=\pm\mathbf{x}^0$ and $\mathbf{s}_\Delta=\mathbf{1}$,
we may set $(\mathbf{x},\mathbf{\Delta})=\mathbf{0}$. However, recall that we demand all variables
to be non-negative so $\mathbf{s}_-$ may not be equal to ${-\mathbf{x}^0}$. To resolve this
problem, we introduce a set of \emph{artificial} variables $\mathbf{a}_-$ and extend the
restrictions involving $\mathbf{s}_-$ as follows:
\begin{displaymath}
\mathbf{x} + M\mathbf{1} - \mathbf{s}_- + \mathbf{a}_- = \mathbf{x}_0.
\end{displaymath} 
This yields the following tableau.
\begin{displaymath}
\left[\begin{array}{r|rrrrrrr|l}
f   & \mathbf{x}^T  &\mathbf{\Delta} & \mathbf{s}_x &\mathbf{s}_+  & \mathbf{s}_- &\mathbf{s}_\Delta&\mathbf{a}_-\\
1   & \mathbf{0}    &-\mathbf{w}^T   & \mathbf{0}   & \mathbf{0}   & \mathbf{0}   &\mathbf{0} &\mathbf{0}     & 0\\
\hline
0   & \mathbf{A}    &  \mathbf{0}    & \mathbb{1} & \mathbf{0} & \mathbf{0}   &\mathbf{0} &\mathbf{0} &\mathbf{b}\\
0   & \mathbb{1}    & -\mathbf{M}    & \mathbf{0} & \mathbb{1} & \mathbf{0}   &\mathbf{0} &\mathbf{0} &\mathbf{x}^0\\
0   & \mathbb{1}    &  \mathbf{M}    & \mathbf{0} & \mathbf{0} & -\mathbb{1}  &\mathbf{0} &\mathbb{1} &\mathbf{x}^0\\ 
0   & \mathbf{0}    &  \mathbb{1}    & \mathbf{0} & \mathbf{0} & \mathbf{0}   &\mathbb{1} &\mathbf{0} & \mathbf{1}\\ 
\end{array}\right].
\label{eq:tab1again}
\end{displaymath}
%
The essential point to note is that the tableau now contains a unit matrix in
columns 4, 5, 7 and 8, so choosing $\mathbf{s}_x=\mathbf{b}$,
$\mathbf{s}_+=\mathbf{x}^0$, $\mathbf{s}_\Delta=\mathbf{1}$ and
$\mathbf{a}_-=\mathbf{x}^0$, and
$(\mathbf{x},\mathbf{\Delta},\mathbf{s}_-)=\mathbf{0}$ is a solution obeying
all restrictions. The artificial variables have no relation to the original
problem, so we want them to be zero in the final solution. Since the tableau
represents a set of linear equalities, we are allowed to multiply rows with a
constant and add and subtract rows. If this is done in such a way that we are
again left with a unit matrix in part of the columns, a new valid solution is
generated. Here, we add the fourth row to row three and subtract $\mathbf{M}^{-1}$
times the fourth row from the last row. Row four is multiplied with $\mathbf{M}^{-1}$.
This gives
\begin{displaymath}
\left[\begin{array}{r|rrrrrrr|l}
f   & \mathbf{x}^T   &\mathbf{\Delta}& \mathbf{s}_x &\mathbf{s}_+  & \mathbf{s}_- &\mathbf{s}_\Delta&\mathbf{a}_-\\
1   & \mathbf{0}     &-\mathbf{w}^T  & \mathbf{0}   & \mathbf{0}   & \mathbf{0}   &\mathbf{0} &\mathbf{0}     & 0\\
\hline
0   & \mathbf{A}     &  \mathbf{0}   & \mathbb{1} & \mathbf{0} & \mathbf{0}       &\mathbf{0} &\mathbf{0} &\mathbf{b}\\
0   & 2\mathbb{1}    &  \mathbf{0}   & \mathbf{0} & \mathbb{1} & \mathbf{0}       &\mathbf{0} &\mathbb{1} &2\mathbf{x}^0\\
0   & \mathbf{M}^{-1}&  \mathbb{1}   & \mathbf{0} & \mathbf{0} & -\mathbf{M}^{-1} &\mathbf{0} &\mathbf{M}^{-1} &\mathbf{M}^{-1}\mathbf{x}^0\\ 
0   &-\mathbf{M}^{-1}&  \mathbf{0}   & \mathbf{0} & \mathbf{0} & \mathbf{M}^{-1}  &\mathbb{1} &-\mathbf{M}^{-1} 
                                                                                                    & \mathbf{1}-\mathbf{M}^{-1}\mathbf{x}^0\\ 
\end{array}\right].
\label{eq:tab1again}
\end{displaymath}
%
This almost gives a new valid solution, except that the first row, representing the objective function has not
vanished at the third column. However, we may re-express the objective function in terms of the other variables. Namely
using row four, we have
\begin{displaymath}
\mathbf{w}^{T}
\mathbf{\Delta} =\mathbf{w}^T \mathbf{M}^{-1}\mathbf{x}^0  +\mathbf{w}^T\mathbf{M}^{-1}(\mathbf{s}_- -\mathbf{x} -\mathbf{a}_-).
\end{displaymath}
Substituting this equation in the top row of the tableau shows that the
solution $\mathbf{\Delta}=\mathbf{M}^{-1}\mathbf{x}^0$,
$\mathbf{s}_x=\mathbf{b}$, $\mathbf{s}_+=2\mathbf{x}^0$ and
$\mathbf{s}_\Delta=\mathbf{1}-\mathbf{M}^{-1}\mathbf{x}^0$, and
$(\mathbf{x},\mathbf{s}_-,\mathbf{a}_-)=\mathbf{0}$ represents a valid solution.
Since the artificial variables have vanishing values, we may now delete the corresponding
column, and arrive at the tableau of Eq.~\eqref{eq:phaseI}.

\end{document}

