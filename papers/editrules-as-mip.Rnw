%\VignetteIndexEntry{Error localization for numerical and categorical edits as a mixed integer problem}
\documentclass[10pt, fleqn]{cbsreport}
%\usepackage{inconsolata}
\usepackage[english]{babel}
\usepackage{amsmath, amsthm}
\usepackage{bbm} % mathbb numbers
\usepackage{array}
\usepackage{natbib}
\usepackage{threeparttable}
\usepackage{makeidx}
\usepackage{todonotes}
\makeindex

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\Lor}{\lor}
\DeclareMathOperator*{\Land}{\land}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\R}{\code{R}} %% call as \R{} to avoid extra spaces.

%\renewcommand{\mathbf}[1]{\ensuremath{\mathbf{#1}}}

\usepackage{float} 
\floatstyle{boxed}
\newfloat{Rcode}{t!}{rco}
\floatname{Rcode}{{\rm Figure}}


% stimulate latex to put multiple floats on a page.
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{3}
\setcounter{dbltopnumber}{2}
\renewcommand{\topfraction}{.9}
\renewcommand{\textfraction}{.1}
\renewcommand{\bottomfraction}{.75}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\dblfloatpagefraction}{.9}
\renewcommand{\dbltopfraction}{.9}


\hyphenation{
}

<<init, echo=FALSE, results='hide', message=FALSE>>=
library(editrules)
#library(xtable)
@

\title{Error localization as a mixed integer problem with the {\tt editrules} package}
\author{Edwin de Jonge and Mark van der Loo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle
\begin{abstract}
Error localization is the problem of finding out which fields in raw data
records contain erroneous values. The \code{editrules} extension package for
the \R{} environment for statistical computing was recently extended with a module
that allows for error localization based on a mixed integer programming
formulation (mip). In this paper we describe the mip formulation of the error
localization problem for the case of numerical, categorical, or mixed numerical
and categorical datasets. The new module is benchmarked against a previously
available module, which is based on a branch-and-bound approach. The benchmark
shows that the mip-based approach is significantly faster and has a smaller memory
footprint. Trade-offs between the branch-and-bound and mip approaches are discussed
as well.
\end{abstract}

\newpage
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Analyses of data are often hindered by occurrences of incomplete or
inconsistent raw data records.  The process of locating and correcting such
errors is referred to as {\em data editing}, and it has been estimated that
National Statistics Institutes may spend up to 40\% of their resources on this
process \citep{waal:2011}. Moreover, data are often required to obey many
cross-variable consistency rules which significantly complicate the data
editing process. Indeed, \citet{winkler:1999} mentions practical cases
(household surveys) where records have to obey 250, 300 or even 750
user-defined interrelated consistency rules.  For these reasons, considerable
attention is paid to the development of data editing methods that can be
automated. 

\subsection{Error localization}
Automated (and even manual) data editing strategies typically consist of three steps:
\begin{enumerate}
\item Find out which rules (expectations) a record violates;
\item Find out which fields in a record cause those violations;
\item Replace the values in those fields with better estimates, such that no rules
are violated anymore.
\end{enumerate}
The second step is usually referred to as the \emph{error localization}
problem, which is the focus of the current paper.  Although it is widely
recognized that data editing is a necessary step in the statistical process,
the amount of changes made to the data should obviously be minimized to avoid
introducing bias in estimations based on the edited data. This then, led to the
development of the following minimization problem.
%
\begin{quote}
Given a record of $n$ variables, subject to a number of possibly multivariate
consistency rules. Find the smallest (weighted) subset of fields, such that their
original values can be replaced in such a way that no rules are violated anymore. 
\end{quote}
%
This minimization problem is named after \cite{fellegi:1976}, who first
formulated and solved the problem for the case of categorical data. Error
localization has been extensively discussed in literature\footnote{See
\citet{waal:2011} and references therein}, so we will suffice with a few
remarks. First, note that the minimization problem is possibly over a search
space of size $2^n$, rendering a brute-force approach that runs through all
possible solution candidates computationally infeasible\todo{'unfeasible'?}.
Second, the problem is
complicated by the occurrence of \emph{implied rules}. That is, the solution
set must not only allow the record to obey the original, user-defined set of
rules, but also rules that are logically or arithmetically implied by the
original set. To cope with these complications, several algorithmic approaches
have been developed, of which two are worth mentioning in this context:
the first is the branch-and-bound approach, developed by \cite{waal:2003a}.
The second is an approach based on mixed-integer programming (mip), described
in \cite{waal:2011}.

\subsection{The \code{editrules} package}
Over the past five to ten years, the \R{} statistical environment received a
surge in popularity and as a consequence it has been extended with many
packages that allow for statistical analyses of data. However, the number of
packages specifically aimed at data editing seems to be somewhat limited,
except possibly in the area of imputation.  The {\sf R} package {\sf editrules}
\citep{jonge:2011a} was developed to help to bridge the gap between raw data
retrieval and data analysis with {\sf R}.  The main purpose of the package is
to provide an easy and consistent interface to define data consistency rules
(often referred to as \emph{edit rules}) in \R{} and to confront them with
data. Furthermore, the package allows for basic rule manipulation (deriving new
rules, finding inconsistencies, \emph{etc.}) and for error localization
functionality.  As such, the package does not offer functionality
to correct data.  Rather, it is aimed at identifying the set of solutions to an
error correction problem: the second step mentioned in the data editing
strategy above. Previous developments of the package have been described
in \cite{jonge:2011,loo:2011b} and \cite{loo:2011a}.


The \code{editrules} package offers a fairly complete toolbox, allowing users
to work with numerical, categorical or mixed-type data editing rules. Up until
now, error localization was performed by an implementation of the
branch-and-bound algorithm described by \cite{waal:2003}. The main disadvantage
of this approach is that the branch-and-bound algorithm has $\mathcal{O}(2^n)$
worst-case time and memory complexity, where $n$ is the number of variables
occurring in a connected set of rules. Moreover, the branch-and-bound solver is
written in pure \R{}, making it intrinsically slower than a compiled language
implementation. The main advantages of this approach are the ease of
implementation and the opportunity for users to exert fine-grained control over
the algorithm.

As stated before, the error localisation problem can, under mild conditions, be
translated to a mixed-integer programming problem, which offers the opportunity
to reuse well-established results from the field of linear and mixed-integer
programming. Indeed, many advanced algorithms for solving such problems have
been developed and in many cases implementations in a compiled language are
available under a permissive license. In \code{editrules}, the solver of the
\code{lpsolve} library \citep{berkelaar:2004} is used through \R{}'s
\code{lpSolveAPI} package \citep{lpSolveAPI:2011}. The \code{lpsolve} library
is written in \code{ANSI} \code{C} and has been tried and tested extensively.

The strategy to solve error localisation problems through this library from 
\R{} therefore consists of translating the problem to a suitable mixed-integer
programming problem, feeding this problem to \code{lpSolveAPI} and translating
the results back to an error location. It is necessary
to distinguish between:
\begin{itemize}
  \item linear restrictions on purely numerical data,
  \item restrictions on purely categorical data, and
  \item restrictions on mixed-type data,
\end{itemize}
since for each data type a different translation strategy is necessary.

The main part of this paper will focus on how to translate these types of error
localisation problems to a mixed-integer formulation, paying attention to both
theoretical and practical details.  We will give examples of how users can
apply this functionality to their own problem  in \R{} and benchmark the new
mip-functionality against the existing branch-and-bound solution.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Error localization and mixed integer programming}
\label{sec:mipproblem}
%
A mixed integer programming problem is an optimization problem that can be
written in the form
\begin{equation}
\begin{array}{r}
\textrm{Minimize } \mathbf{c}^T\mathbf{z}; \\
\textrm{s.t. }\mathbf{Rz} \leq \mathbf{d},
\label{eq:mipmin}
\end{array}
\end{equation}
%
where $\mathbf{c}$ is a constant vector and $\mathbf{z}$ is a vector consisting
of real and integer coefficients. The inner product $\mathbf{c}^T\mathbf{z}$ is
referred to as the \emph{objective function}.  Furthermore, $\mathbf{R}$ is a
coefficient matrix and $\mathbf{d}$ a vector of upper bounds. Formally, the
elements of $\mathbf{c}$, $\mathbf{R}$ and $\mathbf{d}$ are limited to the rational
numbers \citep{schrijver:1998}, which is never a problem in practice since we
are always working with a computer representation of numbers. 

The name \emph{mixed-integer programming} stems from the fact that $\mathbf{z}$
contains continuous as well as integer variables. When $\mathbf{z}$ consists
solely of continuous or integer variables, Problem~\eqref{eq:mipmin} reduces
respectively to a \emph{linear} or an \emph{integer programming} problem.  An
important special case occurs when the integer coefficients of $\mathbf{z}$ may
only take values from \{0,1\}. Such variables are often called binary variables
or decision variables. It occurs as a special case since defining $\mathbf{z}$ to
be integer and applying the appropriate restrictions yields the same problem.

Mixed integer programming is well understood and several software packages
packages are available that implement efficient solvers.  Most mip software
allows for a broader (but equivalent) formulation of the mip problem, allowing
the set of restrictions to include inequalities as well as equalities. As a
side note, we mention that under equality constraints, solutions for the
integer part of $\mathbf{z}$ only exist when the equality constraints
pertaining to the integer part of $\mathbf{z}$ are \emph{totally
unimodular}\footnote{This means that every square submatrix of the coefficient
matrix pertaining to the integer part of $\mathbf{z}$ has determinant 0 or
$\pm1$.}. However, as we will see below, constraints that pertain to the real
and/or integer part of $\mathbf{z}$ are always inequalities in our case, so this
is of no particular concern to us.

In this paper we reformulate Felligi Holt error localization
\citep{fellegi:1976} for numerical, categorical and mixed constraints in terms
of mip problems.  The precise reformulations of the error localization problem
for the three types of rules are different, but in each case the objective
function is of the form 
%
\begin{equation}
\mathbf{w}^T\mathbf{\Delta},
\end{equation}
where $\mathbf{w}$ is a vector of positive weights and $\mathbf{\Delta}$ a vector
of binary variables, one for each variable in the original record, that
indicates whether its value should be adapted. More precisely,
for a record $\mathbf{r}=(r_1,r_2,\ldots,r_n)$ of $n$ variables, we have
\begin{equation}
\Delta_i =\left\{\begin{array}{l}
1 \textrm{ if the value of }r_i\textrm{ must be adapted}\\
0 \textrm{ otherwise}.
\end{array}\right.
\label{eq:defineDelta}
\end{equation}
%
This objective function obviously  meets the requirement that the minimal
(weighted) number of variables should be adapted. 

For an error localisation problem, the restrictions of Problem
\eqref{eq:mipmin} consist of two parts, which we denote
\begin{equation}
\left[\begin{array}{c}
  \mathbf{R}^H\\
  \mathbf{R}^0
\end{array}\right] \mathbf{z} \leq 
\left[\begin{array}{c}
  \mathbf{d}^H\\
  \mathbf{d}^0
\end{array}\right]
\label{eq:errlocasmip}
\end{equation}
Here, the restrictions indicated with $H$ represent a matrix representation of
the user-defined (hard) restrictions that the original record $\mathbf{r}$ must
obey. The vector $\mathbf{z}$ is a numerical vector, containing at least a
numerical representation of the values in a record and the binary variables
$\mathbf{\Delta}$. An algorithmic mip-solver will iteratively alter the values of
$\mathbf{z}$ until a solution satisfying \eqref{eq:errlocasmip} is reached. To make
sure that the objective function reflects the (weighted) number of variables
altered in the process, the restrictions in $\mathbf{R}^0$ serve to make sure that
the values in $\mathbf{z}$ that represent values in $\mathbf{r}$ cannot be altered
without setting the corresponding value in $\mathbf{\Delta}$ to 1.

Summarizing, in order to translate the error localisation problem for the cases
of linear, categorical or conditional mixed restrictions to a mixed integer
problem, we need to properly define $\mathbf{z}$, the restriction set
$\mathbf{R}^H\mathbf{z}\leq \mathbf{d}^H$ and the restriction set
$\mathbf{R}^0\mathbf{z}\leq \mathbf{d}^0$.

\subsection{Linear edit rules}
\label{sec:linedits:mip}
For a numerical record $\mathbf{x}$ taking values in $\mathbb{R}^n$, a set of linear
restrictions can be written as
\begin{equation}
\label{eq:linedits}
\mathbf{Ax}\leq \mathbf{b},
\end{equation}
where in \code{editrules}, we allow the set of restrictions to contain equalities,
inequalities ($\leq$) and strict inequalities ($<$). 
The formulation of these edit rules is very close to the formulation of the original
mip problem of Eq.\ \eqref{eq:mipmin}.
The vector to minimize over is defined as follows:
\begin{equation}
\mathbf{z} = (x_1,x_2,\ldots,x_n,\Delta_1,\Delta_2,\ldots,\Delta_n).
\end{equation}
with the $\Delta_i$ as in Eq.\ \eqref{eq:defineDelta}. The set of restrictions
$\mathbf{R}^H\mathbf{z}\leq \mathbf{d}^H$ is equal to the set of restrictions
of Eqn.\ \eqref{eq:linedits}, except in the case of strict inequalities. The
reason is that while \code{editrules} allows the user to define strict
inequalities ($<$), the \code{lpsolve} library used by \code{editrules} only allows
for inclusive inequalities ($\leq$). For this reason, strict inequalities of the
form $\mathbf{a}^T\mathbf{x}<b$ are rewritten as $\mathbf{a}^T\mathbf{x}\leq
b-\epsilon$, with $\epsilon$ a suitably small constant.

In the case of linear edits, the set of constraints $\mathbf{R}^0\mathbf{z}\leq\mathbf{d}^0$
consists of pairs of the form
\begin{eqnarray}
  x_i - M\Delta_i   &\leq& x_i^0\nonumber\\
  -x_i - M\Delta_i  &\leq& -x^0_i
\label{eq:deltanum}
\end{eqnarray}
for $i=1,2,\ldots,n$. Here The $x^0_i$ are the actual observed values in the
record and $M$ is a suitably large constant allowing $x_i$ to vary between
$x_i^0-M$ and $x_i^0+M$. It is not difficult to see that if $x_i$ is different
from $x_i^0$ then $\Delta_i$ must equal 1. For, if we choose
$\Delta_i=0$ we obtain the set of restrictions
\begin{equation}
  x_i^0 \leq  x_i  \leq x_i^0
\end{equation}
which states that $x_i$ equals $x_i^0$

\subsection{Categorical edit rules}
\label{sec:catdataerror}
Categorical records $\mathbf{v}\in \mathbf{D}$ take values in a Cartesian product domain 
\begin{equation}
\label{eq:defineD}
\mathbf{D} = D_1\times D_2\times\cdots\times D_m,
\end{equation}
where each $D_i$ is a finite set of categories for the $i^\textrm{th}$
categorical variable. The category names are unimportant in this formulation, 
so for each $D_i$ we may write
\begin{equation}
\label{eq:defineD}
D_i = \{1,2,\ldots,|D_i|\}.
\end{equation}
The size of the domain, $|\mathbf{D}|$, for categorical records is equal to the product
of the $|D_i|$.

A categorical edit splits $\mathbf{D}$ into a region $\overline{\mathbf{F}}$ 
where records are 
considered valid and a region $\mathbf{F}$ where records are considered invalid. 
Typically, each categorical edit $e$ is specified by defining region 
$\mathbf{F}$ as an invalid combination of categories:
\begin{equation}
\mathbf{F} = F_{1}\times F_{2}\times\cdots \times F_{m},
\end{equation}
where each $F_{i}$ is a subset of $D_i$. It is understood that if a record
$\mathbf{v}\in \mathbf{F}$ then the record violates $e$. 
Hence, categorical edits are
negatively formulated (they specify the region of $D$ where $\mathbf{v}$ may
not be) in contrast to linear edits which are positively formulated (they
specify the region of $\mathbb{R}^n$ where $\mathbf{x}$ must be). To be able to
translate categorical edits to a MIP problem, we need to specify 
$\overline{\mathbf{F}}$, such that if $\mathbf{v}\in\overline{F}$ then
$\mathbf{v}$ satisfies $e$.  Here, $\overline{\mathbf{F}}$ is the complement of
$\mathbf{F}$ in $D$, which can be written as
%
\begin{eqnarray}
\label{eq:invedit}
\lefteqn{
\overline{\mathbf{F}} = \overline{F}_{1}\times D_{2}\times\cdots \times D_{m}
}\nonumber\\
&\cup& D_{1}\times \overline{F}_{2}\times\cdots \times D_{m}\cup\cdots 
\cup D_{1}\times D_{2}\times\cdots \times \overline{F}_{m},
\label{eq:complement}
\end{eqnarray}
%
where for each variable $v_i$, $\overline{F}_{i}$ is the complement of
$F_{i}$ in $D_i$.  Observe that Eq.\ \eqref{eq:complement} states that if at
least one $v_i\in\overline{F}_{i}$, then $\mathbf{v}$ satisfies $e$. Below, we
will use this property and construct a linear relation that counts the number
of $v_i\in\overline{F}_i$ over all variables.

To be able to formulate the Felligi Holt-problem in terms of a MIP problem, we
first associate with each categorical variable $v_i$ a binary vector
$\mathbf{d}$ of which the coefficients are defined as follows (also see Eq.\
\eqref{eq:defineD}).
\begin{equation}
\label{eq:definedvec}
d_j(v_i) = \left\{\begin{array}{l}
1\textrm{ if } v_i = j\\
0\textrm{ otherwise}.
\end{array}\right.
\end{equation}
Thus, each element of $\mathbf{d}(v_i)$ corresponds to one category in $D_i$.
It is zero everywhere except at the value of $v_i\in D_i$. We will write
$\mathbf{d}(\mathbf{v})$ to indicate the concatenated vector
$(\mathbf{d}(v_1),\ldots,\mathbf{d}(v_m))$ which represents a complete record.
Similarly, each edit $e$ is defined as a binary vector $\mathbf{e}$ given by
\begin{equation}
\mathbf{e} = \Big(\bigvee _{v_1\in \overline{F}_1}\mathbf{d}(v_1), \ldots ,\bigvee _{v_m\in \overline{F}_m}\mathbf{d}(v_m)
\Big),
\end{equation}
where we interpret $1$ and $0$ as \code{true} and \code{false} respectively and
the logical 'or' ($\lor$) is applied element-wise to the coefficients of
$\mathbf{d}$.  


The above relation can be interpreted as stating that \todo{This is not correct}
$\mathbf{e}$ is the union of all records satisfying $e$.

\todo[inline]{Or this formulation?}
\begin{equation}
\mathbf{e} = \big(\mathbf{d}(\overline{F}_1), \ldots , \mathbf{d}(\overline{F}_m)
\big),
\end{equation}


To set up the hard restriction matrix $\mathbf{R}^H$ of  Eq.\ \eqref{eq:errlocasmip},
we first impose the obvious restriction that each variable can take but a single value:
\begin{equation}
\label{eq:hardmaxcat}
\sum_{j=1}^{|D_i|}d_j(v_i) = 1,
\end{equation}
for $i=1,2,\ldots,m$. It is now not difficult to see that the demand (Eq.\ \eqref{eq:complement}) that at 
least one of the $v_i\in\overline{F}_i$ may be written as
\begin{equation}
\label{eq:hardcat}
\mathbf{e}^T\mathbf{d}(\mathbf{v}) \geq 1.
\end{equation}
Equations \eqref{eq:hardcat} and \eqref{eq:hardmaxcat} constitute the hard restrictions, stored in
$\mathbf{R}^H$.

Using the binary vector notation for $\mathbf{v}$, and adding the $\Delta$-variables
that indicate variable change, the vector to minimize over (Eq.\
\eqref{eq:mipmin}) is written as
\begin{equation}
\mathbf{z} = (\mathbf{d}(\mathbf{v}),\Delta_1,\Delta_2,\ldots,\Delta_m).
\end{equation}
To ensure that a change in $v_i$ results in a change in $\Delta_i$, the 
matrix $\mathbf{R}^0$ contains the restrictions
\begin{equation}
\label{eq:deltacat}
d_{j^0}(v_i) =  1-\Delta_i,
\end{equation}
for $i=1,2,\ldots,m$.  Here, $j^0\in D_i$ is the observed
value for variable $v_i$. It is not difficult to check, using Eq.
\eqref{eq:definedvec}, that the above equation can only hold for all $j$ when
either $v_i=j^0$ and $\Delta_i=0$ (the original value is retained) or
$v_i\not=j^0$ and $\Delta_i=1$ (the value changes).


\subsection{Edits rules on data of mixed type}
\label{sec:mixdataerror}
Records $\mathbf{r}$ containing both numerical and categorical data can be denoted
as a concatenation of numerical and categorical variables 
taking values in $\mathbf{D}\times\mathbb{R}^n$: 
\begin{equation}
\mathbf{r} = (v_1,\ldots,v_m,x_1,\ldots,x_n) = (\mathbf{v},\mathbf{x}),
\end{equation}
where $D$ is defined in Eq.\ \eqref{eq:defineD}. Edits on $D\times\mathbb{R}^n$.
As stated above, categorical edits are usually defined negatively as a region of
$D$ that is disallowed while linear edits define regions in $\mathbb{R}^n$ that
are allowed. We may choose to formulate a negative formulation of edits containing
both variable types: \todo{$s$ vs $F$ in previous sections}
\begin{equation}
e = \{\mathbf{r}\in D\times\mathbb{R}^n: 
\lnot(\mathbf{v}\in \overline{s}) \land \lnot(\mathbf{a}^T\mathbf{x}\leq b)\}
\end{equation}
where it is understood that $s\subseteq D$ and if $\mathbf{r}\in e$, then $\mathbf{r}$ violates $e$.
To obtain a positive formulation, we first rewrite the set membership condition by applying basic
rules of first-order logic:
\begin{eqnarray}
&&\lnot(\mathbf{v}\in \overline{s}) \land \lnot(\mathbf{a}^T\mathbf{x}\leq b)\nonumber\\
&\Leftrightarrow& \lnot (\mathbf{v}\in \overline{s} \lor \mathbf{a}^T\mathbf{x}\leq b)\nonumber\\
&\Leftrightarrow& \lnot (\mathbf{v}\in s \Rightarrow \mathbf{a}^T\mathbf{x}\leq b).
\label{eq:implication}
\end{eqnarray}
This then yields a positive formulation of $e$ given by
\begin{equation}
\overline{e} = \{\mathbf{r}\in D\times\mathbb{R}^n: 
\mathbf{v}\in s \Rightarrow \mathbf{a}^T\mathbf{x}\leq b\}.
\end{equation}
This formulation of edits on mixed-type data is called the \emph{standard form}
by \citet{waal:2003}. It is also the form in which the \code{editrules} package
accepts mixed-type edits and has the property that if
$\mathbf{r}\in\overline{e}$ then $\mathbf{r}$ satisfies $e$.

We now define a binary variable $l$ that indicates whether the linear restriction in 
the consequent is obeyed or not:
\begin{equation}
l = \left\{\begin{array}{l}
0 \textrm{ when } \mathbf{a}^T\mathbf{x}\leq b\\
1 \textrm{ otherwise.}
\end{array}\right.
\end{equation}
Using the or-form of the set condition (the second line of Eq.\ \eqref{eq:implication})
and the definition of $\mathbf{e}$, we can write the mixed-data edit as
\begin{equation}
\label{eq:mixedit}
\mathbf{e}(s)^T\mathbf{v} - l \geq 0.
\end{equation}
These form the user-defined part of the $\mathbf{R}^H$ part of the restriction matrix.
To connect $l$ with the linear restriction in the consequent we also add
\begin{equation}
\label{eq:linconsequent}
\mathbf{a}^T\mathbf{x} \leq b + Ml,
\end{equation}
to $\mathbf{R}^H$ with $M$ a suitably large positive constant. Indeed, if the
$l=0$, the inequality $\mathbf{a}^T\mathbf{x}\leq b$ is enforced. When $l=0$ it
is unimportant for the whole restriction to hold whether the inequality holds.
Finally, similar to the purely categorical case we need to add restrictions on
the binary representation of $\mathbf{v}$ as in Eq.\ \eqref{eq:hardmaxcat}, so
Eq.\ \eqref{eq:hardmaxcat}, Eq.\ \eqref{eq:linconsequent} and Eq.\
\eqref{eq:mixedit} constitute $\mathbf{R}^H$.

In general there may be $k$ multiple mixed-type edits yielding an equal number of indicator
variables for inequalities in the consequent. So the vector to minimize over becomes
\begin{equation}
\mathbf{z} = (\mathbf{d}(\mathbf{v}),\mathbf{x},\Delta_1,\ldots,\Delta_m,\ldots,\Delta_{m+n},
l_1,\ldots,l_k).
\end{equation}
Finally, the $\mathbf{R}^0$ matrix, connecting the $\Delta$ variables with the actual
recorded values consists of the union of the restrictions for categorical variables
(Eq.\ \eqref{eq:deltacat}) and those for numerical variables (Eq.\ \eqref{eq:deltanum}).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical stability issues}
\label{sec:linedits:num}
Reformulating the error localisation problem in terms of a mip problem
introduces two constants: $M$ and $\epsilon$. These constants end up in the
coefficient matrix of the mip solver.  Mathematically, there are no bounds on
the values of $M$ and $\epsilon$, as long as they are chosen so that the
problem remains feasible. In practice however, limitations on the accuracy with
which numbers are represented on a computer put restrictions on the value range
of numbers that enter the mip solver. In the \code{lpsolve} library (as well
as in \R{}), numbers on the real line are represented as \code{double precision}
numbers, meaning that there are about 15 decimal positions of precision. 

To avoid significant round off errors, it is a good idea to restrict the range
of input data. In fact, the authors of the \code{lpsolve} reference manual
\citep{lpsolveman:2012} state the following.
%
\begin{quote}
The chance for numerical instability and rounding errors is considerably 
larger when the input data contains both large and small numbers. 
So to improve stability, one must try to work with numbers that are somewhat 
in the same range. Ideally in the neighborhood of 1.
[...]You should realize, that you the user are probably in a better position to 
scale the problem than any computer algorithm. 
\end{quote}


\begin{equation}
\left[\begin{array}{c|c}
  \mathbf{A}^H &\mathbf{b}^H\\
  \mathbf{A}^0 &\mathbf{b}^0
\end{array}\right] 
= 
\left[\begin{array}{rr|r}
  \multicolumn{1}{c}{ \mathbf{A}^H} & \multicolumn{1}{c|}{\mathbf{0}} & \mathbf{b}^H\\
  \mathbbm{1} & M\mathbbm{1}  &\mathbf{x}^0\\
  -\mathbbm{1} & -M\mathbbm{1} &-\mathbf{x^0}
\end{array}\right] 
\end{equation}
In {\sf editrules} by default $M = 10^7$ and $\epsilon = 10^{-3}$ is used. This 
will 
be sufficient for most purposes, since numerical values and coefficients of the
constraints 
in most cases will be in this range. If not, you should rescale some of your
variables and constraints to make it so.

The linear programming manual from \code{lpsolve}  \citep{lpsolveman:2012} says the 
following on this matter:


\section{Usage}
\subsection{Error localization}
\code{editrules} provides the \code{localizeErrors} function for finding errors
in a \code{data.frame} given an \code{editset}, \code{editmatrix} or \code{editarray}.
The \code{localizeErrors} function is extended to include an extra parameter
\code{method}. The default value of \code{method} is \code{'bb'}: branch and bound.
Setting \code{method} to \code{'mip'} gives identical but faster result. 
This implementation makes it possible to switch between the two implementations.
<<num example>>=
E <- editmatrix("x <= y")
dat <- data.frame(x=10, y=1)
weight <- data.frame(x=2, y=1)

localizeErrors(E, dat, weight=weight, method="bb")$adapt
localizeErrors(E, dat, weight=weight, method="mip")$adapt
@
When \code{method=mip} is specified \code{localizeErrors} uses for each record 
the function \code{errorLocalizer.mip}.
Unlike {\em branch and bound} this is not a \code{backtracker} object.
\code{errorlocalizer.mip} writes the constraints and values into a mip problems,
feeds it into the \code{lpSolveApi} and formats the resulting output.
<<num mip>>=
E <- editmatrix("x <= y")
r <- c(x=10, y=1)
weight <- c(x=2, y=1)
el <- errorLocalizer_mip(E, r, weight=weight)
ls.str(el)
@
Both \code{localizeErrors} as \code{errorLocalizer.mip} can be executed with an 
\code{editmatrix}, \code{editarray} or \code{editset} object.

Note in the example that the {\em mip} solver returns two extra properties that are
not available in {\em branch and bound}: \code{lp} and \code{x\_feasible}.
Since a mip-solvers goal is to find a feasible solution, a set of feasible x 
values is available. It can be helpful to have these values, but note 
that the set is not unique and that the proposed values for the variables to be 
adjusted lie on the boundary of a convex solution region. 
So  in many cases it is better to use a proper imputation method to generate
sensible values for the variable that need to be adjusted.
Secondly \code{errorlocalizer.mip} returns the \code{lp} object. This object is 
a \code{lpSolveApi} object and can be further manipulated or written to disk.
For more information consult the manual of \code{lpSolveApi}.

An advantage of the branch and bound algorithm is that it can generate all optimal
solutions for a given record.  The mip implementation only generates one of the 
best. This is implemented by adding a small uniform perturbation to the weights, 
so mip chooses one of the best at random. This is identical 
to the default behavior of \code{localizeErrors} for \code{method='bb'}.


\subsection{Setting up your own lp problem}
\code{editrules} uses \code{editarray} for dealing with categorical edits. These
are described in \cite{loo:2011b}. An example \code{editarray} is given below.
<<tidy=FALSE>>=
(E <- editarray(expression(
    gender %in% c('male','female'),
    pregnant %in% c(TRUE, FALSE),
    if (pregnant) gender == 'female'
    )
))
@
\code{E} is an editarray. That \code{E} parses the constraints can be seen in 
the pregnant constraint: it has been rewritten in an equivalent form.
To create a mip formulation for the categorical error localization problem,
\code{editrules} rewrites categorical edits into a numerical matrix using the
function \code{as.mip}.
<<>>=
as.mip(E)
@
Above example shows the rewriting of ${\bf A}^H$. All variables are binary 
variables $\in \{0,1\}$. Constraint \code{num1} states
that \code{gender} either must be \code{'male'} or \code{'female'}. Constraint 
\code{num2} states that \code{pregnant} and \code{gender:male} cannot both be
1 (true).

Note that \code{as.mip} 
recognizes that \code{pregnant} is a \code{logical}. The mip formulation contains
just one binary variable for \code{pregnant}.
(explain...)

If we add the $A^0$
constraints we get:
<<tidy=FALSE>>=
as.mip( E
      , x=list(gender="male", pregnant=TRUE)
      , weight=c(1,2)
      )
@
(explain)

\subsection{Implementation}

<<tidy=FALSE>>=
E <- editset(expression(
  married %in% c(TRUE, FALSE),
  if (married) age >= 17
  ))
@

Writing this to ${\bf A}^H$ form we get:
<<>>=
as.mip(E)
@
(explain...)

Adding some ${\bf A}^0$ we get:
<<>>=
as.mip(E, x=list(age=9, married=TRUE))
@
(explain...)

\section{Benchmarks}
\subsection{Tests}


\section{Conclusion}
\code{editrules} includes a mip implementation of the localize errors formalism 
of Felligi and Holt. This mip implementation is typically must faster than the
branch and bound algorithm. 

For most problems mip will generate a good solution. However for problems where
coefficients or values exceed the range of $[10^{-3} - 10^7]$ the user has to rescale
the problem: otherwise the mip solver will become unstable.

\bibliographystyle{chicago}
\bibliography{editrules}

\end{document}

