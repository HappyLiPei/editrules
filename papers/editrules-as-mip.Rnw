%\VignetteIndexEntry{Error localization for numerical and categorical edits as a mixed integer problem}
\documentclass[10pt, fleqn, english, rapport]{cbsreport}
%\usepackage{inconsolata}
\usepackage[english]{babel}
\usepackage{amsmath, amsthm}
\usepackage{bbm} % mathbb numbers
\usepackage{array}
\usepackage{natbib}
\usepackage{threeparttable}
\usepackage{makeidx}
\usepackage{todonotes}
\makeindex

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\Lor}{\lor}
\DeclareMathOperator*{\Land}{\land}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\R}{\code{R}} %% call as \R{} to avoid extra spaces.

\newcommand{\E}{E}
\newcommand{\F}{F}
\newcommand{\Fc}{\mathbf{\overline{F}}}
\newcommand{\D}{D}
\newcommand{\Ls}{\mathbf{L}}
\newcommand{\FD}{\mathbb{F}}
\newcommand{\FDc}{\overline{\mathbb{F}}}


%\renewcommand{\mathbf}[1]{\ensuremath{\mathbf{#1}}}

\usepackage{float} 
\floatstyle{boxed}
\newfloat{Rcode}{t!}{rco}
\floatname{Rcode}{{\rm Figure}}


% stimulate latex to put multiple floats on a page.
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{3}
\setcounter{dbltopnumber}{2}
\renewcommand{\topfraction}{.9}
\renewcommand{\textfraction}{.1}
\renewcommand{\bottomfraction}{.75}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\dblfloatpagefraction}{.9}
\renewcommand{\dbltopfraction}{.9}


\hyphenation{
}

<<init, echo=FALSE, results='hide', message=FALSE>>=
library(editrules)
#library(xtable)
@

\title{Error localization as a mixed integer problem with the {\tt editrules} package}
\author{Edwin de Jonge and Mark van der Loo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle
\begin{abstract}
Error localization is the problem of finding out which fields in raw data
records contain erroneous values. The \code{editrules} extension package for
the \R{} environment for statistical computing was recently extended with a module
that allows for error localization based on a mixed integer programming
formulation (mip). In this paper we describe the mip formulation of the error
localization problem for the case of numerical, categorical, or mixed numerical
and categorical datasets. The new module is benchmarked against a previously
available module, which is based on a branch-and-bound approach. The benchmark
shows that the mip-based approach is significantly faster and has a smaller memory
footprint. Trade-offs between the branch-and-bound and mip approaches are discussed
as well.
\end{abstract}

\newpage
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Analyses of data are often hindered by occurrences of incomplete or
inconsistent raw data records.  The process of locating and correcting such
errors is referred to as {\em data editing}, and it has been estimated that
National Statistics Institutes may spend up to 40\% of their resources on this
process \citep{waal:2011}. Moreover, data are often required to obey many
cross-variable consistency rules which significantly complicate the data
editing process. Indeed, \citet{winkler:1999} mentions practical cases
(household surveys) where records have to obey 250, 300 or even 750
user-defined interrelated consistency rules.  For these reasons, considerable
attention is paid to the development of data editing methods that can be
automated. 

\subsection{Error localization}
Automated (and even manual) data editing strategies typically consist of three steps:
\begin{enumerate}
\item Find out which rules (expectations) a record violates;
\item Find out which fields in a record cause those violations;
\item Replace the values in those fields with better estimates, such that no rules
are violated anymore.
\end{enumerate}
The second step is usually referred to as the \emph{error localization}
problem, which is the focus of the current paper.  Although it is widely
recognized that data editing is a necessary step in the statistical process,
the amount of changes made to the data should obviously be minimized to avoid
introducing bias in estimations based on the edited data. This then, led to the
development of the following minimization problem.
%
\begin{quote}
Given a record of $n$ variables, subject to a number of possibly multivariate
consistency rules. Find the smallest (weighted) subset of fields, such that their
original values can be replaced in such a way that no rules are violated anymore. 
\end{quote}
%
This minimization problem is named after \cite{fellegi:1976}, who first
formulated and solved the problem for the case of categorical data. Error
localization has been extensively discussed in literature\footnote{See
\citet{waal:2011} and references therein}, so we will suffice with a few
remarks. First, note that the minimization problem is possibly over a search
space of size $2^n$, with $n$ the number of fields, rendering a
brute-force approach that runs through all
possible solution candidates computationally unfeasible.
Second, the problem is
complicated by the occurrence of \emph{implied rules}. That is, the solution
set must not only allow the record to obey the original, user-defined set of
rules, but also rules that are logically or arithmetically implied by the
original set. To cope with these complications, several algorithmic approaches
have been developed, of which two are worth mentioning in this context:
the first is the branch-and-bound approach, developed by \cite{waal:2003a}.
The second is an approach based on mixed-integer programming (mip), described
in \cite{waal:2011}.

\subsection{The \code{editrules} package}
Over the past five to ten years, the \R{} statistical environment received a
surge in popularity and as a consequence it has been extended with many
packages that allow for statistical analyses of data. However, the number of
packages specifically aimed at data editing seems to be somewhat limited,
except possibly in the area of imputation.  The {\sf R} package {\sf editrules}
\citep{jonge:2011a} was developed to help to bridge the gap between raw data
retrieval and data analysis with {\sf R}.  The main purpose of the package is
to provide an easy and consistent interface to define data consistency rules
(often referred to as \emph{edit rules}) in \R{} and to confront them with
data. Furthermore, the package allows for basic rule manipulation (deriving new
rules, finding inconsistencies, \emph{etc.}) and for error localization
functionality.  As such, the package does not offer functionality
to correct data.  Rather, it is aimed at identifying the set of solutions to an
error correction problem: the second step mentioned in the data editing
strategy above. Previous developments of the package have been described
in \cite{jonge:2011,loo:2011b} and \cite{loo:2011a}.


The \code{editrules} package offers a fairly complete toolbox, allowing users
to work with numerical, categorical or mixed-type data editing rules. Up until
now, error localization was performed by an implementation of the
branch-and-bound algorithm described by \cite{waal:2003}. The main disadvantage
of this approach is that the branch-and-bound algorithm has $\mathcal{O}(2^n)$
worst-case time and memory complexity, where $n$ is the number of variables
occurring in a connected set of rules. Moreover, the branch-and-bound solver is
written in pure \R{}, making it intrinsically slower than a compiled language
implementation. The main advantages of this approach are the ease of
implementation and the opportunity for users to exert fine-grained control over
the algorithm.

As stated before, the error localization problem can, under mild conditions, be
translated to a mixed-integer programming problem, which offers the opportunity
to reuse well-established results from the field of linear and mixed-integer
programming. Indeed, many advanced algorithms for solving such problems have
been developed and in many cases implementations in a compiled language are
available under a permissive license. In \code{editrules}, the solver of the
\code{lpsolve} library \citep{berkelaar:2004} is used through \R{}'s
\code{lpSolveAPI} package \citep{lpSolveAPI:2011}. The \code{lpsolve} library
is written in \code{ANSI} \code{C} and has been tried and tested extensively.

The strategy to solve error localization problems through this library from 
\R{} therefore consists of translating the problem to a suitable mixed-integer
programming problem, feeding this problem to \code{lpSolveAPI} and translating
the results back to an error location. It is necessary
to distinguish between:
\begin{itemize}
  \item linear restrictions on purely numerical data,
  \item restrictions on purely categorical data, and
  \item restrictions on mixed-type data,
\end{itemize}
since for each data type a different translation strategy is necessary.

The main part of this paper will focus on how to translate these types of error
localization problems to a mixed-integer formulation, paying attention to both
theoretical and practical details.  We will give examples of how users can
apply this functionality to their own problem  in \R{} and benchmark the new
mip-functionality against the existing branch-and-bound solution.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Error localization and mixed integer programming}
\label{sec:mipproblem}
%
A mixed integer programming problem is an optimization problem that can be
written in the form
\begin{equation}
\begin{array}{r}
\textrm{Minimize } \mathbf{c}^T\mathbf{z}; \\
\textrm{s.t. }\mathbf{Rz} \leq \mathbf{d},
\label{eq:mipmin}
\end{array}
\end{equation}
%
where $\mathbf{c}$ is a constant vector and $\mathbf{z}$ is a vector consisting
of real and integer coefficients. The inner product $\mathbf{c}^T\mathbf{z}$ is
referred to as the \emph{objective function}.  Furthermore, $\mathbf{R}$ is a
coefficient matrix and $\mathbf{d}$ a vector of upper bounds. Formally, the
elements of $\mathbf{c}$, $\mathbf{R}$ and $\mathbf{d}$ are limited to the rational
numbers \citep{schrijver:1998}, which is never a problem in practice since we
are always working with a computer representation of numbers. 

The name \emph{mixed-integer programming} stems from the fact that $\mathbf{z}$
contains continuous as well as integer variables. When $\mathbf{z}$ consists
solely of continuous or integer variables, Problem~\eqref{eq:mipmin} reduces
respectively to a \emph{linear} or an \emph{integer programming} problem.  An
important special case occurs when the integer coefficients of $\mathbf{z}$ may
only take values from \{0,1\}. Such variables are often called binary variables
or decision variables. It occurs as a special case since defining $\mathbf{z}$ to
be integer and applying the appropriate restrictions yields the same problem.

Mixed integer programming is well understood and several software packages
packages are available that implement efficient solvers.  Most mip software
allows for a broader (but equivalent) formulation of the mip problem, allowing
the set of restrictions to include inequalities as well as equalities. As a
side note, we mention that under equality constraints, solutions for the
integer part of $\mathbf{z}$ only exist when the equality constraints
pertaining to the integer part of $\mathbf{z}$ are \emph{totally
unimodular}\footnote{This means that every square submatrix of the coefficient
matrix pertaining to the integer part of $\mathbf{z}$ has determinant 0 or
$\pm1$.}. However, as we will see below, constraints that pertain to the real
and/or integer part of $\mathbf{z}$ are always inequalities in our case, so this
is of no particular concern to us.

In this paper we reformulate Felligi Holt error localization
\citep{fellegi:1976} for numerical, categorical and mixed constraints in terms
of mip problems.  The precise reformulations of the error localization problem
for the three types of rules are different, but in each case the objective
function is of the form 
%
\begin{equation}
\mathbf{w}^T\mathbf{\Delta},
\end{equation}
where $\mathbf{w}$ is a vector of positive weights and $\mathbf{\Delta}$ a vector
of binary variables, one for each variable in the original record, that
indicates whether its value should be adapted. More precisely,
for a record $\mathbf{r}=(r_1,r_2,\ldots,r_n)$ of $n$ variables, we have
\begin{equation}
\Delta_i =\left\{\begin{array}{l}
1 \textrm{ if the value of }r_i\textrm{ must be adapted}\\
0 \textrm{ otherwise}.
\end{array}\right.
\label{eq:defineDelta}
\end{equation}
%
This objective function obviously  meets the requirement that the minimal
(weighted) number of variables should be adapted. 

For an error localization problem, the restrictions of Problem
\eqref{eq:mipmin} consist of two parts, which we denote
\begin{equation}
\left[\begin{array}{c}
  \mathbf{R}^H\\
  \mathbf{R}^0
\end{array}\right] \mathbf{z} \leq 
\left[\begin{array}{c}
  \mathbf{d}^H\\
  \mathbf{d}^0
\end{array}\right]
\label{eq:errlocasmip}
\end{equation}
Here, the restrictions indicated with $H$ represent a matrix representation of
the user-defined (hard) restrictions that the original record $\mathbf{r}$ must
obey. The vector $\mathbf{z}$ is a numerical vector, containing at least a
numerical representation of the values in a record and the binary variables
$\mathbf{\Delta}$. An algorithmic mip-solver will iteratively alter the values of
$\mathbf{z}$ until a solution satisfying \eqref{eq:errlocasmip} is reached. To make
sure that the objective function reflects the (weighted) number of variables
altered in the process, the restrictions in $\mathbf{R}^0$ serve to make sure that
the values in $\mathbf{z}$ that represent values in $\mathbf{r}$ cannot be altered
without setting the corresponding value in $\mathbf{\Delta}$ to 1.

Summarizing, in order to translate the error localization problem for the cases
of linear, categorical or conditional mixed restrictions to a mixed integer
problem, we need to properly define $\mathbf{z}$, the restriction set
$\mathbf{R}^H\mathbf{z}\leq \mathbf{d}^H$ and the restriction set
$\mathbf{R}^0\mathbf{z}\leq \mathbf{d}^0$.

\subsection{Linear edit rules}
\label{sec:linedits:mip}
For a numerical record $\mathbf{x}$ taking values in $\mathbb{R}^n$, a set of linear
restrictions can be written as
\begin{equation}
\label{eq:linedits}
\mathbf{Ax}\leq \mathbf{b},
\end{equation}
where in \code{editrules}, we allow the set of restrictions to contain equalities,
inequalities ($\leq$) and strict inequalities ($<$). 
The formulation of these edit rules is very close to the formulation of the original
mip problem of Eq.\ \eqref{eq:mipmin}.
The vector to minimize over is defined as follows:
\begin{equation}
\mathbf{z} = (x_1,x_2,\ldots,x_n,\Delta_1,\Delta_2,\ldots,\Delta_n).
\end{equation}
with the $\Delta_i$ as in Eq.\ \eqref{eq:defineDelta}. The set of restrictions
$\mathbf{R}^H\mathbf{z}\leq \mathbf{d}^H$ is equal to the set of restrictions
of Eqn.\ \eqref{eq:linedits}, except in the case of strict inequalities. The
reason is that while \code{editrules} allows the user to define strict
inequalities ($<$), the \code{lpsolve} library used by \code{editrules} only allows
for inclusive inequalities ($\leq$). For this reason, strict inequalities of the
form $\mathbf{a}^T\mathbf{x}<b$ are rewritten as $\mathbf{a}^T\mathbf{x}\leq
b-\epsilon$, with $\epsilon$ a suitably small constant.

In the case of linear edits, the set of constraints $\mathbf{R}^0\mathbf{z}\leq\mathbf{d}^0$
consists of pairs of the form
\begin{eqnarray}
  x_i - M\Delta_i   &\leq& x_i^0\nonumber\\
  -x_i - M\Delta_i  &\leq& -x^0_i
\label{eq:deltanum}
\end{eqnarray}
for $i=1,2,\ldots,n$. Here The $x^0_i$ are the actual observed values in the
record and $M$ is a suitably large constant allowing $x_i$ to vary between
$x_i^0-M$ and $x_i^0+M$. It is not difficult to see that if $x_i$ is different
from $x_i^0$ then $\Delta_i$ must equal 1. For, if we choose
$\Delta_i=0$ we obtain the set of restrictions
\begin{equation}
  x_i^0 \leq  x_i  \leq x_i^0
\end{equation}
which states that $x_i$ equals $x_i^0$

\subsection{Categorical edit rules}
\label{sec:catdataerror}
Categorical records $\mathbf{v}\in D$ take values in a Cartesian product domain 
\begin{equation}
\label{eq:defineD}
\D = D_1\times D_2\times\cdots\times D_m,
\end{equation}
where each $D_i$ is a finite set of categories for the $i^\textrm{th}$
categorical variable. The category names are unimportant in this formulation, 
so for each $D_i$ we may write
\begin{equation}
\label{eq:defineD}
D_i = \{1,2,\ldots,|D_i|\}.
\end{equation}
The size of the domain, $|\D|$, for categorical records is equal to the
product of the $|D_i|$.

A categorical edit is a subset $\F$ of $\D$ where records are
considered invalid, and we may write
\begin{equation}
\label{eq:defcatedit}
\F = F_{1}\times F_{2}\times\cdots \times F_{m},
\end{equation}
where each $F_{i}$ is a subset of $D_i$. It is understood that if a record
$\mathbf{v}\in \F$ then the record violates the edit.
Hence, categorical edits are
negatively formulated (they specify the region of $D$ where $\mathbf{v}$ may
not be) in contrast to linear edits which are positively formulated (they
specify the region of $\mathbb{R}^n$ where $\mathbf{x}$ must be). To be able to
translate categorical edits to a MIP problem, we need to specify 
$\overline{\F}$, such that if $\mathbf{v}\in\overline{\F}$ then
$\mathbf{v}$ satisfies $e$.  Here, $\overline{\F}$ is the complement of
$\F$ in $D$, which can be written as
%
\begin{eqnarray}
\label{eq:invedit}
\lefteqn{
\overline{\F} = \overline{F}_{1}\times D_{2}\times\cdots \times D_{m}
}\nonumber\\
&\cup& D_{1}\times \overline{F}_{2}\times\cdots \times D_{m}\cup\cdots 
\cup D_{1}\times D_{2}\times\cdots \times \overline{F}_{m},
\label{eq:complement}
\end{eqnarray}
%
where for each variable $v_i$, $\overline{F}_{i}$ is the complement of
$F_{i}$ in $D_i$.  Observe that Eq.\ \eqref{eq:complement} states that if at
least one $v_i\in\overline{F}_{i}$, then $\mathbf{v}$ satisfies $e$. 
%%% OPM: dit is extra notatie, die de zaak niet per se duidelijker maakt.
%Membership to $\overline{\F}$ can also be 
%written with the following logical statement:
%\begin{equation}
%\label{eq:cat_logical}
%\mathbf{v} \in \overline{\F}= \bigvee^m_{i=1} v_i \in \overline{F}_i
%\end{equation}
Below, we
will use this property and construct a linear relation that counts the number
of $v_i\in\overline{F}_i$ over all variables.

To be able to formulate the Felligi Holt-problem in terms of a MIP problem, we
first associate with each categorical variable $v_i$ a binary vector
$\mathbf{d}$ of which the coefficients are defined as follows (see also Eq.\
\eqref{eq:defineD}).
\begin{equation}
\label{eq:definedvec}
d_j(v_i) = \left\{\begin{array}{l}
1\textrm{ if } v_i = j\\
0\textrm{ otherwise}.
\end{array}\right.
\end{equation}
Thus, each element of $\mathbf{d}(v_i)$ corresponds to one category in $D_i$.
It is zero everywhere except at the value of $v_i\in D_i$. We will write
$\mathbf{d}(\mathbf{v})$ to indicate the concatenated vector
$(\mathbf{d}(v_1),\ldots,\mathbf{d}(v_m))$ which represents a complete record.
Similarly, each edit can be represented by a binary vector $\mathbf{e}$ given by
\begin{equation}
\mathbf{e} = \left(\bigvee _{v_1\in \overline{F}_1}\mathbf{d}(v_1), \ldots ,\bigvee _{v_m\in \overline{F}_m}\mathbf{d}(v_m)
\right),
\end{equation}
where we interpret $1$ and $0$ as \code{true} and \code{false} respectively and
the logical 'or' ($\lor$) is applied element-wise to the coefficients of
$\mathbf{d}$.  The above relation can be interpreted as stating that
$\mathbf{e}$ is the set of all records satisfing the edit.
\todo[inline]{
[MvdL]: When $\overline{F}_i=\varnothing$, $F_i=D_i$ and $v_i$ is not contained in the
edit.  In that case it doesn't and shouldn't contribute to the sum in Eq.\
\eqref{eq:hardcat}. In other words $\mathbf{d}(v_i)=\mathbf{0}$ means that
the edit check does not involve the $i$th variable. I think we should rephrase
that $\mathbf{e}$ {\em represents} all records obeying the edit.
}
\todo[inline]{[EdJ]Hmmm, I have a slight/minor problem with \textit{represents 
all records},  $\mathbf{e}$ actually represents all sets $\overline{F}_i$.
It works as a filter for records: if a record contains one of the categories of 
$\mathbf{e}$ then it obeys the edit.
}
To set up the hard restriction matrix $\mathbf{R}^H$ of  Eq.\ \eqref{eq:errlocasmip},
we first impose the obvious restriction that each variable can take but a single value:
\begin{equation}
\label{eq:hardmaxcat}
\sum_{j=1}^{|D_i|}d_j(v_i) = 1,
\end{equation}
for $i=1,2,\ldots,m$. It is now not difficult to see that the demand (Eq.\ \eqref{eq:complement}) that at 
least one of the $v_i\in\overline{F}_i$ may be written as
\begin{equation}
\label{eq:hardcat}
\mathbf{e}^T\mathbf{d}(\mathbf{v}) \geq 1.
\end{equation}
Equations \eqref{eq:hardmaxcat} and \eqref{eq:hardcat} constitute the hard restrictions, stored in
$\mathbf{R}^H$.

Using the binary vector notation for $\mathbf{v}$, and adding the $\Delta$-variables
that indicate variable change, the vector to minimize over (Eq.\
\eqref{eq:mipmin}) is written as
\begin{equation}
\mathbf{z} = (\mathbf{d}(\mathbf{v}),\Delta_1,\Delta_2,\ldots,\Delta_m).
\end{equation}
To ensure that a change in $v_i$ results in a change in $\Delta_i$, the 
matrix $\mathbf{R}^0$ contains the restrictions
\begin{equation}
\label{eq:deltacat}
d_{j^0}(v_i) =  1-\Delta_i,
\end{equation}
for $i=1,2,\ldots,m$.  Here, $j^0\in D_i$ is the observed
value for variable $v_i$. It is not difficult to check, using Eq.
\eqref{eq:definedvec}, that the above equation can only hold for all $j$ when
either $v_i=j^0$ and $\Delta_i=0$ (the original value is retained) or
$v_i\not=j^0$ and $\Delta_i=1$ (the value changes).


\subsection{Mixed-type edit rules}
\label{sec:mixdataerror}
Records $\mathbf{r}$ containing both numerical and categorical data can be denoted
as a concatenation of numerical and categorical variables 
taking values in $D\times\mathbb{R}^n$: 
\begin{equation}
\mathbf{r} = (v_1,\ldots,v_m,x_1,\ldots,x_n) = (\mathbf{v},\mathbf{x}),
\end{equation}
where $D$ is defined in Eq.\ \eqref{eq:defineD}. Edits on $D\times\mathbb{R}^n$.
As stated above, categorical edits are usually defined negatively as a region of
$D$ that is disallowed while linear edits define regions in $\mathbb{R}^n$ that
are allowed. We may choose a negative formulation of edits containing
both variable types by defining $\E$: 
\begin{equation}
\E = \{\mathbf{r}\in \D \times\mathbb{R}^n: 
\mathbf{v}\in \F \land \mathbf{x} \in P \},
\end{equation}
where $\F \subseteq \D$ and $P$ is a convex subset of $\mathbb{R}^n$ defined by
(possibly empty) set of $k$ linear inequalities of the form $\mathbf{a}^T\mathbf{x}
> b$.  It is understood that if $\mathbf{r} \in \E$, then $\mathbf{r}$ violates
the edit. 

To obtain a positive reformulation, we first negate the set membership
condition and apply basic rules of proposition logic:
\begin{eqnarray}
&&\lnot\big(\mathbf{v}\in \F \land \mathbf{x} \in P \big)\nonumber\\
&\Leftrightarrow& \lnot \big( \mathbf{v}\in \F \land \mathbf{x} \in P_1 \ldots \land \mathbf{x} \in P_{k})
\nonumber\\
&\Leftrightarrow& \mathbf{v} \not\in \F \lor \mathbf{x} \not\in P_1 \ldots \lor 
\mathbf{x} \not\in P_{k}\nonumber\\
&\Leftrightarrow& \mathbf{v}\in \overline{\F} \lor \mathbf{a}^T_{1}\mathbf{x}\leq b_{1} 
\lor \ldots \lor \mathbf{a}^T_{k}\mathbf{x}\leq b_{k}.
\label{eq:implication}
\end{eqnarray}
This then yields a positive formulation of $E$. That is, a record $\mathbf{r}$
satisfies $E$ if and only if
\begin{equation}
  \label{eq:mixedtype}
  \mathbf{r} \in \overline{\E} \Leftrightarrow 
\bigvee_{i=1}^m v_i \in \overline{F}_i \:\lor\: \bigvee^{k}_{j=1} \mathbf{a}_j^T\mathbf{x} \leq b_j.
\end{equation}
%
%This formulation of edits on mixed-type data is more general than the \emph{standard form}
%by \citet{waal:2003}, which allows but a single linear inequality.
%\todo[inline]{De standaard form dekt geen restricties van de volgende
%vorm:
%if ($x>0$) then ($y>0$)
%die we ook aankunnen.
%Wat doen we met de standaardform, die ook geven?
%\begin{equation}
%\FDc = \{\mathbf{r}\in D\times\mathbb{R}^n: 
%\mathbf{v}\in F \Rightarrow \mathbf{a}^T\mathbf{x}\leq b\}.
%\end{equation}
%} 
This formulation is both a generalization of linear inequality and
categorical edits (Eq.\ \ref{eq:defcatedit}). Choosing $k=0$, we get
$P=\mathbb{R}^n$ and only the categorical part remains. Similarly, choosing
$\F=\D$, only the linear inequalities remain. The definition in 
Eq.\ \eqref{eq:implication} can be rewritten as a ``conditional edit'' by using
the implication replacement rule from propositinal logic  which states that
$\lnot p\lor q$ may be replaced by $p\Rightarrow q$. If we limit
Eq.\ \eqref{eq:implication} to a single inequality, we obtain the normal form of
\cite{waal:2003}.
\begin{equation}
\mathbf{v} \in \F\Rightarrow \mathbf{a}^T\mathbf{x}\leq b.
\end{equation}
If we choose $\overline{F}=D$ and leave two inequalities we obtain a conditional edit
on numerical data:
\begin{equation}
\mathbf{a}_1^T\mathbf{x} > b_1 \Rightarrow \mathbf{a}_2^T\mathbf{x} \leq b_2.
\end{equation}
These conditional type edits are more natural for users as they can directly be
translated to an \code{if} statement in a scripting language. Finally, note
that equalities can be introduced by defining pairs of
edits like so:
\begin{equation}
\left\{\begin{array}{l}
\mathbf{v} \in \F\Rightarrow \mathbf{a}^T\mathbf{x}\leq b\\
\mathbf{v} \in \F\Rightarrow -\mathbf{a}^T\mathbf{x}\leq -b.
\end{array}\right.
\end{equation}


To reformulate Eq. \eqref{eq:implication} as a restriction in a MIP problem, we first define binary
variables $p_i$ that indicate whether $\mathbf{x}$ obeys
$\mathbf{a}^T_k\mathbf{x}\leq b$ or $\mathbf{x} \in P_j$:
\begin{equation}
p_j = \left\{\begin{array}{l}
0 \textrm{ when } \mathbf{a}^T_j\mathbf{x}\leq b_j\\
1 \textrm{ when } \mathbf{a}^T_j\mathbf{x} > b_j %( = \mathbf{x} \in P_j)
\end{array}\right.
\end{equation}
Using the or-form of the set condition (the second line of Eq.\ \eqref{eq:implication})
and the definition of $\mathbf{e}$, we can write the mixed-data edit as
\begin{equation}
\label{eq:mixedit}
\mathbf{e}^T\mathbf{d}(\mathbf{v}) + \sum^{k}_{j=1} (1-p_j) \geq 1.
\end{equation}
Rules of this form constitute the user-defined part of the $\mathbf{R}^H$ part
of the restriction matrix. To connect $p_j$ with the linear restrictions we
also add
\begin{equation}
\label{eq:linconsequent}
\mathbf{a}^T_j\mathbf{x} \leq b_j + Mp_j,
\end{equation}
to $\mathbf{R}^H$ with $M$ a suitably large positive constant. Indeed, if
$p_j=0$, the inequality $\mathbf{a}^T_j\mathbf{x}\leq b_j$ is enforced. When
$p_j=1$ it is unimportant for the whole restriction to hold whether the
inequality holds.  Finally, similar to the purely categorical case we need to
add restrictions on the binary representation of $\mathbf{v}$ as in Eq.\
\eqref{eq:hardmaxcat}, so Eq.\ \eqref{eq:hardmaxcat}, Eq.\
\eqref{eq:linconsequent} and Eq.\ \eqref{eq:mixedit} constitute $\mathbf{R}^H$.

In general there may be $k$ multiple mixed-type edits yielding an equal number of indicator
variables for inequalities in the consequent. So the vector to minimize over becomes
\begin{equation}
\mathbf{z} = (\mathbf{d}(\mathbf{v}),\mathbf{x},\Delta_1,\ldots,\Delta_m,\ldots,\Delta_{m+n},
l_1,\ldots,l_K),
\end{equation}
where $K$ is the total number of linear edits occuring in all the mixed-type edits.
Finally, the $\mathbf{R}^0$ matrix, connecting the $\Delta$ variables with the actual
recorded values consists of the union of the restrictions for categorical variables
(Eq.\ \eqref{eq:deltacat}) and those for numerical variables (Eq.\ \eqref{eq:deltanum}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical stability issues}
\label{sec:linedits:num}
Reformulating the error localization problem in terms of a mip problem
introduces two constants: $M$ and $\epsilon$. These constants end up in the
coefficient matrix of the mip solver.  Mathematically, there are no bounds on
the values of $M$ and $\epsilon$, as long as they are chosen so that the
problem remains feasible. In practice however, limitations on the accuracy with
which numbers are represented on a computer put restrictions on the value range
of numbers that enter the mip solver. In the \code{lpsolve} library (as well
as in \R{}), numbers on the real line are represented as \code{double precision}
numbers, meaning that there are about 15 decimal positions of precision. 

To avoid significant round off errors, it is a good idea to restrict the range
of input data. In fact, the authors of the \code{lpsolve} reference manual
\citep{lpsolveman:2012} state the following.
%
\begin{quote}
The chance for numerical instability and rounding errors is considerably 
larger when the input data contains both large and small numbers. 
So to improve stability, one must try to work with numbers that are somewhat 
in the same range. Ideally in the neighborhood of 1.
[...]You should realize, that you the user are probably in a better position to 
scale the problem than any computer algorithm. 
\end{quote}


\begin{equation}
\left[\begin{array}{c|c}
  \mathbf{A}^H &\mathbf{b}^H\\
  \mathbf{A}^0 &\mathbf{b}^0
\end{array}\right] 
= 
\left[\begin{array}{rr|r}
  \multicolumn{1}{c}{ \mathbf{A}^H} & \multicolumn{1}{c|}{\mathbf{0}} & \mathbf{b}^H\\
  \mathbbm{1} & M\mathbbm{1}  &\mathbf{x}^0\\
  -\mathbbm{1} & -M\mathbbm{1} &-\mathbf{x^0}
\end{array}\right] 
\end{equation}
In {\sf editrules} by default $M = 10^7$ and $\epsilon = 10^{-3}$ is used. This 
will 
be sufficient for most purposes, since numerical values and coefficients of the
constraints 
in most cases will be in this range. If not, you should rescale some of your
variables and constraints to make it so.

The linear programming manual from \code{lpsolve}  \citep{lpsolveman:2012} says the 
following on this matter:


\section{Usage}
\subsection{Error localization}
\code{editrules} provides the \code{localizeErrors} function for finding errors
in a \code{data.frame} given an \code{editset}, \code{editmatrix} or \code{editarray}.
The \code{localizeErrors} function is extended to include an extra parameter
\code{method}. The default value of \code{method} is \code{'bb'}: branch and bound.
Setting \code{method} to \code{'mip'} gives identical but faster result. 
This implementation makes it possible to switch between the two implementations.
<<num example>>=
E <- editmatrix("x <= y")
dat <- data.frame(x=10, y=1)
weight <- data.frame(x=2, y=1)

localizeErrors(E, dat, weight=weight, method="bb")$adapt
localizeErrors(E, dat, weight=weight, method="mip")$adapt
@
When \code{method=mip} is specified \code{localizeErrors} uses for each record 
the function \code{errorLocalizer\_mip}.
Unlike {\em branch and bound} this is not a \code{backtracker} object.
\code{errorlocalizer\_mip} writes the constraints and values into a mip problems,
feeds it into the \code{lpSolveApi} and formats the resulting output.
<<num mip>>=
el <- errorLocalizer_mip(E, dat[1,])
ls.str(el)
@
Both \code{localizeErrors} as \code{errorLocalizer\_mip} can be executed with an 
\code{editmatrix}, \code{editarray} or \code{editset} object.

Note in the example that the {\em mip} solver returns two extra properties that are
not available in {\em branch and bound}: \code{lp} and \code{x\_feasible}.
Since a mip-solvers goal is to find a feasible solution, a set of feasible x 
values is available. It can be helpful to have these values, but note 
that this solution in general is not unique and that the proposed values for the 
variables to be adjusted lie on the boundary of a convex solution region. 
In many cases it is better to use a proper imputation method to generate
sensible values for the variable that needs to be adjusted.
Secondly \code{errorlocalizer\_mip} returns the \code{lp} object. This object is 
a \code{lpSolveApi} object and can be further manipulated or written to disk.
For more information consult the manual of \code{lpSolveApi}.

An advantage of the branch and bound algorithm is that it can generate all optimal
solutions for a given record.  The mip implementation only generates one of the 
best. This is implemented by adding a small uniform perturbation to the weights, 
so mip chooses one of the best at random. This is identical 
to the default behavior of \code{localizeErrors} for \code{method='bb'}.

\subsection{Setting up your own lp problem}
\code{errorLocalizer\_mip} uses the function \code{as.mip} to transform a set of 
edits and a record into a mip problem. 

\code{as.mip} implements the mip formulations of section \ref{sec:mipproblem}.
We illustrate this with a small example of a mixed-typed edit.

<<tidy=FALSE>>=
E <- editset(expression(
  married %in% c(TRUE, FALSE),
  if (married) age >= 17
  ))
@
The hard restrictions \code{E} can be transformed into ${\bf R}^H$ with
\code{as.mip}
<<>>=
as.mip(E)
@
\code{num1} states that variable \code{.num.1} and \code{married} cannot both be true 
($=1$).\\
If \code{married==TRUE} than the restriction of line 2 turns into 
\code{age >= 17}.

Supplying values \code{x} to the function \code{as.mip} adds ${\bf R}^0$:
<<>>=
as.mip(E, x=list(age=9, married=TRUE))
@
Two variables \code{delta.age} and \code{delta.married} are introduced, that
will signify if variables \code{age} and \code{married} need to be adapted.\\
Line 3 and 4 try to restrict \code{age}  to $9$. As can be seen the value of $M$
is set to $10^7$. The value of $M$ can be supplied to \code{as.mip}.
If \code{delta.age} is $1$  the value of \code{age} is not restricted
to $9$, but to $[-10^7+9, 10^7+9]$.\\
Line 5 tries to restrict 
\code{married} to \code{TRUE}.\\
The last line shows the objective function, which
can be influenced by supplying \code{weight} to \code{as.mip}.

The mip object is transformed into a 
\code{lp} object needed by \code{lpSolveAPI} with the function \code{as.lp.mip}.

\section{Benchmarks}

The {\emph branch and bound} and {\emph mip} method for error localization 
have a different performance.
Both the memory and time complexity is different. The performance of error 
localization methods depends highly on the number of variables, the number of errors,
the number of constraints and the order of the variables.
The performance of numerical error localization with branch and bound is reasonable,
but for categorical and mixed records rapidly becomes unsatisfying for the typical
problem sizes of 100 variables and 100 constraints.

To compare the performance we conducted benchmarks for the three types of constraints
using edits with synthetic data. However the benchmarks reflect properties of a 
typical error localization problem in practice.  

\subsection{Numerical linear edits}
The synthetic edits used in this benchmark are a balance system. This type of edits
are found frequently e.g. in business statistics and energy statistics. 
The numerical
variables form a binary tree where the value of a node is the sum of its child node
values. The value of the top node is constraint to be non-negative.
<<>>=
# plot example of balance_mip (generate_E())
@

A solution to this system is the zero vector $(x_1, \ldots, x_n) = (0,\ldots,0)$.
We can introduce an error in the data by setting an $x_i$ to $-1$. This way we 
can test the algorithm with increasing number of errors. 

This benchmark was run with the number of variables up to 100 and the number of errors
up to 10.

\todo[inline]{results}

\subsection{Categorical edits}
For categorical edits we create a chain of categorical edits of the form:\\
\code{if ($A_{i}$==TRUE) $A_{i+1}$ == TRUE}, for \code{$i$} is $1$ to $n$.\\
We restrict ${A_1}$ to \code{TRUE}.
In practice categorical edits form a web of interconnected edits. The chain of
edits reflects this connectivity.

\todo[inline]{plot of generated chain for $n = 10$}

The solution to this system is the vector \code{(TRUE, ..., TRUE)}.
We can introduce an error by setting an $A_i$ to \code{FALSE}.

This benchmark was run with the number of variables up to 100 and the number of errors
up to $10$.
\todo[inline]{results}.

\subsection{Mixed-type edits}
For mixed edits a set of edits of the following form was generated:
\code{if ($C_i$ == TRUE) $n_i >= 0$}, furthermore the constraints 
$n_1 = \sum^{N}_{i=2}$ and $n_1 >= 0$ are added.

\todo[inline]{plot of generated editset for $n = 10$}

The solution to this system is the vector \code{($0$, ..., $0$, TRUE, ..., TRUE)}.
We can introduce an error by setting an $C_i$ to \code{FALSE} or setting a $n_i$
to $-1$.

This benchmark was run with the number of variables up to 100 and the number of errors
up to 10.
\todo[inline]{results}.

\section{Conclusion}
This paper describes the error localization algorithm for linear, categorical and
mixed edits as mip problems. It describes the usage and implementation in
\code{editrules}. This mip implementation is typically much faster than the
branch and bound algorithm as can be seen from the benchmarks.

For most problems mip will generate a good solution. However for problems where
coefficients or values exceed the range of $[10^{-3} - 10^7]$ the user has to rescale
the problem: otherwise the mip solver will become unstable.

\bibliographystyle{chicago}
\bibliography{editrules}

\end{document}

