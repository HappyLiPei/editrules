\documentclass[10pt, fleqn, english, rapport]{cbsdiscussionpaper}
\usepackage[english]{babel}
\usepackage{amsmath, amsthm}
\usepackage{bbm} % mathbb numbers
\usepackage{array}
\usepackage{natbib}
\usepackage{threeparttable}
\usepackage{makeidx}
\usepackage{todonotes}
\usepackage{listings}


% fgcolor komt uit knitr. Pas het knitr theme aan om te veranderen.
% originele color is \color[rgb]{0.345, 0.345, 0.345}
\lstset{basicstyle=\small\color{fgcolor},frame=lines,rulecolor=\basecolor}
\makeindex

% Footnote symbols to avoid confusion with superscript references.
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator{\fl}{\textrm{fl}}
\DeclareMathOperator*{\Lor}{\lor}
\DeclareMathOperator*{\Land}{\land}
\DeclareMathOperator*{\Minimize}{\textrm{\sf Minimize}}


\renewcommand{\code}[1]{\textcolor{fgcolor}{\texttt{#1}}}
\newcommand{\R}{\code{R}} %% call as \R{} to avoid extra spaces.

% \newcommand{\Fc}{\mathbf{\overline{F}}}
% \newcommand{\Ls}{\mathbf{L}}
% \newcommand{\FD}{\mathbb{F}}
% \newcommand{\FDc}{\overline{\mathbb{F}}}


\usepackage{float} 

\newtheorem{example}{Example}

% stimulate latex to put multiple floats on a page.
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{3}
\setcounter{dbltopnumber}{2}
\renewcommand{\topfraction}{.9}
\renewcommand{\textfraction}{.1}
\renewcommand{\bottomfraction}{.75}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\dblfloatpagefraction}{.9}
\renewcommand{\dbltopfraction}{.9}


\hyphenation{
}

<<setup, echo=FALSE, results='hide', message=FALSE>>=
library(editrules)
library(knitr)
set.seed(41)
opts_chunk$set(
   size="small"
  , highlight=FALSE
  , background=rep(1,3)
  , prompt=TRUE
  , comment=NA
)
@

\renewenvironment{kframe}{}{}
%\renewenvironment{knitrout}
%  {\vspace{1ex}\basecolor\hrule\vspace{-3ex}}
%  {\basecolor\hrule\vspace{-2ex}}

\newenvironment{rcode}
  {\vspace{1ex}\basecolor\hrule\vspace{-3ex}}
  {\basecolor\hrule\vspace{-2ex}}

\title{Error localization as a mixed integer problem with the editrules package}
\author{Edwin de Jonge\\  Mark van der Loo}
\Number{2014}{07}
\DocumentNumber{60083201407 X-10}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle
\begin{abstract}
Error localization is the problem of finding out which fields in raw data
records contain erroneous values. The \code{editrules} extension package for
the \R{} environment for statistical computing was recently extended with a
module that allows for error localization based on a mixed integer programming
formulation (MIP). In this paper we describe the MIP formulation of the error
localization problem for the case of numerical, categorical, or mixed numerical
and categorical datasets. We introduce a MIP formulation that is a
generalization of both linear as well as categorical restrictions. We discuss
the numerical boundaries within which a MIP solver generates a stable solution
and give directions on changing them to your own needs.  The new module is
benchmarked against a previously available module, which is based on a
branch-and-bound approach. The benchmark shows that the MIP-based approach is
significantly faster.  Trade-offs between the branch-and-bound and MIP
approaches are discussed as well.
\end{abstract}

\newpage
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sect:introduction}
Analyses of data are often hindered by occurrences of incomplete or
inconsistent raw data records.  The process of locating and correcting such
errors is referred to as {\em data editing}, and it has been estimated that
National Statistics Institutes may spend up to 40\% of their resources on this
process \citep{waal:2011}. Moreover, data often must obey many
cross-variable consistency rules which significantly complicate the data
editing process. Indeed, \citet{winkler:1999} mentions household surveys
where records have to obey 250, 300 or even 750
user-defined interrelated consistency rules.  For these reasons, considerable
attention is paid to the development of data editing methods that can be
automated. 

\subsection{Error localization}
Automated as well as manual data editing strategies for a data record 
typically consist of three steps:
\begin{enumerate}
\item Find out which consistency rules a record violates;
\item Find out which fields in a record cause those violations;
\item Replace the values in those fields with better estimates, such that no rule
is violated.
\end{enumerate}
The second step is usually referred to as the \emph{error localization}
problem, which is the focus of the current paper.  Although it is widely
recognized that data editing is a necessary step in the statistical process,
the amount of changes made to the data should obviously be minimized to avoid
introducing bias in estimations based on the edited data. This then leads to
the following minimization problem.
%
\begin{quote}
Given a record of $n$ variables, subject to a number of possibly multivariate
consistency rules. Find the smallest (weighted) subset of fields, such that 
after replacement of their values, the record violates no rules. 
\end{quote}
%
This minimization problem is named after \cite{fellegi:1976}, who first
formulated and solved the problem for the case of categorical data. Error
localization has been extensively discussed in literature\footnote{See
\citet{waal:2011} and references therein.}, so we will suffice with a few
remarks. First, the search space related to the minimization problem grows
exponentially with the number of fields, rendering a brute-force approach that
runs through all possible solution candidates computationally unfeasible.
Second, the problem is complicated by the occurrence of \emph{implied rules}.
That is, the solution set must not only allow the record to obey the original,
user-defined set of rules, but also rules that are logically or arithmetically
implied by the original set. To cope with these complications several
algorithmic approaches have been developed, two of which are worth mentioning
in this context. The first is the branch-and-bound approach developed by
\cite{waal:2003a}.  The second is an approach based on mixed-integer
programming (MIP) described in \cite{waal:2011}.

\subsection{The editrules package}
Over the past decade the \R{} statistical environment has received a surge in
popularity. As a consequence it has been extended with many user-built packages
that allow for statistical analyses of data. However, the number of packages
specifically aimed at data editing seems to be somewhat limited, except
possibly in the area of imputation.  The \R{} package {\sf editrules}
\citep{jonge:2011a} was developed to help to bridge the gap between raw data
retrieval and data analysis with \R{}.  The main purpose of the package is to
provide a convenient interface to define data consistency rules (often referred
to as \emph{edit rules}) in \R{} and to confront them with data. Furthermore,
the package allows for basic rule manipulation (deriving new rules, finding
inconsistencies, \emph{etc.}) and for error localization functionality.  As
such, the package does not offer functionality to correct data.  Rather, it is
aimed at identifying the set of solutions to an error localization problem: the
second step mentioned in the data editing strategy above. Previous developments
of the package have been described in \cite{jonge:2011,loo:2011b} and
\cite{loo:2011a}.


The \code{editrules} package offers a toolbox that allows users to work with
numerical, categorical or mixed-type data editing rules. Up until now, error
localization was performed by an implementation of the branch-and-bound
algorithm described by \cite{waal:2003}. The main disadvantage of this approach
is that the branch-and-bound algorithm has $\mathcal{O}(2^n)$ worst-case time
and memory complexity, where $n$ is the number of variables occurring in a
connected set of rules. Moreover, the branch-and-bound solver is written in
pure \R{}, making it intrinsically slower than a compiled language
implementation. The main advantages of this approach are the ease of
implementation and the opportunity for users to exert fine-grained control over
the algorithm.

As stated before, the error localization problem can be translated to a
mixed-integer programming problem. This allows us to reuse
well-established results from the field of linear and mixed-integer
programming. Indeed, many advanced algorithms for solving such problems have
been developed, and in many cases implementations in a compiled language are
available under a permissive license. In \code{editrules}, the solver of the
\code{lp\_solve} library \citep{berkelaar:2010} is used through \R{}'s
\code{lpSolveAPI} package \citep{lpSolveAPI:2011}. The \code{lp\_solve} library
is written in \code{ANSI} \code{C} and has been tried and tested extensively.

The strategy to solve error localization problems through this library from 
\R{} therefore consists of translating the problem to a suitable mixed-integer
programming problem, feeding this problem to \code{lpSolveAPI}, and translating
the results back to an error location. It is necessary
to distinguish between:
\begin{itemize}
  \item linear restrictions on purely numerical data,
  \item restrictions on purely categorical data, and
  \item conditional restrictions on mixed-type data,
\end{itemize}
since restriction for each data type calls for a different translation to a MIP
problem.

The second part of this paper will focus on how to translate these types of
error localization problems to a mixed-integer formulation, paying attention to
both theoretical and practical details. In Section \ref{sec:linedits:num}
attention is payed to numerical stability issues, Section \ref{sec:usage} is
devoted to examples in \R{} code and Section \ref{sec:benchmarks} describes
benchmark results.  Conclusions and a further outlook are described in Section
\ref{sec:conclusion}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Error localization and mixed integer programming}
\label{sec:mipproblem}
%
A mixed integer programming problem is an optimization problem that can be
written in the form
\begin{equation}
\begin{array}{r}
\textrm{Minimize } f(\mathbf{z}) = \mathbf{c}^T\mathbf{z}; \\
\textrm{s.t. }\mathbf{Rz} \leq \mathbf{d},
\label{eq:mipmin}
\end{array}
\end{equation}
%
where $\mathbf{c}$ is a constant vector and $\mathbf{z}$ is a vector consisting
of real and integer coefficients. One usually refers to $\mathbf{z}$ as the
\emph{decision vector} and the inner product $\mathbf{c}^T\mathbf{z}$ as the
\emph{objective function}.  Furthermore, $\mathbf{R}$ is a coefficient matrix
and $\mathbf{d}$ a vector of upper bounds. Formally, the elements of
$\mathbf{c}$, $\mathbf{R}$ and $\mathbf{d}$ are limited to the rational numbers
\citep{schrijver:1998}. This is never a problem in practice since we are
always working with a computer representation of numbers. 

The name \emph{mixed-integer programming} stems from the fact that $\mathbf{z}$
contains continuous as well as integer variables. When $\mathbf{z}$ consists
solely of continuous or integer variables, Problem~\eqref{eq:mipmin} reduces
respectively to a \emph{linear} or an \emph{integer programming} problem.  An
important special case occurs when the integer coefficients of $\mathbf{z}$ may
only take values from \{0,1\}. Such variables are often called binary variables. 
It occurs as a special case since defining $\mathbf{z}$ to
be integer and applying the appropriate upper bounds yields the same problem.

Mixed integer programming is well understood and several software 
packages are available that implement efficient solvers.  Most MIP software
support a broader, but equivalent, formulation of the MIP problem, allowing the
set of restrictions to include inequalities as well as equalities. As a side
note we mention that under equality restrictions, solutions for the integer
part of $\mathbf{z}$ are only guaranteed to exist when the equality
restrictions pertaining to the integer part of $\mathbf{z}$ are \emph{totally
unimodular}\footnote{This means that every square submatrix of the coefficient
matrix $\mathbf{R}$ pertaining to the integer part of $\mathbf{z}$ has
determinant 0 or $\pm1$.}. However, as we will see below, restrictions on
$\mathbf{z}$ are always inequalities in our case, so this is of no particular
concern to us.

In this paper we reformulate Fellegi Holt error localization
\citep{fellegi:1976} for numerical, categorical and mixed-type restrictions in
terms of MIP problems.  The precise reformulations of the error localization
problem for the three types of rules are different, but in each case the
objective function is of the form 
%
\begin{equation}
\label{eq:objfun}
\mathbf{w}^T\mathbf{\Delta},
\end{equation}
where $\mathbf{w}$ is a vector of positive weights and $\mathbf{\Delta}$ a vector
of binary variables, one for each variable in the original record, that
indicates whether its value should be replaced. More precisely,
for a record $\mathbf{r}=(r_1,r_2,\ldots,r_n)$ of $n$ variables, we define
\begin{equation}
\Delta_i =\left\{\begin{array}{l}
1 \textrm{ if the value of }r_i\textrm{ must be replaced}\\
0 \textrm{ otherwise}.
\end{array}\right.
\label{eq:defineDelta}
\end{equation}
%
This objective function obviously  meets the requirement that the minimal
(weighted) number of variables should be replaced.  In general, a record may
contain numeric, categorical or both types of data and restrictions may pertain
to either one or both data types.  To distinguish between the data types below
we shall write $\mathbf{r}=(\mathbf{v},\mathbf{x})$ where $\mathbf{v}$
represents the categorical and $\mathbf{x}$ the numerical part of $\mathbf{r}$.

For an error localization problem, the restrictions of Problem
\eqref{eq:mipmin} consist of two parts, which we denote
\begin{equation}
\left[\begin{array}{c}
  \mathbf{R}^H\\
  \mathbf{R}^0
\end{array}\right] \mathbf{z} \leq 
\left[\begin{array}{c}
  \mathbf{d}^H\\
  \mathbf{d}^0
\end{array}\right].
\label{eq:errlocasmip}
\end{equation}
Here, the restrictions indicated with $H$ represent a matrix representation of
the user-defined (hard) restrictions that the original record $\mathbf{r}$ must
obey. 
The vector
$\mathbf{z}$ contains at least a numerical representation of the values in a
record $\mathbf{r}$ and the binary variables $\mathbf{\Delta}$. An algorithmic
MIP-solver will iteratively alter the values of $\mathbf{z}$ until a solution
satisfying \eqref{eq:errlocasmip} is reached. To make sure that the objective
function reflects the (weighted) number of variables altered in the process,
the restrictions in $\mathbf{R}^0$ serve to make sure that the values in
$\mathbf{z}$ that represent values in $\mathbf{r}$ cannot be altered without
setting the corresponding value in $\mathbf{\Delta}$ to 1. 

Summarizing, in order to translate the error localization problem for the
special cases of linear, categorical or conditional mixed-type restrictions to
a general mixed integer problem, for each case we need to properly define
$\mathbf{z}$, the restriction set $\mathbf{R}^H\mathbf{z}\leq \mathbf{d}^H$ and
the restriction set $\mathbf{R}^0\mathbf{z}\leq \mathbf{d}^0$.

\subsection{Linear restrictions}
\label{sec:linedits:mip}
For a numerical record $\mathbf{x}$ taking values in $\mathbb{R}^n$, a set of linear
restrictions can be written as
\begin{equation}
\label{eq:linedits}
\mathbf{Ax}\leq \mathbf{b},
\end{equation}
where in \code{editrules}, we allow the set of restrictions to contain equalities,
inequalities ($\leq$) and strict inequalities ($<$). 
The formulation of these edit rules is very close to the formulation of the original
MIP problem of Eq.\ \eqref{eq:mipmin}.
The vector to minimize over is defined as follows:
\begin{equation}
\label{eq:zlin}
\mathbf{z} = (x_1,x_2,\ldots,x_n,\Delta_1,\Delta_2,\ldots,\Delta_n).
\end{equation}
with the $\Delta_i$ as in Eq.\ \eqref{eq:defineDelta}. The set of restrictions
$\mathbf{R}^H\mathbf{z}\leq \mathbf{d}^H$ is equal to the set of restrictions
of Eq.~\eqref{eq:linedits}, except in the case of strict inequalities. The
reason is that while \code{editrules} allows the user to define strict
inequalities ($<$), the \code{lpsolve} library used by \code{editrules} only allows
for inclusive inequalities ($\leq$). For this reason, strict inequalities of the
form $\mathbf{a}^T\mathbf{x}<b$ are rewritten as $\mathbf{a}^T\mathbf{x}\leq
b-\epsilon$, with $\epsilon$ a suitably small positive constant.

In the case of linear edits, the set of constraints $\mathbf{R}^0\mathbf{z}\leq\mathbf{d}^0$
consists of pairs of the form
\begin{eqnarray}
  x_i - M\Delta_i   &\leq& x_i^0\nonumber\\
  -x_i - M\Delta_i  &\leq& -x^0_i
\label{eq:deltanum}
\end{eqnarray}
for $i=1,2,\ldots,n$. Here The $x^0_i$ are the actual observed values in the
record and $M$ is a suitably large positive constant allowing $x_i$ to vary between
$x_i^0-M$ and $x_i^0+M$. It is not difficult to see that if $x_i$ is different
from $x_i^0$ then $\Delta_i$ must equal 1. For, if we choose
$\Delta_i=0$ we obtain the set of restrictions
\begin{equation}
  x_i^0 \leq  x_i  \leq x_i^0
\end{equation}
which states that $x_i$ equals $x_i^0$

\begin{example}
Consider a record with business survey data, consisting of the variables
\emph{Number of staff} $p$ and \emph{Personnel cost}  $c$.  We have the rules
$p\geq0$, $c\geq 0$ and $c\geq p$. The latter rule expresses the notion that
for each staff member, more than one monetary unit is spent. Given two observed
values $p^0$ and $c^0$, disobeying one or more of the rules, the MIP problem
for error localization has the following form.
\begin{displaymath}
 \Minimize_{(\mathbf{x},\mathbf{\Delta})\in\mathbb{R}^2\times\{0,1\}^2} \Delta_p + \Delta_c
\end{displaymath}
\begin{equation*}
  \textrm{s.t. }\left[\begin{array}{rrrr}
    1 & -1 & 0 & 0\\
   -1 &  0 & 0 & 0\\
    0 & -1 & 0 & 0\\
    1 &  0 & -M & 0\\
   -1 &  0 & -M & 0\\
    0 &  1 & 0 & -M\\
   -1 &  1 & 0 & -M\\
  \end{array}\right]
  \left[\begin{array}{c}
   p \\ c \\ \Delta_p \\ \Delta_c
  \end{array}\right] \leq
  \left[\begin{array}{c}
   0 \\ 0 \\ 0 \\ p^0\\-p^0 \\ c^0\\ -c^0
  \end{array}\right].
\end{equation*}
Here, the first three rows in the set of restrictions represent the consistency rules
while the other rows connect the indicator variables $\mathbf{\Delta}=(\Delta_p,\Delta_c)$
with $p$ and $c$. 
\hfill\qed
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Categorical restrictions}
\label{sec:catdataerror}
Categorical records $\mathbf{v}\in D$ take values in a Cartesian product domain 
\begin{equation}
\label{eq:defineD}
D = D_1\times D_2\times\cdots\times D_m,
\end{equation}
where each $D_i$ is a finite set of categories for the $i^\textrm{th}$
categorical variable. The category names are unimportant so we write
\begin{equation}
\label{eq:defineD1}
D_i = \{1,2,\ldots,|D_i|\}.
\end{equation}
The total number of possible value combinations $|D|$ is equal to the product
of the $|D_i|$.

A categorical edit is a subset $F$ of $D$ where records are
considered invalid, and we may write
\begin{equation}
\label{eq:defcatedit}
F = F_{1}\times F_{2}\times\cdots \times F_{m},
\end{equation}
where each $F_{i}$ is a subset of $D_i$. It is understood that if a record
$\mathbf{v}\in F$ then the record violates the edit.
Hence, categorical edits are
negatively formulated (they specify the region of $D$ where $\mathbf{v}$ may
not be) in contrast to linear edits which are positively formulated (they
specify the region of $\mathbb{R}^n$ where $\mathbf{x}$ must be). To be able to
translate categorical edits to a MIP problem, we need to specify 
$\overline{F}$, such that if $\mathbf{v}\in\overline{F}$ then
$\mathbf{v}$ satisfies $e$.  Here, $\overline{F}$ is the complement of
$F$ in $D$, which can be written as
%
\begin{eqnarray}
\label{eq:invedit}
\lefteqn{
\overline{F} = \overline{F}_{1}\times D_{2}\times\cdots \times D_{m}
}\nonumber\\
&\cup& D_{1}\times \overline{F}_{2}\times\cdots \times D_{m}\cup\cdots 
\cup D_{1}\times D_{2}\times\cdots \times \overline{F}_{m},
\label{eq:complement}
\end{eqnarray}
%
where for each variable $v_i$, $\overline{F}_{i}$ is the complement of
$F_{i}$ in $D_i$.  Observe that Eq.\ \eqref{eq:complement} states that if at
least one $v_i\in\overline{F}_{i}$, then $\mathbf{v}$ satisfies $e$. 
Below, we will use this property and construct a linear relation that counts
the number of $v_i\in\overline{F}_i$ over all variables.

To be able to formulate the Fellegi Holt-problem in terms of a MIP problem, we
first associate with each categorical variable $v_i$ a binary vector
$\mathbf{d}$ of which the coefficients are defined as follows (see also Eq.\
\eqref{eq:defineD}).
\begin{equation}
\label{eq:definedvec}
d_\lambda(v_i) = \left\{\begin{array}{l}
1\textrm{ if } v_i = \lambda\\
0\textrm{ otherwise},
\end{array}\right.
\end{equation}
where $\lambda\in D_i$.
Thus, each element of $\mathbf{d}(v_i)$ corresponds to one category in $D_i$.
It is zero everywhere except at the value of $v_i\in D_i$. We will write
$\mathbf{d}(\mathbf{v})$ to indicate the concatenated vector
$(\mathbf{d}(v_1),\ldots,\mathbf{d}(v_m))$ which represents a complete record.
Similarly, each edit can be represented by a binary vector $\mathbf{e}$ given by
\begin{equation}
\mathbf{e} = \left(\bigvee _{\lambda\in \overline{F}_1}\mathbf{d}(\lambda), \ldots ,\bigvee _{\lambda\in \overline{F}_m}\mathbf{d}(\lambda)
\right),
\label{eq:ecat}
\end{equation}
where we interpret 1 and 0 as \code{TRUE} and \code{FALSE} respectively and
the logical 'or' ($\lor$) is applied element-wise to the coefficients of
$\mathbf{d}$.  The above relation can be interpreted as stating that
$\mathbf{e}$ represents the valid value combinations of variables contained in
the edit. 

To set up the hard restriction matrix $\mathbf{R}^H$ of  Eq.\ \eqref{eq:errlocasmip},
we first impose the obvious restriction that each variable can take but a single value:
\begin{equation}
\label{eq:hardmaxcat}
\sum_{\lambda\in D_i}d_\lambda(v_i) = 1,
\end{equation}
for $i=1,2,\ldots,m$. It is now not difficult to see that the demand (Eq.\ \eqref{eq:complement}) that at 
least one of the $v_i\in\overline{F}_i$ may be written as
\begin{equation}
\label{eq:hardcat}
\mathbf{e}^T\mathbf{d}(\mathbf{v}) \geq 1.
\end{equation}
Equations \eqref{eq:hardmaxcat} and \eqref{eq:hardcat} constitute the hard restrictions, stored in
$\mathbf{R}^H$.

Using the binary vector notation for $\mathbf{v}$, and adding the $\Delta$-variables
that indicate variable change, the vector to minimize over (Eq.\
\eqref{eq:mipmin}) is written as
\begin{equation}
\mathbf{z} = (\mathbf{d}(\mathbf{v}),\Delta_1,\Delta_2,\ldots,\Delta_m).
\end{equation}
To ensure that a change in $v_i$ results in a change in $\Delta_i$, the 
matrix $\mathbf{R}^0$ contains the restrictions
\begin{equation}
\label{eq:deltacat}
d_{\lambda^0}(v_i) =  1-\Delta_i,
\end{equation}
for $i=1,2,\ldots,m$.  Here, $\lambda^0\in D_i$ is the observed value for
variable $v_i$. One may check, using Eq.~\eqref{eq:definedvec}, that the above
equation can only hold when either $v_i=\lambda^0$ and $\Delta_i=0$ (the
original value is retained) or $v_i\not=\lambda^0$ and $\Delta_i=1$ (the value
changes).

\begin{example}
Consider a two-variable record from the census with the variables \emph{Marital status} 
$m$ and \emph{Age class} $a$. We have $\mathbf{v}=(m,a)\in D$ where
\begin{equation*}
D = D_m\times D_a = \{\textsf{married, unmarried}\}\times\{\textsf{child, adult}\}.
\end{equation*}
Using the binary representation we see that a married adult is represented by the vector
$\mathbf{v}^0=(\mathbf{d}(\textsf{married}),\mathbf{d}(\textsf{adult})) = (1,0,0,1)$. The rule that
states ``\emph{A child cannot be married}'' translates to
\begin{equation*}
F = F_m\times F_a =  \{\textsf{married}\}\times\{\textsf{child}\}
\end{equation*}
which gives $\overline{F}_m=\{\textrm{unmarried}\}$ and $\overline{F}_a=\{\textrm{adult}\}$.
Using Eq.\ \eqref{eq:ecat} we get $\mathbf{e}=(0,1,0,1)$ and one may verify
that $\mathbf{e}^T\mathbf{d}(\textsf{married, child})=0$ and therefore invalid
(see \ref{eq:hardcat}).
For $\mathbf{v}^0$, the MIP
problem for error localiation now looks like this.
\begin{equation}
  \Minimize_{(\mathbf{v},\mathbf{\Delta})\in D\times \{0,1\}^2} \Delta_m + \Delta_a
\end{equation}
\begin{equation*}
  \textrm{s.t. }\left[\begin{array}{rrrrrr}
    1 &  0 & 1 & 0 & 0 & 0\\
    1 &  1 & 0 & 0 & 0 & 0\\
    0 &  0 & 1 & 1 & 0 & 0\\
    1 &  0 & 0 & 0 &-1 & 0\\
    0 &  0 & 1 & 0 & 0 &-1\\
  \end{array}\right]
  \left[\begin{array}{c}
   d_{\textrm{married}}(m) \\ d_{\textrm{unmarried}}(m) \\ 
   d_{\textrm{child}}(a)\\ d_{\textrm{adult}}(a)\\ \Delta_m \\ \Delta_a
  \end{array}\right]
\begin{array}{c}
\geq\\=\\=\\=\\=
\end{array}
  \left[\begin{array}{c}
   1 \\ 1 \\ 1 \\ 1 \\ 1
  \end{array}\right].
\end{equation*}
Here, the first row represents the edit rule, the second and third force that
each variable can take but one value (Eq.\ \ref{eq:hardcat}), and the last two
rows connect the indicator variables $\Delta_m$ and $\Delta_a$ with the value
of $m$ and $a$ (Eq.\ \ref{eq:deltacat}).
\hfill\qed
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Mixed-type restrictions}
\label{sec:mixdataerror}
Records $\mathbf{r}$ containing both numerical and categorical data can be
denoted as a concatenation of categorical and numerical variables taking values
in $D\times\mathbb{R}^n$: 
\begin{equation}
\mathbf{r} = (v_1,\ldots,v_m,x_1,\ldots,x_n) = (\mathbf{v},\mathbf{x}),
\end{equation}
where $D$ is defined in Eq.\ \eqref{eq:defineD}. 
As stated above, categorical edits are usually defined negatively as a region of
$D$ that is disallowed while linear edits define regions in $\mathbb{R}^n$ that
are allowed. We may choose a negative formulation of edits containing
both variable types by defining a single edit $E$ as follows: 
\begin{equation}
E = \{\mathbf{r}\in D \times\mathbb{R}^n: 
\mathbf{v}\in F \land \mathbf{x} \in P \},
\end{equation}
where $F \subseteq D$ and $P$ is a convex subset of $\mathbb{R}^n$ defined by a
(possibly empty) set of $k$ linear inequalities of the form $\mathbf{a}^T\mathbf{x}
> b$.  It is understood that if $\mathbf{r} \in E$, then $\mathbf{r}$ violates
the edit. An example of a restriction pertaining to a categorical and a numerical
variable is `A company employs staff if and only if it has positive personell cost',
The corresponding edit can be denoted as $\{\textrm{no staff}\}\times\{c>0\}$.



To obtain a positive reformulation, we first negate the set membership
condition and apply basic rules of proposition logic:
\begin{eqnarray}
&&\lnot\big(\mathbf{v}\in F \land \mathbf{x} \in P \big)\nonumber\\
&\Leftrightarrow& \lnot\big(\mathbf{v}\in F \land \mathbf{a}^T_{1}\mathbf{x} > b_{1} 
\land \cdots \land \mathbf{a}^T_{k}\mathbf{x} > b_{k}\big)\nonumber\\
&\Leftrightarrow& \mathbf{v}\in \overline{F} \lor \mathbf{a}^T_{1}\mathbf{x}\leq b_{1} 
\lor \cdots \lor \mathbf{a}^T_{k}\mathbf{x}\leq b_{k}.
\label{eq:implication}
\end{eqnarray}
This then yields a positive formulation of $E$. That is, a record $\mathbf{r}$
satisfies $E$ if and only if
\begin{equation}
  \label{eq:mixedtype}
  \mathbf{r} \in \overline{E} \Leftrightarrow 
\bigvee_{i=1}^m v_i \in \overline{F}_i \:\lor\: \bigvee^{k}_{j=1} \mathbf{a}_j^T\mathbf{x} \leq b_j.
\end{equation}
Observe that this formulation allows one to define multiple disconnected
regions in $D\times \mathbb{R}^n$ containing valid records using just a single
edit. For example, one may define a numeric variable to be either smaller than
0 or larger than 1. This type of restriction cannot be formulated
using just linear numerical restrictions.


This formulation is both a generalization of linear inequality (Eq.\
\ref{eq:linedits}) and categorical edits (Eq.\ \ref{eq:defcatedit}). Choosing
$k=0$, we get $P=\mathbb{R}^n$ and only the categorical part remains.
Similarly, choosing $F=\varnothing$, only the disjunction of linear
inequalities remains. A system of linear equations that must simultaneously be
obeyed like in Eq.~\eqref{eq:linedits} can be obtained by defining multiple
edits $E$, each containing a single linear restriction.


The definition in Eq.\ \eqref{eq:implication} can be rewritten as a
`conditional edit' by using the implication replacement rule from
propositional logic  which states that $\lnot p\lor q$ may be replaced by
$p\Rightarrow q$. If we limit Eq.\ \eqref{eq:implication} to a single
inequality, we obtain the normal form of
\cite{waal:2003}.
\begin{equation}
\mathbf{v} \in F\Rightarrow \mathbf{a}^T\mathbf{x}\leq b.
\end{equation}
If we choose $F=\varnothing$ and leave two inequalities we obtain a conditional edit
on numerical data:
\begin{equation}
\mathbf{a}_1^T\mathbf{x} > b_1 \Rightarrow \mathbf{a}_2^T\mathbf{x} \leq b_2.
\end{equation}
Writing mixed-type edits in conditional form seems more user-friendly as
they can directly be translated into an \code{if} statement in a scripting
language. Finally, note that equalities can be introduced by defining pairs of
edits like so:
\begin{equation}
\left\{\begin{array}{l}
\mathbf{v} \in F\Rightarrow \mathbf{a}^T\mathbf{x}\leq b\\
\mathbf{v} \in F\Rightarrow -\mathbf{a}^T\mathbf{x}\leq -b.
\end{array}\right.
\end{equation}

To reformulate Eq. \eqref{eq:implication} as a MIP problem, we
first define binary variables $\ell_j$ that indicate whether $\mathbf{x}$ obeys
$\mathbf{a}^T_k\mathbf{x} > b$:
%
\begin{equation}
\ell_j = \left\{\begin{array}{l}
0 \textrm{ when } \mathbf{a}^T_j\mathbf{x}\leq b_j\\
1 \textrm{ when } \mathbf{a}^T_j\mathbf{x} > b_j %( = \mathbf{x} \in P_j)
\end{array}\right.
\end{equation}
%
Using the or-form of the set condition (Eq.~\eqref{eq:implication}) we can
write the mixed-data edit as
\begin{equation}
\label{eq:mixedit}
\mathbf{e}^T\mathbf{d}(\mathbf{v}) + \sum^{k}_{j=1} (1-\ell_j) \geq 1.
\end{equation}
Recall from Eqs.~\eqref{eq:ecat} and \eqref{eq:definedvec} that $\mathbf{e}$ is
the binary vector representation of a categorical edit and
$\mathbf{d}(\mathbf{v})$ the binary vector representation of a categorical
record. In the above equation, the `$+$' is the arithmetic translation of the
logical `$\lor$' operator in Eq.~\eqref{eq:implication} that connects the
categorical with the linear restrictions. When any of the two terms is
positive, record $r$ satisfies edit $E$.


Rules of this form constitute the user-defined part of the $\mathbf{R}^H$ part
of the restriction matrix. To explicitly identify $\ell_j$ with the linear
restrictions we also add
%
\begin{equation}
\label{eq:linconsequent}
\mathbf{a}^T_j\mathbf{x} \leq b_j + M\ell_j,
\end{equation}
%
to $\mathbf{R}^H$ with $M$ a suitably large positive constant. Indeed, if
$\ell_j=0$, the inequality $\mathbf{a}^T_j\mathbf{x}\leq b_j$ is enforced and
Eq. \ref{eq:mixedit} always is satisfied. When
$\ell_j=1$ the whole restriction can hold regardless of whether the inequality
holds.  Finally, similar to the purely categorical case we need to add
restrictions on the binary representation of $\mathbf{v}$ as in Eq.\
\eqref{eq:hardmaxcat}, so Eq.\ \eqref{eq:hardmaxcat}, Eq.\
 \eqref{eq:mixedit} and Eq.\ \eqref{eq:linconsequent} constitute $\mathbf{R}^H$.

There may be multiple mixed-type edits, each yielding one or more $l$ indicator
variables for each edit. The decision vector for the MIP problem may therefore
be written as
\begin{equation}
\mathbf{z} = (\mathbf{d}(\mathbf{v}),\mathbf{x},\Delta_1,\ldots,\Delta_m,\ldots,\Delta_{m+n},
\ell_1,\ldots,\ell_K),
\end{equation}
where $K$ is the total number of linear edits occurring in all the mixed-type
edits.  Finally, the $\mathbf{R}^0$ matrix connecting the change indicator
variables ($\Delta$) with the actual recorded values consists of the union of
the restrictions for categorical variables (Eq.\ \eqref{eq:deltacat}) and those
for numerical variables (Eq.\ \eqref{eq:deltanum}).

\begin{example}
We consider a record $\mathbf{r}$ with the variables \emph{type of business}
$t$, which takes values in $D_t=\{\textrm{sp},\textrm{other}\}$, where ``sp''
stands for ``sole proprietorship'', \emph{personnel cost} $c\in \mathbb{R}$ and
\emph{number of staff} $p\in\mathbb{R}$. Hence, we have $\mathbf{r}=(t,p,c)\in
D_t\times\mathbb{R}^2$. We impose the following rules on $\mathbf{r}$:
$p\geq0$, $c\geq0$, $c\geq p$ and if the \emph{business type} is a sole
proprietorship, then the number of staff must equal zero. This may be expressed
as $(t \in \{\textrm{sp}\})\Rightarrow (p=0)$ or equivalently
$(t\in\{\textrm{other}\})\lor (p=0)$. For a record
$\mathbf{r}^{0}=(\textrm{sp},p^0,c^0)$, the error localization problem takes
the following form.
\begin{equation*}
\Minimize_{(\mathbf{r},\mathbf{\Delta},\ell)\in D_t\times\mathbb{R}^2\times\{0,1\}^4} \Delta_t + \Delta_p + \Delta_c
\end{equation*}
\begin{equation*}
\textrm{s.t. }\left[\begin{array}{rrrrrrrr}
0 & 1& 0& 0& 0& 0& 0&-1\\
0 & 0& 1& 0& 0& 0& 0& 0\\
0 & 0& 0& 1& 0& 0& 0& 0\\
0 & 0& 1&-1& 0& 0& 0& 0\\
1 & 1& 0& 0& 0& 0& 0& 0\\
0 & 0& 1& 0& 0& 0& 0&-M\\
1 & 0& 0& 0& 1& 0& 0& 0\\
0 & 0& 1& 0& 0&-M& 0& 0\\
0 & 0&-1& 0& 0&-M& 0& 0\\
0 & 0& 0& 1& 0& 0&-M& 0\\
0 & 0& 0&-1& 0& 0&-M& 0\\
\end{array}\right]
\left[\begin{array}{cccccccc}
d_{\textrm{sp}}(t)\\ d_{\textrm{other}}(t)\\
p\\c\\ \Delta_t \\ \Delta_p \\ \Delta_c \\ \ell
\end{array}\right]
\begin{array}{c}
\geq \\ \geq \\ \geq \\ \geq \\ = \\ > \\ = \\ \leq \\ \leq \\ \leq \\ \leq
\end{array}
\left[\begin{array}{c}
0 \\ 0 \\ 0 \\  0 \\  1 \\ 0 \\ 1 \\ p^0 \\ -p^0 \\ c^0 \\ -c^0
\end{array}\right]
\end{equation*}
The first row in the restriction represents the mixed-type rule, translated as shown
in Eq.\ \eqref{eq:mixedit}. Row six connects the indicator variable $\ell$
with the numerical edit in the consequent of
$t\in\{\textrm{sp}\}\Rightarrow(p=0)$.  Rows two, three and four represent the
numerical edits limiting values of $p$ and $c$. Row five forces $t$ to have only one 
value and row seven connects the value of $t$ with that of $\Delta_t$. Finally, 
rows eight to eleven connect the numerical variables with the corresponding
change indicators.
\hfill\qed
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SECTION 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical stability issues}
\label{sec:linedits:num}
An error localization problem, in its original formulation, is an optimization
problem over $n$ binary decision variables that indicate which variables in a record
should be adapted. Depending on the type of rules, its reformulation as a MIP
problem adds at least $n$ variables and $2n$ restrictions. Moreover, the
reformulation as a MIP problem introduces a constant $M$, the value of which
has no mathematical significance but for which a value must be chosen in
practice. Because of limitations in machine accuracy, which is typically on the order of
$10^{-16}$, the range of problems that can be solved is limited as well. In
particular, MIP problems that involve both very large and very small numbers in
the objective function and/or the restriction matrix  may yield erroneous
solutions or become numerically unfeasible. Indeed, the manual of
\code{lp\_solve} \citep{berkelaar:2010} points out that `\emph{[...] to improve
stability, one must try to work with numbers that are somewhat in the same
range. Ideally in the neighborhood of 1}'. The following subsections point out
a number of sources of numerical instabilities and provides ways to handle
them.


\subsection{A short overview of MIP-solving}
Consider a set of linear restrictions on numerical data of the form
$\mathbf{Ax}\leq\mathbf{b}$, where we assume $\mathbf{b}\geq\mathbf{0}$ and the
restrictions consist solely of inequalities ($\leq$). In practice, these
restrictions will not limit the type of linear rules covered by this
discussion, since it can be shown that all linear rules can be brought to this
form, possibly by introducing dummy variables [see \emph{e.g.}
\citet{schrijver:1998,bradley:1977}]. Furthermore, suppose we have a record
$\mathbf{x}^0\geq\mathbf{0}$ which doesn't obey the restrictions. The MIP
formulation of the error localization problem can be written as follows.
\begin{equation}
\begin{array}{l}
  \textrm{Minimize }f = \mathbf{w}^T\mathbf{\Delta}\\
  \textrm{s.t.}
  \left[\begin{array}{rr}
    \mathbf{A} & \mathbf{0}\\
    \mathbb{1} &-\mathbf{M}\\
   -\mathbb{1} &-\mathbf{M}\\
    \mathbf{0} &\mathbb{1}
  \end{array}\right]
  \left[\begin{array}{c}
    \mathbf{x}\\ \mathbf{\Delta}
  \end{array}\right]
  \leq
  \left[\begin{array}{c}
    \mathbf{b}\\ \mathbf{x}^0 \\ -\mathbf{x}^0\\ \mathbf{1}
  \end{array}\right],
\end{array}
\label{eq:numxx}
\end{equation}
and $\mathbf{x},\mathbf{\Delta}\geq\mathbf{0}$. Also, $\mathbb{1}$ denotes the
unit matrix, $\mathbf{1}$ a vector with all coefficients equal to $1$ and
$\mathbf{M}=\mathbb{1}M$. The last row is added to force $\mathbf{\Delta}\leq
\mathbf{1}$. This is necessary because we will initially treat the binary
variables $\Delta_j$ as if they are real numbers in the range $[0,1]$.

The \code{lp\_solve} library uses an approach based on the revised Phase I -
Phase II simplex algorithm to solve MIP problems. In this approach every
inequality of Eq.\ \eqref{eq:numxx} is transformed to an equality by adding
dummy variables: each row $\mathbf{a}^T\mathbf{x}\leq b$ is replaced by
$\mathbf{a}^T\mathbf{x}+s = b$, with $s\ge0$. Depending on the sign of the
inequality, the extra variable $s$ is called a \emph{slack} or \emph{surplus}
variable. In Eq.~\eqref{eq:numxx} there are four sets of restrictions (rows).
We therefore need to add four sets of surplus and slack variables (columns) in
order to rewrite the whole system in terms of equalities.

Note that after this transformation, the whole problem including the cost
function is written in terms of equalities. It is customary to organize this
set of equality objective function in a single \emph{tableau} notation as
follows.
\begin{equation}
\left[\begin{array}{r|rrrrrr|l}
1   & \mathbf{0}    &-\mathbf{w}^T  & \mathbf{0}   & \mathbf{0}   & \mathbf{0} &\mathbf{0}      & 0\\
\hline
0   & \mathbf{A}    &  \mathbf{0}    & \mathbb{1} & \mathbf{0} & \mathbf{0}   &\mathbf{0} &\mathbf{b}\\
0   & \mathbb{1}    & -\mathbf{M}    & \mathbf{0} & \mathbb{1} & \mathbf{0}   &\mathbf{0} &\mathbf{x}^0\\
0   & \mathbb{1}    &  \mathbf{M}    & \mathbf{0} & \mathbf{0} & -\mathbb{1}  &\mathbf{0} &\mathbf{x}^0\\ 
0   & \mathbf{0}    &  \mathbb{1}    & \mathbf{0} & \mathbf{0} & \mathbf{0}   &\mathbb{1} & \mathbf{1}\\ 
\end{array}\right].
\label{eq:tab1}
\end{equation}
%
Here, the first row and column represents the cost function. Columns two and
three correspond to the original set of variables in Eq.~\eqref{eq:numxx} while
columns four to seven correspond to sets of slack and surplus variables.  The
final column contains the constant vector. 

A tableau representation shows all the numbers that are relevant in an
LP-problem at a glance. By examining how LP-solvers typically manipulate these
numbers we gain some insight into how and where numerical stability issues may
arise. 

Since the tableau represents a set of linear equalities, it may be manipulated
as such.  In fact, the simplex method is based on performing  a number of
cleverly chosen Gauss-Jordan elimination steps on the tableau. For a complete
discussion the reader is referred to one of the many textbooks discussing it
(e.g. \citet{bradley:1977}), but in short the Phase I - Phase II simplex
algorithm consists of the following steps.
\begin{description}
\item[Phase I:] Repeatedly apply Gauss-Jordan elimination steps (called
\emph{pivots}) to derive a decision vector that obeys all restrictions. 
A vector obeying all restrictions is called a \emph{basic solution}.
\item[Phase II:] Repeatedly apply pivots to move from the initial non-optimal
solution to the solution that minimizes the objective function $f$. 
\end{description}
%
In Phase I, a decision vector $(\mathbf{x},\mathbf{\Delta},\mathbf{s})$ (with
$\mathbf{s}$ the vector of slack and surplus variables) is derived that obeys
all restrictions. The precise algorithm need not be described here.  It
involves adding again extra variables where necessary and then manipulating the
system of equalities represented by the tableau so that those extra variables
are driven to zero. The binary variables $\mathbf{\Delta}$ are first treated as
if they are real variables. In the Appendix it is shown in detail how an
initial solution for Eq.~\eqref{eq:tab1} can be found, here we just state the
result of a Phase-I operation:
%
\begin{equation}
\left[\begin{array}{r|cccccc|l}
1 & \mathbf{w}^T\mathbf{M}^{-1} & \mathbf{0} & \mathbf{0}   & \mathbf{0}   & -\mathbf{w}^T\mathbf{M}^{-1} &\mathbf{0} & \mathbf{w}^T\mathbf{M}^{-1}\mathbf{x}^0\\
\hline
0 & \mathbf{A}     & \mathbf{0} & \mathbb{1}   & \mathbf{0}   & \mathbf{0}       &\mathbf{0} & \mathbf{b}\\
0 &2\mathbb{1}     & \mathbf{0} & \mathbf{0}   & \mathbb{1}   & -\mathbb{1}      &\mathbf{0} & 2\mathbf{x}^0\\
0 &\mathbf{M}^{-1} &\mathbb{1}  & \mathbf{0}   & \mathbf{0}   & -\mathbf{M}^{-1} &\mathbf{0} & \mathbf{M}^{-1}\mathbf{x}^0\\
0 &-\mathbf{M}^{-1}&\mathbf{0}  & \mathbf{0}   & \mathbf{0}   & \mathbf{M}^{-1}  &\mathbb{1} & \mathbf{1}-\mathbf{M}^{-1}\mathbf{x}^0 
\end{array}\right].
\label{eq:phaseI}
\end{equation}
%
This tableau immediately suggests a valid solution: it is easily confirmed by
matrix multiplication that the vector $(\mathbf{x},\mathbf{\Delta},\mathbf{s})
=
(\mathbf{0},\mathbf{x}^0\mathbf{M}^{-1},[\mathbf{b},2\mathbf{x^0},\mathbf{0},\mathbf{1}-\mathbf{M}^{-1}\mathbf{x}^0])$
obeys all restrictions. The above form of a tableau, where the restriction
matrix contains a (column permutation of) the unit matrix, the right-hand-side
has only non-negative coefficients, and the cost vector equals zero for the
columns above the unit matrix is called the \emph{canonical form}.

Now, a \emph{pivot} operation consists of the following steps:
\begin{enumerate}
\item Select a positive element $R_{ij}$ from the restriction matrix. This is called the \emph{pivot element}.
\item Multiply the $i$th row by $R_{ij}^{-1}$.
\item Subtract the $i$th row, possibly after rescaling, from all other rows of the tableau such that
their $j$th column equals zero.
\end{enumerate}
%
The result of a pivot operation is again a tableau in canonical form but with
possibly a different value for the cost function. The simplex algorithm proceeds
by selecting pivots that decrease the cost function until the minimum is reached
or the problem is shown to be unfeasible.

Up until this point, we have treated the binary variables $\mathbf{\Delta}$ as
if they were real variables, so the tableaux discussed above do not represent
solutions to our original problem which demands that all $\Delta_j$ are either
$0$ or $1$. In the \code{lp\_solve} library this is solved as follows. 
\begin{enumerate}
\item For each optimized value $\Delta_j^*$ test whether it is $0$ or $1$. If
all $\Delta_j^*$ are integer, we have a valid solution of objective value
$\mathbf{w}^T\mathbf{\Delta}$ and we are done.
\item For the first variable $\Delta_j^*$ that is not integer, create two
sub-models: one where the minimum value of $\Delta_j$ equals $1$ and one where
the maximum value of $\Delta_j$ equals $0$.
\item Optimize the two sub-models. If solutions exist, the result will contain
an integer $\Delta_j$.
\item For the sub-models that have a solution and whose current objective value
does not exceed that of an earlier found solution, return to step 1.
\end{enumerate}

The above \emph{branch-and-bound} approach completes this overview. The
discussion of pivot and branch-and-bound operations has so far been purely
mathematical: no choices have been made regarding issues such as how to decide
when the floating-point representation of a value is regarded zero or how to
handle badly scaled problems. Do note however, that in the course of going from
Phase-I to Phase-II, the LP-solver is handling numbers that may range from
$M^{-1}$ to $M$ which typically differ many orders of magnitude


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scaling numerical records}
In the MIP formulation of error localization over numerical records under
linear restrictions, Eq.~\eqref{eq:deltanum} restricts the search space around
the original value $x^0$ to $|x-x^0|\leq M$. This restriction may prohibit a
MIP solver from finding the actual minimal set of values to adapt or even
render the MIP-problem unsolvable. As an example, consider the following error
localization problem on a two-variable record.
\begin{displaymath}
\left\{\begin{array}{l}
x_1 \geq x_2\\
\mathbf{x}^0 = (10^6,10^9).
\end{array}\right.
\end{displaymath}
Obviously, the record can be made to obey the restriction by multiplying
$x^0_1$ by $10^3$ or by dividing $x^0_2$ by the same amount. However, in
\code{editrules} the default value for $M=10^7<10^9-10^6$ which renders the
corresponding MIP problem unsolvable. Practical examples where such errors
occur is when a value is recorded in the wrong unit of measure (\emph{e.g.} in
\euro{} instead of k\euro{}.).

It is therefore advisable to remove such unit-of-measure errors prior to error
localization\footnote{Methods for detecting such errors exist, see for example
\cite{waal:2011}, Chapter 2. In fact, the principle of minimal change is not
applicable here since a better value can be deduced from the cause of the
error.} and to express numerical records on a scale such
that all $|x^0|\ll M$.  Note that under linear restrictions (Eq.~\ref{eq:linedits})
one may always apply a scaling factor $k>0$ to a numerical record $\mathbf{x}$
by replacing $\mathbf{A}\mathbf{x}\leq \mathbf{b}$ with
$\mathbf{A}(k\mathbf{x})\leq k\mathbf{b}$. In the above example, one
may replace $\mathbf{x}^0$ by $10^{-6}\mathbf{x}^0$ for the purpose of error
localization. If $\mathbf{b}=\mathbf{0}$ and the coefficients of $\mathbf{x}$
do not vary over many orders of magnitude, such a scaling will suffice to
numerically stabilize the MIP problem. 

%
\subsection{Setting numerical threshold values}
On most modern computer systems real numbers are represented in
\citet{ieee:2008} double precision format. In essence, real numbers are 
represented as rounded-off fractions so arithmetic operations on such numbers
always result in loss of precision and round-off errors. For example, even though
mathematically we have $0.7-0.5=0.2$, in the floating point
representation (denoted $\fl(\cdot)$) we have
$\fl(0.7)-\fl(0.5)\not=\fl(0.2)$. In fact, the difference is about
$0.56\cdot10^{-16}$ in this case. 

This means that in practice one cannot rely on equality tests to determine
whether two floating point numbers are equal.  Rather, one considers two
numbers $v$ and $w$ equal when $|\fl(v)-\fl(w)|$ is smaller than a predefined
tolerance. For this reason \code{lp\_solve} comes with a number of predefined
tolerances. These tolerances have default values but these may be altered by
the user.

The tolerances implemented by \code{lp\_solve} are summarized in
Table~\ref{tab:tols}. The value of \code{epspivot} is used to determine whether
an element of the restriction matrix is positive so it may be used as a
pivoting element. Its default value is $2\cdot10^{-7}$, but note that after
Phase I, our restriction matrix contains elements on the order of
$M^{-1}=10^{-7}$. For this reason, the value of \code{epspivot} is lowered in
\code{editrules} by default, but users may override these settings. For
the same reason, the value of \code{epsint}, which determines when a value for
one of the $\mathbf{\Delta}_j$ can be considered integer is lowered in
\code{editrules} as well.  The other tolerance settings of \code{lp\_solve}: 
\code{epsb} (to test if the right-hand-side of the restrictions
differ from $0$), \code{epsd} (to test if two values of the objective function
differ), \code{epsel} (all other values) and \code{mip\_gap} (to test whether a
bound condition has been hit in the branch-and-bound algorithm) have not been altered.

The limited precision inherent to floating point calculations imply that
computations get more inaccurate as the operands differ more in magnitude. For
example, on any system that uses double precision arithmetic the difference
$\fl(1)-\fl(10^{-17})$ is indistinguishable from $\fl(1)$. This then, leads to
two contradictory demands on our translation of an error localization problem
to a MIP problem. On one hand, one would like to set $M$ as large as possible
so the ranges $x^0_j\pm M$ contain a valid value of $x_j$. On the other hand,
large values for $M$ imply that MIP problems such as Eq.~\eqref{eq:numxx}
may become numerically unstable. 

In practice, the tableau used by \code{lp\_solve} will not be exactly the same
as represented in Eq.~\eqref{eq:phaseI}. Over the years, many optimizations and
heuristics have been developed to make solving linear programming problems fast
and reliable, and several of those optimizations have been implemented in
\code{lp\_solve}. However, the tableau of Eq.~\eqref{eq:phaseI} does
fundamentally show how numerical instabilities may occur: the tableau
simultaneously contains numbers on the order of $M^{-1}$ and on the order of
$\mathbf{x}^0$. It is not at all unlikely that the two differ in many orders of
magnitude.

The above discussion suggests the following rules of thumb to avoid numerical
instabilities in error localization problems.
\begin{enumerate}
\item Make sure that elements of $\mathbf{x}^0$ are expressed in units such
that $\mathbf{A}$, $\mathbf{b}$ and $\mathbf{x}$ are on the order of 1 wherever
possible.
\item Choose a value of $M$ appropriate for $\mathbf{x}^0$.
\item If the above does not help in stabilizing he problem, try lowering the
numerical constants of Table \ref{tab:tols}.
\end{enumerate}

In our experience, the settings denoted in Table \ref{tab:tols} have performed
well in a range of problems where elements of $\mathbf{A}$ and $\mathbf{b}$ are
on the order of $1$ and values of $\mathbf{x}^0$ are in the range $[1,10^8]$.
However, these settings have been made configurable so users may choose their
own settings as needed.





\begin{table}
\begin{threeparttable}
\caption{Numerical parameters for MIP based error localization.}
\label{tab:tols}
\begin{tabular}{lrrl}
\hline
&\multicolumn{2}{c}{Default value} \\
\cline{2-3}
Parameter       & \code{lp\_solve}  & \code{editrules} & meaning \\
\hline
\code{M}       & \multicolumn{1}{c}{$-$}& $10^7$     & set bounds so $\mathbf{x}\in\mathbf{x}^0\pm M$\\
\code{eps}     & \multicolumn{1}{c}{$-$}& $10^{-3}$  & translate $x<0$ to $x\leq\varepsilon$\\
\code{epspivot}& $2\cdot10^{-7}$      & $10^{-15}$ & test if pivot element $R_{ij}>0$ \\
\code{epsint}  & $10^{-7}$            & $10^{-15}$ & test if $\Delta_j\in\mathbb{N}$ \\
\code{epsb}    & $10^{-10}$           & $10^{-10}$ & test if $b_i>0$ \\
\code{epsd}    & $10^{-9}$            & $10^{-9}$  & test if obj. values $|f-f'|>0$ during simplex \\
\code{epsel}   & $10^{-12}$           & $10^{-12}$ & test if other numbers $\neq0$\\
\code{mip\_gap}& $10^{-11}$           & $10^{-11}$ & test if obj. values $|f-f'|>0$ during B\&B\\
\hline
\end{tabular}
\end{threeparttable}
\end{table}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION IV: USAGE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Usage}
\label{sec:usage}
In the \code{editrules} package, edits can be defined with the \code{editset}
function. For example, the command
\begin{rcode}
<<tidy=FALSE>>=
E <- editset(expression(
   x + y == z
  , if ( x > y ) y > 0
))
@
\end{rcode}
defines two edits on the variables $x$, $y$ and $z$ and stores them in an object
called \code{E}. Here, \code{E} is an object of type \code{editset} and it can
be used to store and manipulate linear (in)equality edits, edits on categorical
data as well as edits on mixed-type data. Besides \code{editset} there are
specialized functions called \code{editmatrix} and \code{editarray} which can
be used to define rules on purely numerical or purely categorical data
respectively. Edits are defined in basic \code{R} syntax; one may use
multiplication, addition, \code{if}-\code{else} statements, logical operators
and the \code{\%in\%} operator for set inclusion on categorical variables. 

Besides defining rules on the command line, as in the example above,
one may store the rules in a text file and read the rules into \R{} using
the \code{editfile} function.
\begin{rcode}
<<eval=FALSE>>=
E <- editfile('myedits.txt')
@
\end{rcode}
Here, \code{myedits.txt} is the name of a textfile containing the edits. The
resulting object is by default of class \code{editset}. If the extra argument
\code{type="num"} or \code{type="cat"} is passed, only numerical or categorical
edits are read from the file. For a further discussion of the functions
mentioned here we refer the reader to the technical manual that is included
with the software. Also see the papers of \cite{loo:2011b, loo:2012a} for a
precise description of the edit-definition syntax.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Error localization}
The main interface to error localization functionality is the
\code{localizeErrors} function.  The function accepts editrules in the form of 
an \code{editset}, \code{editmatrix} or \code{editarray} object, and a data set
in the form of a \code{data.frame}. By default the function localizes errors
using the branch-and-bound algorithm. One may switch to the MIP-based approach
by setting the parameter \code{method="mip"} as in the following example.
\begin{rcode}
<<num example>>=
E <- editmatrix("x <= y")
dat <- data.frame(x=c(10,3), y=c(1,5))
el <- localizeErrors(E, dat, method="mip")
@
\end{rcode}
The object returned by \code{localizeErrors} contains the error locations as
well as some details on how the algorithm ran. The error locations are stored
in a boolean array called \code{adapt} which can be accessed as follows.
\begin{rcode}
<<>>=
el$adapt
@
\end{rcode}
Here, the array has dimension $2\times2$ since the input data set consisted of
two records with two variables. Here, the result indicates for the first record
that by altering the value for $x$, the record can be corrected to obey the edit rule stored in
\code{E}. For the second record, no alterations are necessary.

Details on the error localization procedure are stored in a \code{data.frame}
called \code{status}, which can be accessed as follows.
\begin{rcode}
<<>>=
el$status
@
\end{rcode}
The status \code{data.frame} contains one record of information for each record
in the input data. The column \code{weight} contains the value of the objective
function as defined by Eq.~\eqref{eq:objfun}. The time it took to perform the
calculation is subdivided into user, system and elapsed time, where the latter
corresponds to the actual time that has passed on the clock.  The boolean
variable (\code{maxDurationExceeded})  indicates whether the time limit for
finding a solution was exceeded.  There are two status columns that have no
relevance when the error localization method is MIP.  First, the
indicator \code{memfail} can only be set to \code{TRUE} when the
branch-and-bound algorithm is used. It indicates that perhaps the optimal
solution could not be found because of memory limitations.  Second, and more
importantly, the \code{degeneracy} parameter is not set when
\code{method="mip"}. This parameter indicates how many equivalent solutions
there are to each error localization problem.  Contrary to the branch-and-bound
method for error localization, the MIP-based approach does not return
this information.

The output of \code{localizeErrors} can be controlled with two parameters.
Most importantly, positive weights for each variable and optionally
for each record can be set; variables with lower weights attached to them are
more likely to be part of a chosen solution which is otherwise degenerate.
Furthermore, a maximum search time per record can be specified, the
default setting being 10 minutes. Finally, the optional parameter \code{lpcontrol}
may contain a list of parameters to be passed to \code{lpSolveAPI}. The default settings
can be listed as follows
\begin{rcode}
<<>>=
options("er.lpcontrol")
@
\end{rcode}
These options are precisely the values that differ from \code{lpSolve}'s
default settings listed in Table \ref{tab:tols}. Changing or adding options can
be done either by passing the \code{lpcontrol} parameter to
\code{localizeErrors}, in which case it is only used for the current error
localization problem. To adapt a parameter for the remainder of the running
R-session or until the option is reset one can use \R{}'s \code{options}
function. For example, to alter the \code{epsb} parameter through
\code{localizeErrors}, one may use either of the two calls below.
\begin{rcode}
<<eval=FALSE,tidy=FALSE>>=
localizeErrors( E, dat, lpcontrol = c(epsb=1e-12, options("er.lpcontrol")) )
options( er.lpcontrol = c(options("er.lpcontrol"), epsb=1e=12) )
@
\end{rcode}
The important thing to note is that it is up to the user to merge new options
with existing ones since \code{lpcontrol} completely overwrites the default settings.
A precise description of possible options is also given in the reference manual of
\code{lpSolveApi}.


\subsection{Fine-grained control options}
For users who wish to exert more fine-grained control on the MIP-solver or who
wish to interface \code{editrules} with another MIP-solving engine, two lower-level
functionalities have been exposed to the user.

The first is \code{errorLocalizer\_mip}. This function takes
a set of edit rules in the form of an \code{editset}, \code{editmatrix}
or \code{editarray} and a single data record in the form of a named list.
\begin{rcode}
<<num mip>>=
L <- errorLocalizer_mip(E, list(x=10,y=1))
@
\end{rcode}
Here, \code{errorLocalizer\_mip} translates the error localization problem for
a single record to a MIP problem, feeds it to \code{lpSolveAPI} and returns all
the results in a list. The list contains two extra pieces of information not
available in the output of \code{localizeErrors}. The first is a parameter
called \code{x\_feasible}, containing a record that actually obeys all the edit
rules.
\begin{rcode}
<<>>=
L$x_feasible
@
\end{rcode}
The second parameter is called \code{lp}. This is an object of class
\code{lpExtPtr} which points to an object of \code{lpSolveApi}, stored outside
of \R{}'s memory. It contains precise information on the definition of the MIP
problem as interpreted by \code{lpSolveAPI}. It can be manipulated or
exported to a text file using \code{write.lp} of the \code{lpSolveAPI} package.

The second functionality entails the functions \code{\code{as.mip}} and
\code{as.lp.mip}.  The function \code{as.mip} allows users to translate the
combination of a set of editrules and a data record to a MIP problem.
\begin{rcode}
<<>>=
mip <- as.mip(E,list(x=10,y=1))
print(mip)
@
\end{rcode}
The object returned by \code{as.mip} can be used to inspect how
\code{editrules} translates an error localization problem to a MIP problem. The
interested reader may want to compare the above representation with
Eqs.~\eqref{eq:objfun}, \eqref{eq:zlin} and \eqref{eq:deltanum}.

This representation of the MIP problem can be translated to a form that is
suited for solving with \code{lpSolveAPI}.
\begin{rcode}
<<>>=
lp <- as.lp.mip(mip)
@
\end{rcode} 
The \code{lp} object can directly be used as input for \code{lpSolveAPI} or
written to disk with \code{write.lp} as follows.
\begin{rcode}
<<eval=FALSE>>=
write.lp(lp,file="myLPfile.lp")
@
\end{rcode}
This command produces a text file that is written in a syntax understood by the
\code{lp\_solve} commandline program.  \code{lp\_solve} also has facilities to
translate this syntax to other formats allowing export to other LP-solvers,
including some commercial ones.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% benchmarks
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Benchmarks}
\label{sec:benchmarks}
The \emph{branch and bound} and \emph{MIP}-based algorithms for error
localization differ in consumption of both memory and computational time. The
performance of error localization methods depends on the number of variables,
the number of erroneous fields, the number of violated restrictions, and the
total number of restrictions. 

Below we describe benchmarks based edit sets which can be systematically
extended to encompass more variables. The edit sets were designed so that
synthetic records can be created where the size of the solution to the error
localization problem (the minimal number of fields to alter so the record can
be made consistent) can be easily controlled as well. This then gives an
impression of computational time consumed by the MIP and branch-and-bound
approaches as a function of the number of variables/edits and the number of
erroneous fields.


Benchmarking the two approaches is further complicated by the fact that
performance of especially the branch-and-bound algorithm is strongly affected
by the order in which variables are treated by the algorithm. Indeed,
preliminary tests showed that if the erroneous variables are treated first
(which may be achieved by setting appropriate reliability weights), performance
of the branch-and-bound method is largely on par with that of the MIP approach.
If variables are ordered such that erroneous variables are treated halfway, or
at the end of the variable set, the branch-and-bound method performs much
slower than the MIP method. Below, we report on benchmarks where errors were
injected into variables which were positioned around the center of the record.
This mimics the case when there is no knowledge on the reliability of the variables. 
 
\subsection{Linear restrictions}
The edits used in this benchmark form a balance system. Balance systems occur
for example in energy or business statistics where main variables (total energy
consumption, total turnover) are the sum of several other variables. Moreover,
these main variables are also connected by linear restrictions (total energy
production equals total energy consumption). In balance systems variables are
therefore connected through a tree-like structure where the value corresponding to
a node equals the sum of its child node values.

In our benchmark we generate a balance system on $2n+1$ variables. Here, the
restrictions connect the variables through a binary tree where the value of a
node is the sum of its two child node values. The value of the top node is
restricted to be non-negative and always at least as large as any other value.
\begin{eqnarray*}
x_1 & =    & x_2 + x_3 \\
&\vdots & \\
x_n &=     & x_{2n} + x_{2n+1} \\
x_1 & \geq & 0 \\
x_1 & \geq & x_i\textrm{, }\quad i=2,3,\ldots2n+1 
\end{eqnarray*}
This edit set is fully connected and completely fixed by choosing an $n>0$.
The $x_1 \geq 0$ and $x_1 \geq x_i$ for all $2\leq i\leq 2n+1$ implies that all
$x_i\geq0$. To see this, observe that since $x_1\geq0$ and and $x_1\geq x_3$,
we cannot have $x_2<0$ without having $x_3>x_1$, which violates the
restrictions saying that $x_1$ must be larger than or equal to $x_i$, $i>1$.
This reasoning applies recursively to all $x_i$ because of the chained sum-rules.

One solution to this system is the zero vector $(x_1, \ldots, x_{2n+1}) =
(0,\ldots,0)$.  An error can be introduced in the data by setting some $x_i$ to
$-1$.  Because of the implied nonnegativity contstraint, the number of
variables in the optimal error localization solution is exactly equal to the
number of $x_i$ set to $-1$. The benchmark was performed for balance systems
with $1$ to $101$ variables ($n=0$ to $50$). For each system records with $1$
to $10$ errors were generated.  

<<numbench, echo=FALSE,warning=FALSE, fig.height=3.5, fig.cap="Linear edits, each corresponds to a different number of errors">>=
library(ggplot2)
theme_cbs <- theme_bw() + 
             theme( panel.grid.major.x = element_blank()
                  , axis.title.y = element_text(angle=0)
                  )
data("benchmip_balance")
sdat <- subset(benchmip_balance, errorloc=="middle")
#sdat <- within(sdat, elapsed[weight == 0] <- 200)
ann <- subset(sdat, elapsed <= 150 & method=="bb")
ann <- aggregate(elapsed ~ nerrors, data=ann, FUN=max)
ann <- merge(ann, sdat)
qplot( data=sdat, x=nvar, y=elapsed, color=method, group=paste0(method, nerrors), geom=c("point","line"), 
       ) +
       labs(title="", x="number of variables", y="", color="method") + 
       theme_cbs + theme() + 
       scale_y_continuous(labels=function(x){paste0(x,"s")}) + coord_cartesian(ylim=c(0,150)) + 
       annotate("text", x=ann$nvar+1,y=ann$elapsed+1, label=ann$nerrors, size=2.5)
@
%
Figure \ref{fig:numbench} shows the time of error localization for increasing
number of variables and increasing number of errors, both for the
branch-and-bound method and the MIP method.  The branch-and-bound method hits
the `exponential wall' around 30 variables and 5 errors or 20 variables with
10 errors, showing reasonable performance only when the number of errors is 2
or less. The branch-and-bound algorithm was broken off when no solution was
found in less than 10 minutes which occurred at problems with more than 50
variables and 4 errors.  In contrast, the MIP-based approach performs well
(under a few seconds) for all problems tested here. 

We caution the reader to conclude that the MIP approach is better performing in
all circumstances: when a good set of reliability weights can be determined,
variables that are most likely to be erroneous can be treated early on by the
algorithm, yielding a considerable performance boost.  In such a case, the
optimal solution is found first by the algorithm and branches leading to
suboptimal solutions can be quickly rejected. However, in the generic case
where no reliability weights can be derived with great confidence, the
MIP-approach is obviously better performing.


\subsection{Categorical restrictions}
For this benchmark we use a chain of interconnected edits:
%
\begin{eqnarray*}
v_{1}&\texttt{==}&\texttt{TRUE}\\
\texttt{if (}v_{1}\texttt{==TRUE) }v_{2} &\texttt{==}&\texttt{TRUE}\\
&\vdots&\\
\texttt{if (}v_{n-1}\texttt{==TRUE) }v_{n} &\texttt{==}&\texttt{TRUE}.
\end{eqnarray*}
%
Examples of such edit chains occur in practice, for example: \code{if}
\emph{married}$==$\code{TRUE} \code{then} \emph{adult}$==$\code{TRUE} and
\code{if} \emph{adult}$==$\code{TRUE} \code{then} \emph{allowed to drive a
car}$==$\code{TRUE}.  Here, we demand that $v_1==\code{TRUE}$ which forces the
only solution to this editset to be
$(v_1,\ldots,v_n)=(\code{TRUE},\ldots,\code{TRUE})$. An error in the data can
therefore be introduced by setting on of the $v_i$'s to \code{FALSE}.

%
<<catbench, echo=FALSE, fig.height=3.5, warning=FALSE, fig.cap="Categorical edits: each line corresponds to a different number of errors.">>=

data(benchmip_categorical)
dat <- benchmip_categorical
sdat <- subset(dat, errorloc=="middle")
ann <- subset(sdat, elapsed <= 150 & method=="bb")
ann <- aggregate(elapsed ~ nerrors, data=ann, FUN=max)
ann <- merge(ann, sdat)
qplot( data=sdat, x=nvar, y=elapsed, color=method, group=paste0(method, nerrors), geom=c("point","line"), 
       ) +
       labs(title="", x="number of variables", y="", color="") + 
       theme_cbs + theme() + 
       scale_y_continuous(labels=function(x){paste0(x,"s")})+ coord_cartesian(ylim=c(0,150)) + 
       annotate("text", x=ann$nvar+0.5,y=ann$elapsed+1, label=ann$nerrors, size=2.5)
@

Benchmarks have been performed for systems with $n=1$ to $n=50$ variables. For
each system records with $1$ to $10$ errors were created. Again, the errors
were introduced in variables with intermediate positions in the record.

Figure~\ref{fig:catbench}  shows time spent on error localization for the
branch-and-bound and MIP-approaches as a function of number of variables and
errors.  Again, the MIP-based algorithm outperforms the branch-and-bound
approach in nearly every case.  Problems with more than 30 variables and 4
errors yield no solution with the branch-and-bound approach within 10 minutes
causing calculations to be terminated. The same caution holds here as for the
benchmarks on linear edits described above: carefully chosen reliability weights
can improve performance of the branch-and-bound method.  Without such knowledge
however, MIP is the better generic choice when performance is important.



\subsection{Mixed-type restrictions}
We use a chain of interconnected restrictions defined as follows:
\begin{eqnarray*}
&& x_1 \geq 0\\
\texttt{if (}x_{1} \geq 0 \texttt{)} && x_{2} \geq 0 \\
&\vdots\\
\texttt{if (}x_{N-1} \geq 0 \texttt{)} && x_{N} \geq 0. \\
\end{eqnarray*}
Here, all $x_i$ are numeric but since the conditional restrictions are
internally modeled using dummy boolean variables, it serves as a model for
mixed-type variable restrictions. Examples of such chains do occur in practice,
for example the following restriction is often found in the context of business
statistics: \code{if} \emph{number of employees} $>0$ \code{then} \emph{amount
of salary payed} $>0$.

One viable solution for this system of edits is the zero vector
$(x_i,\ldots,x_n)=(0,\ldots,0)$.  Note that the restriction set implies that 
$x_i \geq 0$ for $i=1 \ldots N$. An error can be injected by setting one or
more $x_i=-1$. The set 
of edits is engineered such that if $k$ variables are set
to $-1$, then $k$ variables must be adapted to re-establish a viable solution.
However, setting $k$ variables to $-1$ does not mean that $k$ explicitly
defined edits are violated.  For example, it is easily confirmed that for
$n=4$ and $\mathbf{x}=(0,0,-1,-1)$ only the third (out of five) edit is
violated. However both $x_3$ and $x_4$ need to be adapted in order to repair
the record.

The benchmark was performed with 1 to 50 variables and 1 to 10 errors
introduced. Again, errors were injected at variables with intermediate
positions in the records.

%
<<mixedbench, echo=FALSE, fig.height=3.5, warning=FALSE, fig.cap="Mixed type edits: each line represents a different number of errors.">>=
data(benchmip_mixed2)
dat <- benchmip_mixed2
sdat <- subset(dat, errorloc=="middle")
sdat <- within(sdat, {elapsed[weight==0] <- 200})
ann <- subset(sdat, elapsed <= 150 & method=="bb")
#ann <- subset(sdat, method=="bb")

ann <- aggregate(elapsed ~ nerrors, data=ann, FUN=max)
ann <- merge(ann, sdat)
qplot( data=sdat, x=nvar, y=elapsed, color=method, group=paste0(method, nerrors), geom=c("point", "line"), 
       ) +
       labs(title="", x="# variables", y="", color="") + 
       theme_cbs + theme() + 
       scale_y_continuous(labels=function(x){paste0(x,"s")})+ coord_cartesian(ylim=c(0,150)) +
       annotate("text", x=ann$nvar+0.5,y=ann$elapsed+1, label=ann$nerrors, size=2.5)
@


The results of the benchmarks are shown in Figure \ref{fig:mixedbench}. Results
are comparable with the benchmarks for linear and categorical data type edits,
except for the `bump' computational time for the branch-and-bound method around
10-25 variables. Interpretation is difficult without precisely following the
state of variables during the run of the algorithm, but a plausible explanation
is fundamental difference between the branch-and-bound approaches for
single-type and mixed-type edits. For linear and categorical edits the
branch-and-bound algorithm traverses a tree that branches over the variables
whereas for mixed-type edits there is also a bifurcation over each condition in
the mixed-type edits. This bifurcation generates a lot of extra edits compared
to the situation with single-type edits. On the other hand, branching over
variables includes simplifying steps that reduce the number of edits. Tests
have shown that interaction between these two effects strongly depends on the
order of the variables and which variables contain the actual error. The
observed `bump' should therefore be regarded an artefact of this specific
benchmark. However, this does not alter the conclusion that the MIP approach
performs consistently better.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCLUSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}
We described a formulation of the error localization problem for linear,
categorical and mixed-type restriction in terms of mixed-integer programming
problems. It was shown that in this formulation the mixed-type restrictions can
be understood as a generalisation of both linear and categorical restrictions.

Although mixed integer programming problems can be solved by readily available
software packages, there may be a trade-off in numerical stability with respect to
a branch-and-bound approach. This holds especially when typical default
settings of such software is left unchanged and data records and/or linear
coefficients of the restriction sets cover several orders of magnitude. In this
paper we locate the origin of these instabilities and provide some pointers to
avoiding such problems.

The \code{lp\_solve} package has now been introduced as a MIP-solver backend to
our \code{editrules} R package for error localization and rule management. Our
benchmarks indicate that in generic cases where no prior knowledge is available
about which values in a record may be erroneous (as may be expressed by lower
reliability weights), the MIP method is much faster than the previously
implemented branch-and-bound based algorithms. On the other hand, the
branch-and-bound based approach returns extra information, most notably the
number of equivalent solutions.  The latter can be used as a indicator for
quality of automatic data editing.


\bibliographystyle{chicago}
\bibliography{editrules}

\appendix
\section{Derivation of Equation \ref{eq:phaseI}}
In the Phase I Phase II simplex method, phase I is aimed to derive a valid
solution which is then iteratively updated to an optimal solution in Phase II.
Here, we derive a Phase I solution, specific for error localization problems.


Recall the tableau of Eq.~\eqref{eq:tab1}; for clarity, the top row indicates to what variables
the columns of the tableau pertain.
%
\begin{displaymath}
\left[\begin{array}{r|rrrrrr|l}
f   & \mathbf{x}^T  &\mathbf{\Delta} & \mathbf{s}_x &\mathbf{s}_+  & \mathbf{s}_- &\mathbf{s}_\Delta\\
1   & \mathbf{0}    &-\mathbf{w}^T   & \mathbf{0}   & \mathbf{0}   & \mathbf{0} &\mathbf{0}      & 0\\
\hline
0   & \mathbf{A}    &  \mathbf{0}    & \mathbb{1} & \mathbf{0} & \mathbf{0}   &\mathbf{0} &\mathbf{b}\\
0   & \mathbb{1}    & -\mathbf{M}    & \mathbf{0} & \mathbb{1} & \mathbf{0}   &\mathbf{0} &\mathbf{x}^0\\
0   & \mathbb{1}    &  \mathbf{M}    & \mathbf{0} & \mathbf{0} & -\mathbb{1}  &\mathbf{0} &\mathbf{x}^0\\ 
0   & \mathbf{0}    &  \mathbb{1}    & \mathbf{0} & \mathbf{0} & \mathbf{0}   &\mathbb{1} & \mathbf{1}\\ 
\end{array}\right].
\label{eq:tab1again}
\end{displaymath}
%
Here, the $\mathbf{s}_i$ are slack or surplus variables, aimed to write the
original inequality restrictions as equalities. The $\mathbf{s}_x$ are used to
rewrite restrictions on observed variables, the $\mathbf{s}_\pm$ to write the
upper and lower limits on $\mathbf{x}$ as equalities and $\mathbf{s}_\Delta$ to
write the upper limits on $\mathbf{\Delta}$ as equalities.

Observe that the above tableau \emph{almost} suggests a trivial solution. If we choose
$\mathbf{s}_x=\mathbf{b}$, $\mathbf{s}_\pm=\pm\mathbf{x}^0$ and $\mathbf{s}_\Delta=\mathbf{1}$,
we may set $(\mathbf{x},\mathbf{\Delta})=\mathbf{0}$. However, recall that we demand all variables
to be non-negative so $\mathbf{s}_-$ may not be equal to ${-\mathbf{x}^0}$. To resolve this
problem, we introduce a set of \emph{artificial} variables $\mathbf{a}_-$ and extend the
restrictions involving $\mathbf{s}_-$ as follows:
\begin{displaymath}
\mathbf{x} + M\mathbf{1} - \mathbf{s}_- + \mathbf{a}_- = \mathbf{x}_0.
\end{displaymath} 
This yields the following tableau.
\begin{displaymath}
\left[\begin{array}{r|rrrrrrr|l}
f   & \mathbf{x}^T  &\mathbf{\Delta} & \mathbf{s}_x &\mathbf{s}_+  & \mathbf{s}_- &\mathbf{s}_\Delta&\mathbf{a}_-\\
1   & \mathbf{0}    &-\mathbf{w}^T   & \mathbf{0}   & \mathbf{0}   & \mathbf{0}   &\mathbf{0} &\mathbf{0}     & 0\\
\hline
0   & \mathbf{A}    &  \mathbf{0}    & \mathbb{1} & \mathbf{0} & \mathbf{0}   &\mathbf{0} &\mathbf{0} &\mathbf{b}\\
0   & \mathbb{1}    & -\mathbf{M}    & \mathbf{0} & \mathbb{1} & \mathbf{0}   &\mathbf{0} &\mathbf{0} &\mathbf{x}^0\\
0   & \mathbb{1}    &  \mathbf{M}    & \mathbf{0} & \mathbf{0} & -\mathbb{1}  &\mathbf{0} &\mathbb{1} &\mathbf{x}^0\\ 
0   & \mathbf{0}    &  \mathbb{1}    & \mathbf{0} & \mathbf{0} & \mathbf{0}   &\mathbb{1} &\mathbf{0} & \mathbf{1}\\ 
\end{array}\right].
\label{eq:tab1again}
\end{displaymath}
%
The essential point to note is that the tableau now contains a unit matrix in
columns 4, 5, 7 and 8, so choosing $\mathbf{s}_x=\mathbf{b}$,
$\mathbf{s}_+=\mathbf{x}^0$, $\mathbf{s}_\Delta=\mathbf{1}$ and
$\mathbf{a}_-=\mathbf{x}^0$, and
$(\mathbf{x},\mathbf{\Delta},\mathbf{s}_-)=\mathbf{0}$ is a solution obeying
all restrictions. The artificial variables have no relation to the original
problem, so we want them to be zero in the final solution. Since the tableau
represents a set of linear equalities, we are allowed to multiply rows with a
constant and add and subtract rows. If this is done in such a way that we are
again left with a unit matrix in part of the columns, a new valid solution is
generated. Here, we add the fourth row to row three and subtract $\mathbf{M}^{-1}$
times the fourth row from the last row. Row four is multiplied with $\mathbf{M}^{-1}$.
This gives
\begin{displaymath}
\left[\begin{array}{r|rrrrrrr|l}
f   & \mathbf{x}^T   &\mathbf{\Delta}& \mathbf{s}_x &\mathbf{s}_+  & \mathbf{s}_- &\mathbf{s}_\Delta&\mathbf{a}_-\\
1   & \mathbf{0}     &-\mathbf{w}^T  & \mathbf{0}   & \mathbf{0}   & \mathbf{0}   &\mathbf{0} &\mathbf{0}     & 0\\
\hline
0   & \mathbf{A}     &  \mathbf{0}   & \mathbb{1} & \mathbf{0} & \mathbf{0}       &\mathbf{0} &\mathbf{0} &\mathbf{b}\\
0   & 2\mathbb{1}    &  \mathbf{0}   & \mathbf{0} & \mathbb{1} & \mathbf{0}       &\mathbf{0} &\mathbb{1} &2\mathbf{x}^0\\
0   & \mathbf{M}^{-1}&  \mathbb{1}   & \mathbf{0} & \mathbf{0} & -\mathbf{M}^{-1} &\mathbf{0} &\mathbf{M}^{-1} &\mathbf{M}^{-1}\mathbf{x}^0\\ 
0   &-\mathbf{M}^{-1}&  \mathbf{0}   & \mathbf{0} & \mathbf{0} & \mathbf{M}^{-1}  &\mathbb{1} &-\mathbf{M}^{-1} 
                                                                                                    & \mathbf{1}-\mathbf{M}^{-1}\mathbf{x}^0\\ 
\end{array}\right].
\label{eq:tab1again}
\end{displaymath}
%
This almost gives a new valid solution, except that the first row, representing the objective function has not
vanished at the third column. However, we may re-express the objective function in terms of the other variables. Namely
using row four, we have
\begin{displaymath}
\mathbf{w}^{T}
\mathbf{\Delta} =\mathbf{w}^T \mathbf{M}^{-1}\mathbf{x}^0  +\mathbf{w}^T\mathbf{M}^{-1}(\mathbf{s}_- -\mathbf{x} -\mathbf{a}_-).
\end{displaymath}
Substituting this equation in the top row of the tableau shows that the
solution $\mathbf{\Delta}=\mathbf{M}^{-1}\mathbf{x}^0$,
$\mathbf{s}_x=\mathbf{b}$, $\mathbf{s}_+=2\mathbf{x}^0$ and
$\mathbf{s}_\Delta=\mathbf{1}-\mathbf{M}^{-1}\mathbf{x}^0$, and
$(\mathbf{x},\mathbf{s}_-,\mathbf{a}_-)=\mathbf{0}$ represents a valid solution.
Since the artificial variables have vanishing values, we may now delete the corresponding
column, and arrive at the tableau of Eq.~\eqref{eq:phaseI}.

\end{document}

